{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7 Sentiment Analysis\n",
    "감정분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 문장 시퀀스 뒤에 감정을 결정하는 과정으로 정의한다\n",
    "# Speaker 혹은 Text사고를 표현하는 사람의 감정을 판단하는데 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 감정분석 소개\n",
    "# 2 NER을 사용한 감정분석\n",
    "# 3 기계학습을 활용한 감정분석\n",
    "# 4 NER 시스템의 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "감정분석 소개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# target : 이진분류(긍정, 부정), 멀티분류 (긍정, 부정, 중립)\n",
    "# 감정과 토픽 마이닝을 결합한 '토픽-감정분석'을 시행한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 감정분석 : lexicon (어휘목록) 을 사용해서 수행할 수 있다\n",
    "# 1 labMT (10,000단어 분석)\n",
    "# 2 Warringer (13,915단어 분석)\n",
    "# 3 OpinionFinder's Subjectivity Lexic (8221단어 분석)\n",
    "# 4 ANEW  (1034단어 분석)    : Affective Norms for English Words\n",
    "# 5 AFINN (2477단어 분석)    : Finn Arup Nielson 에 의한 분류\n",
    "# 6 Balance Affective (277 단어) : 1(긍정), 2(부정), 3(불안정), 4(중립)\n",
    "# 7 BAWL  (2200단어 분석)    : Berlin Affective Word List Reloaded\n",
    "# 8 BFAN  (210단어로 구성)    : Bilingual Finnish Affective Norms\n",
    "# 9 CDGE  : Compass DeRose Guide to Emotion Words\n",
    "# 10 DAL  : Dictionary of Affect in Language\n",
    "# 11 WDAL : Whissell's Dictionary of Affect in Language\n",
    "# 등등..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 영어 리뷰의 감정분석\n",
    "sentiment analysis for movie review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "movie_reviews.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg/cv000_29416.txt',\n",
       " 'neg/cv001_19502.txt',\n",
       " 'neg/cv002_17424.txt',\n",
       " 'neg/cv003_12683.txt',\n",
       " 'neg/cv004_12641.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.fileids(movie_reviews.categories()[0])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 2 879\n",
      "2000 2 1144\n"
     ]
    }
   ],
   "source": [
    "docs = [(list(movie_reviews.words(fid)), cat) \n",
    "        for cat in movie_reviews.categories()   # ['neg', 'pos']\n",
    "        for fid in movie_reviews.fileids(cat)]  # 'neg/cv000_29416.txt',....\n",
    "\n",
    "# 2000개 문서, 긍/부정 분류, 개별 문서의 token 목록으로 정리\n",
    "print(len(docs), len(docs[0]), len(docs[0][0]))\n",
    "print(len(docs), len(docs[13]), len(docs[13][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영화리뷰 token의 총 갯수 : 39768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['plot',\n",
       " 'arrow',\n",
       " 'mir',\n",
       " 'indication',\n",
       " 'seymour',\n",
       " 'house',\n",
       " 'digital',\n",
       " 'terrible',\n",
       " 'strives',\n",
       " 'willing']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 긍/부정 분류된 textdml token을 뒤섞어서 1개로 생성\n",
    "import nltk, random\n",
    "\n",
    "random.shuffle(docs)\n",
    "all_tokens = nltk.FreqDist(x.lower() for x in movie_reviews.words())\n",
    "print('영화리뷰 token의 총 갯수 :', len(all_tokens.keys()))\n",
    "\n",
    "token_features = list(all_tokens.keys())[:2000]\n",
    "token_features[::200] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos/cv957_8737.txts Token : 597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'contains(plot)': True}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk의 영화리뷰 테이터 묶음 'token_features'에 'docs'데이터가 포함여부를 판단\n",
    "# Accuracy Boolean 사용자 함수\n",
    "\n",
    "def doc_features(docs):\n",
    "    doc_words = set(docs)  # 대상 문서의 token을 집합으로 추출\n",
    "    features = {}\n",
    "    for word in token_features:\n",
    "        features['contains(%s)' % word] = (word in doc_words)\n",
    "        return features\n",
    "\n",
    "token_file = 'pos/cv957_8737.txt' \n",
    "print(token_file + 's Token :', len(movie_reviews.words(token_file)))\n",
    "doc_features(movie_reviews.words( token_file ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n",
      "Most Informative Features\n",
      "          contains(plot) = True              neg : pos    =      1.4 : 1.0\n",
      "          contains(plot) = False             pos : neg    =      1.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# docs : nltk 영화리뷰의 모든 token\n",
    "feature_sets = [(doc_features(d), c) for (d,c) in docs]\n",
    "\n",
    "# 2000개 뒤섞은 리뷰 100개씩 추출하여 train/ test 데이터를 생성\n",
    "train_sets, test_sets = feature_sets[100:], feature_sets[:100]\n",
    "\n",
    "# 나이브 베이즈 분류기로 훈련데이터 만들기\n",
    "classifiers = nltk.NaiveBayesClassifier.train(train_sets)\n",
    "\n",
    "# 나이브 베이즈 분류기 자료와, test_set의 정확도 판단  \n",
    "print(nltk.classify.accuracy(classifiers, test_sets))\n",
    "classifiers.show_most_informative_features(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 결과\n",
    "# 정보 특징 (Most Informative Features) 이  문서내 존재여부를 체크한다\n",
    "# (5) 는 없어도 결과는 위처럼 2개만 결과로 출력....\n",
    "# plot데 대해서만 귀무가설, 대립가설 일치도를 판단\n",
    "# 책과 다르다.... GitHub의 결과도 위와 동일.....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 텍스트 전처리\n",
    "nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text --> 문장 --> token\n",
    "import nltk\n",
    "class Splitter(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nltk_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.nltk_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self, text):\n",
    "        sentences = self.nltk_splitter.tokenize(text)  # text --> 문장\n",
    "        tokenized_sentences = [self.nltk_tokenizer.tokenize(sent) # 문장 --> token\n",
    "                               for sent in sentences]\n",
    "        return tokenized_sentences\n",
    "\n",
    "# 문장의 3개 token으로 묶어서 정리한다\n",
    "# 단어(word), 표제어(lemme), 태그(tag)\n",
    "class POSTagger(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def pos_tag(self, sentences):\n",
    "        pos = [nltk.pos_tag(sentence) for sentence in sentences]\n",
    "        pos = [[(word, word, [postag]) \n",
    "                for (word, postag) in sentence] \n",
    "                for sentence in pos]\n",
    "        return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Why', 'are', 'you', 'looking', 'disappointed', '.'],\n",
       " ['We', 'will', 'go', 'to', 'restaurant', 'for', 'dinner', '.']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Why are you looking disappointed. \n",
    "We will go to restaurant for dinner.\"\"\"\n",
    "\n",
    "splitter = Splitter()\n",
    "postagger = POSTagger()\n",
    "splitted_sentences = splitter.split(text)\n",
    "splitted_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Why', 'Why', ['WRB']),\n",
       "  ('are', 'are', ['VBP']),\n",
       "  ('you', 'you', ['PRP']),\n",
       "  ('looking', 'looking', ['VBG']),\n",
       "  ('disappointed', 'disappointed', ['VBN']),\n",
       "  ('.', '.', ['.'])],\n",
       " [('We', 'We', ['PRP']),\n",
       "  ('will', 'will', ['MD']),\n",
       "  ('go', 'go', ['VB']),\n",
       "  ('to', 'to', ['TO']),\n",
       "  ('restaurant', 'restaurant', ['VB']),\n",
       "  ('for', 'for', ['IN']),\n",
       "  ('dinner', 'dinner', ['NN']),\n",
       "  ('.', '.', ['.'])]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tagged_sentences = postagger.pos_tag(splitted_sentences)\n",
    "pos_tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/sujithvm/nlp-modules/blob/master/sentiment%20analysis/sentiment_analyzer.py\n",
    "# 폴더내 파일들의 text에 대해 긍정/ 부정 tag 작업 시행\n",
    "# dictionary를 활용해서 tag를 생성\n",
    "\n",
    "class DictionaryTagger(object):\n",
    "\n",
    "    def __init__(self, dictionary_paths):\n",
    "        files = [open(path, 'r') for path in dictionary_paths]      \n",
    "        map(lambda x: x.close(), files)\n",
    "\n",
    "        dictionaries = [yaml.load(dict_file) for dict_file in files]\n",
    "        self.dictionary = {}\n",
    "        self.max_key_size = 0\n",
    "        \n",
    "        for curr_dict in dictionaries:\n",
    "            for key in curr_dict:\n",
    "                if key in self.dictionary:  # dictionary에 포함된 목록인 경우           \n",
    "                    self.dictionary[key].extend(curr_dict[key])                    \n",
    "                else:                       # 새로운 dictionary 목록인 경우\n",
    "                    self.dictionary[key] = curr_dict[key] # dict을 생성/ 최대값 추가                   \n",
    "                    self.max_key_size = max(self.max_key_size, len(key))\n",
    "                \n",
    "    def tag(self, postagged_sentences):\n",
    "        return [self.tag_sentence(sentence) for sentence in postagged_sentences]\n",
    "\n",
    "    def tag_sentence(self, sentence, tag_with_lemmas=False):\n",
    "        tag_sentence = []\n",
    "        N = len(sentence)\n",
    "        if self.max_key_size == 0: \n",
    "            self.max_key_size = N\n",
    "        i = 0\n",
    "        while (i < N):\n",
    "            j = min(i + self.max_key_size, N)\n",
    "            tagged = False\n",
    "            while (j > i):\n",
    "                expression_form = ' '.join([word[0] for word in sentence[i:j]]).lower()\n",
    "                expression_lemma = ' '.join([word[1] for word in sentence[i:j]]).lower()\n",
    "\n",
    "                if tag_with_lemmas:\n",
    "                    literal = expression_lemma\n",
    "                else:\n",
    "                    literal = expression_form\n",
    "\n",
    "                if literal in self.dictionary:\n",
    "                    is_single_token = j - i == 1\n",
    "                    original_position = i\n",
    "                    i = j\n",
    "                    taggings = [tag for tag in self.dictionary[literal]]\n",
    "                    tagged_expression = (expression_form, expression_lemma, taggings)\n",
    "                    if is_single_token: \n",
    "                        original_token_tagging = sentence[original_position][2]\n",
    "                        tagged_expression[2].extend(original_token_tagging)\n",
    "                    tag_sentence.append(tagged_expression)\n",
    "                    tagged = True\n",
    "                else:\n",
    "                    j = j - 1\n",
    "\n",
    "            if not tagged:\n",
    "                tag_sentence.append(sentence[i])\n",
    "                i += 1\n",
    "        return tag_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 긍/부정 정리된 dictionary 목록의 표현들을 Counting\n",
    "def value_of(sentiment):\n",
    "    if sentiment == 'positive': return 1\n",
    "    if sentiment == 'negative': return -1\n",
    "    return 0\n",
    "\n",
    "def sentiment_score(review):\n",
    "    return sum([sentence_score(sentence, None, 0.0) for sentence in review])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 NER를 사용한 감정분석 -  176 p\n",
    "개체명(고유명사) 인식 Named-entity recognition (NER) - 감정식별을 위한 stopword 구분절차\n",
    "\n",
    "    token의 개체명을 별도의 기준으로 식별 후, 클래스로 분류하는 과정으로 \n",
    "    히든마르코프, 최대엔트로피 마르코프, SVM, 의사결정나무 등을 활용한다\n",
    "    개체명으로 인식되면, 감정분석에 기여하지 않으므로, 제외한 나머지들로 감정분석을 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 기계학습을 사용한 감정분석\n",
    "nltk.sentiment.sentiment_analyzer() 는 기계학습 기반의 감정분석 모듈이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markbaum/Python/python36/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "#from __future__ import print_function\n",
    "from collections import defaultdict\n",
    "from nltk.classify.util import apply_features, accuracy as eval_accuracy\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import (BigramAssocMeasures, precision as eval_precision, \n",
    "                          recall as eval_recall, f_measure as eval_f_measure)\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.sentiment.util import save_file, timer\n",
    "\n",
    "# 기계학습에 기반한 감정분석도구\n",
    "class SentimentAnalyzer(object):\n",
    "    def __init__(self, classifier=None):\n",
    "        self.feat_extractors = defaultdict(list)\n",
    "        self.classifier = classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트에서 모든 (중복)단어를 반환\n",
    "def all_words(self, documents, labeled=None):\n",
    "    all_words = []\n",
    "    if labeled is None:\n",
    "        labeled = documents and isinstance(documents[0], tuple)\n",
    "    if labeled == True:\n",
    "        for words, sentiment in documents:\n",
    "            all_words.extend(words)\n",
    "    elif labeled == False:\n",
    "        for words in documents:\n",
    "            all_words.extend(words)\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 특징 추출 함수 feature extraction function\n",
    "def apply_features(self, documents, labeled=None):\n",
    "    return apply_features(self.extract_features, documents,labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 단어의 특징을 반환하는 코드\n",
    "def unigram_word_feats(self, words, top_n=None, min_freq=0):\n",
    "    unigram_feats_freqs = FreqDist(word for word in words)\n",
    "    return [w    for   w, f   in   unigram_feats_freqs.most_common(top_n) \n",
    "                 if   unigram_feats_freqs[w]  >  min_freq ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bi-gram의 특징을 반환하는 코드\n",
    "def bigram_collocation_feats(self, documents, top_n=None, min_freq=3, assoc_measure=BigramAssocMeasures.pmi):\n",
    "    finder = BigramCollocationFinder.from_documents(documents)\n",
    "    finder.apply_freq_filter(min_freq)\n",
    "    return finder.nbest(assoc_measure, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 사용 가능한 특징세트를 사용하여, 주어진 인스턴스를 분류\n",
    "def classify(self, instance):\n",
    "    instance_feats = self.apply_features([instance],labeled=False)\n",
    "    return self.classifier.classify(instance_feats[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 텍스트에서 특징 추출을 위해 사용\n",
    "def add_feat_extractor(self, function, **kwargs):\n",
    "    self.feat_extractors[function].append(kwargs)\n",
    "\n",
    "def extract_features(self, document):\n",
    "    all_features = {}\n",
    "    for extractor in self.feat_extractors:\n",
    "        for param_set in self.feat_extractors[extractor]:\n",
    "            feats = extractor(document, **param_set)\n",
    "        all_features.update(feats)\n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 훈련데이터를 훈련시키는 함수\n",
    "def train(self, trainer, training_set, save_classifier = None, **kwargs):\n",
    "    print(\"Training classifier\")\n",
    "    self.classifier = trainer(training_set, **kwargs)\n",
    "    if save_classifier:\n",
    "        save_file(self.classifier, save_classifier)\n",
    "    return self.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 테스트데이터를 사용한 분류기의 테스트 및 성능평가\n",
    "def evaluate(self, test_set, classifier = None, accuracy = True, \n",
    "             f_measure = True, precision = True, recall = True, verbose = False):\n",
    "\n",
    "    if classifier is None:  # 분류기에 아무것도 지정하지 않은 경우\n",
    "        classifier = self.classifier  # __init__의 초깃값을 불러와서 작업을 시작\n",
    "    print(\"Evaluating {0} results...\".format(type(classifier).__name__))\n",
    "    metrics_results = {}              # 출력 report dictionary를 생성\n",
    "\n",
    "    if accuracy == True:   # 정확도 측정옵션에 True를 입력시\n",
    "        accuracy_score = eval_accuracy(classifier, test_set) # 분류기 기준, 데이터를 test한다\n",
    "        metrics_results['Accuracy'] = accuracy_score         # report dictionary에 기록 \n",
    "        \n",
    "    # 출처 : https://dongyeopblog.wordpress.com/2016/04/08/python-defaultdict-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0/\n",
    "    gold_results = defaultdict(set)   # key값 지정없어도, default로 key를 자동으로 지정한다\n",
    "    test_results = defaultdict(set)   \n",
    "    labels = set()\n",
    "    for i, (feats, label) in enumerate(test_set): # test 상세내용을 기록한다\n",
    "        labels.add(label)\n",
    "        gold_results[label].add(i)\n",
    "        observed = classifier.classify(feats)\n",
    "        test_results[observed].add(i)\n",
    "\n",
    "    for label in labels:  # test 결과 수집된 labels 에 따라, 평가함수로 계산을 한다\n",
    "        if precision == True:  # 정확도 측정\n",
    "            precision_score = eval_precision(gold_results[label], test_results[label])\n",
    "            metrics_results['Precision [{0}]'.format(label)] = precision_score\n",
    "        if recall == True:     # recall 측정\n",
    "            recall_score = eval_recall(gold_results[label], test_results[label])\n",
    "            metrics_results['Recall [{0}]'.format(label)] = recall_score\n",
    "        if f_measure == True:  # f-measure 측정\n",
    "            f_measure_score = eval_f_measure(gold_results[label], test_results[label])\n",
    "            metrics_results['F-measure [{0}]'.format(label)] = f_measure_score\n",
    "        if verbose == True:    # Data를 정렬\n",
    "            for result in sorted(metrics_results):\n",
    "                print('{0}: {1}'.format(result, metrics_results[result]))\n",
    "    return metrics_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 기계학습을 사용한 감정분석\n",
    "Twitter text 데이터에 대한 통계, 자동화, 기계학습 분류기\n",
    "\n",
    "출처 : http://www.nltk.org/book/ch06.html\n",
    "\n",
    "data : https://github.com/ravikiranj/twitter-sentiment-analyzer\n",
    "\n",
    "<img src = \"http://www.nltk.org/images/supervised-classification.png\" align='left' width = '500'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopWords = []\n",
    "import re, csv\n",
    "# 중복되는 문자를 단일로 처리\n",
    "def replaceTwoOrMore(s):\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    return pattern.sub(r\"\\1\\1\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 불용어 목록을 파일에서 읽어온다\n",
    "def getStopWordList(stopWordListFileName):\n",
    "    stopWords = []   # 불용어 목록 파일의 text를 '공백'을 기준으로 token생성 뒤, list로 출력\n",
    "    stopWords.append('AT_USER')\n",
    "    stopWords.append('URL')\n",
    "    fp = open(stopWordListFileName, 'r')\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        word = line.strip()\n",
    "        stopWords.append(word)\n",
    "        line = fp.readline()\n",
    "    fp.close()\n",
    "    return stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러번 반복된 단어들의 전처리\n",
    "def getFeatureVector(tweet):\n",
    "    featureVector = []\n",
    "    words = tweet.split()       # 공백을 기분으로 token 단어를 생성\n",
    "    for w in words:             \n",
    "        w = replaceTwoOrMore(w) # 2번 이상 반복된 단어를 단일로 전처리 (사용자함수)\n",
    "        w = w.strip('\\'\"?,.')   # 문장부호를 제거\n",
    "        val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", w) # 알파벳 여부 확인\n",
    "    # 해당 단어가 불용어에 해당하면, 무시한다\n",
    "    if(w in stopWords or val is None):\n",
    "        pass # continue 는 '반복문'에서 가능\n",
    "    else:\n",
    "        featureVector.append(w.lower())\n",
    "    return featureVector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://gist.github.com/ravikiranj/2639031\n",
    "# 트위터 데이터 전처리 함수\n",
    "def processTweet(tweet):\n",
    "    tweet = tweet.lower()  # 데이터를 소문자로 바꾼다\n",
    "    # www.* 또는 https?://* 를 URL 로 변환\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)   # @username 을 AT_USER로 변환\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)         # 불필요한 여백제거   \n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)  # 해시태그 '#' 를 제거\n",
    "    tweet = tweet.strip('\\'\"')                  # trim\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "['twitter']\n",
      "[]\n",
      "['makeitcount']\n",
      "['sigh']\n",
      "['hurts']\n",
      "['hurts']\n"
     ]
    }
   ],
   "source": [
    "# 트위터 데이터가 .txt인 경우\n",
    "# Tweets 데이터는 1줄씩 모든 프로세스를 진행한다\n",
    "fp = open('data/sampleTweets.txt', 'r')           # Data text 파일\n",
    "line = fp.readline()\n",
    "st = open('data/stopwords.txt', 'r') # Stop word text 파일 \n",
    "stopWords = getStopWordList('data/stopwords.txt')\n",
    "while line:  # 트위터 데이터 1줄씩 처리\n",
    "    processedTweet = processTweet(line)\n",
    "    featureVector = getFeatureVector(processedTweet)\n",
    "    print(featureVector)\n",
    "    line = fp.readline()\n",
    "fp.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([], 'positive'),\n",
       " ([], 'positive'),\n",
       " ([], 'positive'),\n",
       " (['twitter'], 'neutral'),\n",
       " ([], 'neutral'),\n",
       " (['makeitcount'], 'neutral'),\n",
       " (['sigh'], 'negative'),\n",
       " (['hurts'], 'negative'),\n",
       " (['hurts'], 'negative')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 트위터 데이터가 .CSV 인 경우\n",
    "# Tweets are read one by one and then processed.\n",
    "inpTweets = csv.reader(open('./data/sampleTweets.csv', 'r'), delimiter=',', quotechar='|')\n",
    "tweets = []\n",
    "\n",
    "for row in inpTweets:\n",
    "    sentiment = row[0]\n",
    "    tweet = row[1]\n",
    "    processedTweet = processTweet(tweet)\n",
    "    featureVector = getFeatureVector(processedTweet) #, stopWords)\n",
    "    tweets.append((featureVector, sentiment));\n",
    "\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 특징 추출하는 메서드\n",
    "def extract_features(tweet):\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in featureList:\n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 나이브베이즈 분류기로 감정분석\n",
    "NaiveBClassifier = nltk.NaiveBayesClassifier.train(train_sets)\n",
    "# Testing the classifiertestTweet = 'I liked this book on Sentiment Analysis a lot.'\n",
    "# processedTestTweet = processTweet(test_sets)\n",
    "# NaiveBClassifier.classify(extract_features(getFeatureVector(processedTestTweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "testTweet = 'I am so badly hurt'\n",
    "processedTestTweet = processTweet(testTweet)\n",
    "# NaiveBClassifier.classify(extract_features(getFeatureVector(processedTestTweet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 NER 시스템의 평가\n",
    "Evaluation of the NER system"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
