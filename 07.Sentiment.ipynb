{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7 Sentiment Analysis\n",
    "감정분석 : Gensim을 활용한 한글 긍/부정 분석\n",
    "\n",
    "https://www.lucypark.kr/slides/2015-pyconkr/#1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 문장 시퀀스 뒤에 감정을 결정하는 과정으로 정의한다\n",
    "# Speaker 혹은 Text사고를 표현하는 사람의 감정을 판단하는데 사용\n",
    "\n",
    "    # 1 감정분석 소개\n",
    "    # 2 NER을 사용한 감정분석\n",
    "    # 3 기계학습을 활용한 감정분석\n",
    "    # 4 NER 시스템의 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 1 Introduction\n",
    "감정분석 소개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# target : 이진분류(긍정, 부정), 멀티분류 (긍정, 부정, 중립)\n",
    "# 감정과 토픽 마이닝을 결합한 '토픽-감정분석'을 시행한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 감정분석 : lexicon (어휘목록) 을 사용해서 수행할 수 있다\n",
    "# 1 labMT (10,000단어 분석)\n",
    "# 2 Warringer (13,915단어 분석)\n",
    "# 3 OpinionFinder's Subjectivity Lexic (8221단어 분석)\n",
    "# 4 ANEW  (1034단어 분석)    : Affective Norms for English Words\n",
    "# 5 AFINN (2477단어 분석)    : Finn Arup Nielson 에 의한 분류\n",
    "# 6 Balance Affective (277 단어) : 1(긍정), 2(부정), 3(불안정), 4(중립)\n",
    "# 7 BAWL  (2200단어 분석)    : Berlin Affective Word List Reloaded\n",
    "# 8 BFAN  (210단어로 구성)    : Bilingual Finnish Affective Norms\n",
    "# 9 CDGE  : Compass DeRose Guide to Emotion Words\n",
    "# 10 DAL  : Dictionary of Affect in Language\n",
    "# 11 WDAL : Whissell's Dictionary of Affect in Language\n",
    "# 등등..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 2 영화 리뷰의 감정분석\n",
    "sentiment analysis for movie review\n",
    "\n",
    "http://www.nltk.org/book/ch06.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 학습을 위한 Train 데이터 생성하기\n",
    "nltk의 movie_review 데이터를 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "movie_reviews.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg/cv000_29416.txt',\n",
       " 'neg/cv001_19502.txt',\n",
       " 'neg/cv002_17424.txt',\n",
       " 'neg/cv003_12683.txt',\n",
       " 'neg/cv004_12641.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.fileids(movie_reviews.categories()[0])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a couple of criminals ( mario van peebles and loretta devine ) move into a rich family's house in hopes of conning them out of their jewels . \r\n",
      "however , someone else steals the jewels before they are able to get to them . \r\n",
      "writer mario van peebles delivers a clever script with several unexpected plot twists , but director mario van peebles undermines his own high points with haphazard camera work , editing and pacing . \r\n",
      "it felt as though the film should have been wrapping up at the hour mark , but alas there was still 35 more minutes to go . \r\n",
      "daniel baldwin ( i can't believe i'm about to type this ) gives the best performance in the film , outshining the other talented members of the cast . \r\n",
      "[r] \r\n"
     ]
    }
   ],
   "source": [
    "! cat /home/markbaum/nltk_data/corpora/movie_reviews/neg/cv435_24355.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs        : 2000\n",
      "docs[13][1] : neg\n",
      "docs[13][0] : 1144 ['a', 'all', 'fate', 'like', 'intersperesed', 'the', '.', 'who', 'like', 'characters', '.', 'looks', 'us', 'obviously', 'give']\n"
     ]
    }
   ],
   "source": [
    "docs = [(list(movie_reviews.words(fid)), cat) \n",
    "        for cat in movie_reviews.categories()   # ['neg', 'pos']\n",
    "        for fid in movie_reviews.fileids(cat)]  # 'neg/cv000_29416.txt', ....\n",
    "\n",
    "print('docs        :', len(docs))\n",
    "print('docs[13][1] :', docs[13][1])\n",
    "print('docs[13][0] :', len(docs[13][0]), docs[13][0][::80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영화리뷰 token의 총 합 : 39768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['plot', 'seymour', 'strives']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 긍/부정 분류된 textdml token을 뒤섞어서 1개로 합친다\n",
    "import nltk, random\n",
    "random.shuffle(docs)\n",
    "all_tokens = nltk.FreqDist(x.lower()    for  x  in  movie_reviews.words())\n",
    "print('영화리뷰 token의 총 합 :', len(all_tokens.keys()))\n",
    "\n",
    "# 39,768개 중  2,000개  Train 데이터 추출\n",
    "token_features = list(all_tokens.keys())[:2000]\n",
    "token_features[::800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 중간결과\n",
    "# doc            : neg, pos target을 표시한, text의 token list를 수집\n",
    "# token_features : all_tokens.keys())[:2000]의    token list를 수집"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 all.tokens [:2000]표본을 추출하여 학습모델을 생성\n",
    "'pos/cv957_8737.txt' 리뷰가 긍정/ 부정 여부를 \n",
    "\n",
    "<strong>나이브 베이즈 분류기</strong>를 활용하여 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos/cv957_8737.txts Token : 597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'contains(:)': True,\n",
       " 'contains(couples)': False,\n",
       " 'contains(go)': False,\n",
       " 'contains(plot)': True,\n",
       " 'contains(teen)': False,\n",
       " 'contains(two)': True}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_file = 'pos/cv957_8737.txt' \n",
    "print(token_file + 's Token :', len(movie_reviews.words(token_file)))\n",
    "\n",
    "# 리뷰 데이터의 Token이 표본DB 포함여부 판단\n",
    "def doc_features(docs):\n",
    "    doc_words = set(docs)  # 리뷰 txt의 token을 집합으로 추출\n",
    "    features = {}\n",
    "    \n",
    "    # all_tokens[:2000] 표본에 대해, 리뷰 token의 포함여부를 판단 \n",
    "    for i, word in enumerate(token_features):\n",
    "        features['contains(%s)' % word] = (word in doc_words)\n",
    "        if i == 5 : break\n",
    "    return features\n",
    "\n",
    "doc_features(movie_reviews.words( token_file ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6\n",
      "Most Informative Features\n",
      "       contains(couples) = True              neg : pos    =      1.5 : 1.0\n",
      "          contains(plot) = True              neg : pos    =      1.4 : 1.0\n",
      "          contains(plot) = False             pos : neg    =      1.3 : 1.0\n",
      "          contains(teen) = True              neg : pos    =      1.2 : 1.0\n",
      "             contains(:) = False             pos : neg    =      1.0 : 1.0\n",
      "             contains(:) = True              neg : pos    =      1.0 : 1.0\n",
      "           contains(two) = False             neg : pos    =      1.0 : 1.0\n",
      "           contains(two) = True              pos : neg    =      1.0 : 1.0\n",
      "          contains(teen) = False             pos : neg    =      1.0 : 1.0\n",
      "       contains(couples) = False             pos : neg    =      1.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# docs (긍/부정리뷰의 token 모음) 데이터의 \n",
    "# Train(90%), Test(10%)로 나눈다 \n",
    "feature_sets = [(doc_features(d), c) for (d,c) in docs]\n",
    "train_sets, test_sets = feature_sets[100:], feature_sets[:100]\n",
    "\n",
    "# 나이브 베이즈 분류기로 정확도를 판단\n",
    "classifiers = nltk.NaiveBayesClassifier.train(train_sets)\n",
    "print('Accuracy :', nltk.classify.accuracy(classifiers, test_sets))\n",
    "classifiers.show_most_informative_features() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 결과\n",
    "# 정확도가 60%를 살짝 넘김.. 그리고 token이 많다고 성능이 높진 않음\n",
    "# 책과는 결과가 다르다.... GitHub의 결과도 위와 동일.....\n",
    "# neg : pos 분류차이값이 많이 나던데, 여기선 별로 차이가 덜하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 Document Classification\n",
    "\n",
    "http://www.nltk.org/book/ch06.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contains(:)': True,\n",
       " 'contains(a)': True,\n",
       " 'contains(church)': False,\n",
       " 'contains(couples)': False,\n",
       " 'contains(go)': False,\n",
       " 'contains(party)': False,\n",
       " 'contains(plot)': True,\n",
       " 'contains(teen)': False,\n",
       " 'contains(to)': True,\n",
       " 'contains(two)': True}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 아래 2줄을 함수에 넣으면 loop에 걸리더라.. (일부러 밖에서 처리를 해야만 작동되더라..)\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:2000]\n",
    "\n",
    "def document_features(document, word_features = word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "document_features(movie_reviews.words('pos/cv957_8737.txt'), word_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78\n",
      "Most Informative Features\n",
      " contains(unimaginative) = True              neg : pos    =      8.4 : 1.0\n",
      "        contains(welles) = True              neg : pos    =      8.4 : 1.0\n",
      "        contains(sexist) = True              neg : pos    =      7.7 : 1.0\n",
      "    contains(schumacher) = True              neg : pos    =      7.5 : 1.0\n",
      "        contains(shoddy) = True              neg : pos    =      7.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unimaginative : 상상력이 없는\n",
    "# shoddy        : 겉만 번지르르한 싸구려,  재생 털실\n",
    "# atrocious     : 극악 무도한\n",
    "# 추출한 단어 자체가 선명해서, 결과도 분명하고 정확도도 높은 결과를 도출한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 3 텍스트 전처리\n",
    "text를 \n",
    "1. 단어(word)\n",
    "2. 표제어/주제(lemma)\n",
    "3. 태그(tag) \n",
    "\n",
    "를 포함한 데이터로 추출한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text --> 문장 --> token\n",
    "import nltk\n",
    "class Splitter(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nltk_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.nltk_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self, text):\n",
    "        sentences = self.nltk_splitter.tokenize(text)             # txt --> 문장\n",
    "        tokenized_sentences = [self.nltk_tokenizer.tokenize(sent) # 문장 --> token\n",
    "                               for sent in sentences]\n",
    "        return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Why', 'are', 'you', 'looking', 'disappointed', '.'],\n",
       " ['We', 'will', 'go', 'to', 'restaurant', 'for', 'dinner', '.']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Txt --> 문장 --> token\n",
    "text = \"\"\"Why are you looking disappointed. \n",
    "We will go to restaurant for dinner.\"\"\"\n",
    "\n",
    "splitter = Splitter()\n",
    "splitted_sentences = splitter.split(text)\n",
    "splitted_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Why', ['WRB']),\n",
       "  ('are', ['VBP']),\n",
       "  ('you', ['PRP']),\n",
       "  ('looking', ['VBG']),\n",
       "  ('disappointed', ['VBN']),\n",
       "  ('.', ['.'])],\n",
       " [('We', ['PRP']),\n",
       "  ('will', ['MD']),\n",
       "  ('go', ['VB']),\n",
       "  ('to', ['TO']),\n",
       "  ('restaurant', ['VB']),\n",
       "  ('for', ['IN']),\n",
       "  ('dinner', ['NN']),\n",
       "  ('.', ['.'])]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3개 성분을 추출 후, 묶어서 정리\n",
    "# 단어(word/token), 표제어/주제(lemme), 태그(tag)\n",
    "class POSTagger(object):\n",
    "    def __init__(self): \n",
    "        pass\n",
    "    def pos_tag(self, sentences):\n",
    "        # 입력 sentence 에 token을 첨부한다\n",
    "        pos = [nltk.pos_tag(sentence)       for sentence in sentences]\n",
    "\n",
    "        # 첨부된 token을 tuple 형식으로 묶어서, list에 정리한다\n",
    "        pos = [[(word, [postag]) \n",
    "                for (word, postag) in sentence] \n",
    "                for sentence in pos]\n",
    "        return pos\n",
    "\n",
    "postagger = POSTagger()\n",
    "pos_tagged_sentences = postagger.pos_tag(splitted_sentences)\n",
    "pos_tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/sujithvm/nlp-modules/blob/master/sentiment%20analysis/sentiment_analyzer.py\n",
    "# 폴더내 파일들의 text에 대해 긍정/ 부정 tag 작업 시행\n",
    "# dictionary를 활용해서 tag를 생성\n",
    "import nltk, yaml, sys, re ,os \n",
    "class DictionaryTagger(object):\n",
    "\n",
    "    def __init__(self, dictionary_paths):\n",
    "        # 해당 폴더에 있는 text 파일 목록을 수집한다\n",
    "        files = [open(path, 'r')     for path in dictionary_paths]      \n",
    "        map(lambda x: x.close(),     files)\n",
    "\n",
    "        # http://egloos.zum.com/sweeper/v/3042272\n",
    "        # yaml : 텍스트 파일의 내용을 파싱한다 \n",
    "        dictionaries = [yaml.load(dict_file) \n",
    "                                     for dict_file in files]\n",
    "\n",
    "        self.dictionary ,self.max_key_size = {}, 0\n",
    "        for curr_dict in dictionaries:\n",
    "            for key in curr_dict:\n",
    "                # key 값으로 존재하는 경우, dict에 내용을 추가한다\n",
    "                if key in self.dictionary:             \n",
    "                    self.dictionary[key].extend(curr_dict[key])                    \n",
    "\n",
    "                # key 값이 없는경우, 새로운 dictionary 목록을 생성한다\n",
    "                else:\n",
    "                    self.dictionary[key] = curr_dict[key]                   \n",
    "                    self.max_key_size = max(self.max_key_size, len(key))\n",
    "\n",
    "    # 문장의 token의, tag를 list를 추출한다            \n",
    "    def tag(self, pos_tagged_sentences):\n",
    "        return [self.tag_sentence(sentence) \n",
    "                          for sentence in pos_tagged_sentences]\n",
    "\n",
    "    # token의 tag추출 함수 (주제도 함께 추출할지를 옵션으로 설정)\n",
    "    def tag_sentence(self, sentence, tag_with_lemmas = False):\n",
    "        tag_sentence = []\n",
    "        N = len(sentence)\n",
    "        if self.max_key_size == 0: \n",
    "            self.max_key_size = N\n",
    "        i = 0\n",
    "        while (i < N): # 문장 내 token의 갯수만큼 반복한다\n",
    "            j = min(i + self.max_key_size, N)\n",
    "            \n",
    "            tagged = False\n",
    "            while (j > i):\n",
    "                expression_form  = ' '.join([word[0]   for word in sentence[i:j]]).lower()\n",
    "                expression_lemma = ' '.join([word[1]   for word in sentence[i:j]]).lower()\n",
    "\n",
    "                if tag_with_lemmas:\n",
    "                    literal = expression_lemma\n",
    "                else:\n",
    "                    literal = expression_form\n",
    "\n",
    "                if literal in self.dictionary:\n",
    "                    is_single_token = j - i == 1  # j-1 이 1이 맞는지를  True/ False 로 입력\n",
    "                    original_position = i         # 시작값\n",
    "                    i = j\n",
    "                    taggings = [tag     for tag in self.dictionary[literal]] # value 추출\n",
    "                    tagged_expression = (expression_form, expression_lemma, taggings)\n",
    "\n",
    "                    if is_single_token:  # 위 판단이 True인 경우 \n",
    "                        original_token_tagging = sentence[original_position][2]\n",
    "                        tagged_expression[2].extend(original_token_tagging) # tag 값을 덧붙인다\n",
    "                    tag_sentence.append(tagged_expression)\n",
    "                    tagged = True\n",
    "                else:\n",
    "                    j = j - 1   # j 값을 1씩 줄이면서 위의 작업에 적합한 조건으로 조절을 해 나아간다 \n",
    "\n",
    "            # tagged 값이 기존에 없는경우, 새로 추가한다\n",
    "            if not tagged:\n",
    "                tag_sentence.append(sentence[i])\n",
    "                i += 1\n",
    "        return tag_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 긍/부정 정리된 dictionary 목록의 표현을 Counting\n",
    "def value_of(sentiment):\n",
    "    if sentiment == 'positive': return 1\n",
    "    if sentiment == 'negative': return -1\n",
    "    return 0\n",
    "\n",
    "def sentiment_score(review):\n",
    "    return sum([sentence_score(sentence, None, 0.0) for sentence in review])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 NER를 사용한 감정분석 -  176 p (on_tology)\n",
    "개체명(고유명사) 인식 Named-entity recognition (NER) - 감정식별을 위한 전처리 \n",
    "\n",
    "    token의 개체명을 별도의 기준으로 식별 후, 클래스로 분류하는 과정으로 \n",
    "    히든마르코프, 최대엔트로피 마르코프, SVM, 의사결정나무 등을 활용한다\n",
    "    개체명으로 인식되면, 감정분석에 기여하지 않으므로, 제외한 나머지들로 감정분석을 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 기계학습을 사용한 감정분석\n",
    "Twitter text 데이터에 대한 통계, 자동화, 기계학습 분류기\n",
    "\n",
    "출처 : http://www.nltk.org/book/ch06.html\n",
    "\n",
    "data : https://github.com/ravikiranj/twitter-sentiment-analyzer\n",
    "\n",
    "<img src = \"http://www.nltk.org/images/supervised-classification.png\" align='left' width = '500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 nltk의 모듈\n",
    "nltk.sentiment.sentiment_analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltk.sentiment.sentiment_analyzer() 는 기계학습 기반의 감정분석 모듈\n",
    "import nltk.sentiment.sentiment_analyzer\n",
    "from nltk.sentiment import SentimentAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 트위터 예제 분석\n",
    "https://gist.github.com/ravikiranj/2639121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 중복되는 문자를 단일로 처리\n",
    "def replaceTwoOrMore(s):\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    return pattern.sub(r\"\\1\\1\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 불용어 목록을 파일에서 읽어온다\n",
    "def getStopWordList(stopWordListFileName):\n",
    "    stopWords = []   # 불용어 목록 파일의 text를 '공백'을 기준으로 token생성 뒤, list로 출력\n",
    "    stopWords.append('AT_USER')\n",
    "    stopWords.append('URL')\n",
    "    fp = open(stopWordListFileName, 'r')\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        word = line.strip()\n",
    "        stopWords.append(word)\n",
    "        line = fp.readline()\n",
    "    fp.close()\n",
    "    return stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 여러번 반복된 단어들의 전처리\n",
    "def getFeatureVector(tweet):\n",
    "    featureVector = []\n",
    "    words = tweet.split()       # 공백을 기분으로 token 단어를 생성\n",
    "    for w in words:             \n",
    "        w = replaceTwoOrMore(w) # 2번 이상 반복된 단어를 단일로 전처리 (사용자함수)\n",
    "        w = w.strip('\\'\"?,.')   # 문장부호를 제거\n",
    "        val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", w) # 알파벳 여부 확인\n",
    "\n",
    "    # 해당 단어가 숫자/영어가 아니거나, 불용어에 해당하면 무시\n",
    "    if(w in stopWords or val is None):\n",
    "        pass # continue 는 '반복문'에서 가능\n",
    "    else:\n",
    "        featureVector.append(w.lower()) # 소문자로 변환한다\n",
    "    return featureVector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://gist.github.com/ravikiranj/2639031\n",
    "# 트위터 데이터 전처리 함수\n",
    "def processTweet(tweet):\n",
    "    tweet = tweet.lower()  # 데이터를 소문자로 바꾼다\n",
    "\n",
    "    # www.* 또는 https?://* ==> URL 로 변환\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', tweet)\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)   # @username ==> AT_USER로 변환\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)         # 공백을 제거   \n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)  # 해시태그 '#' 제거\n",
    "    tweet = tweet.strip('\\'\"')                  # triming\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@PrincessSuperC Hey Cici sweetheart! Just wanted to let u know I luv u! OH! and will the mixtape drop soon? FANTASY RIDE MAY 5TH!!!!  \\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 트위터 데이터가 .txt인 경우\n",
    "import re, csv\n",
    "stopWords = []\n",
    "\n",
    "# Tweets 데이터는 1줄씩 모든 프로세스를 진행한다\n",
    "fp = open('data/sampleTweets.txt', 'r') # Data text 파일\n",
    "line = fp.readline()\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426 ['AT_USER', 'URL', '', 'a', 'about', 'above', 'across', 'after', 'again', 'against']\n"
     ]
    }
   ],
   "source": [
    "st = open('data/stopwords.txt', 'r')    # Stop word text 파일 \n",
    "stopWords = getStopWordList('./data/stopwords.txt')\n",
    "print(len(stopWords), stopWords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "['twitter']\n",
      "[]\n",
      "['makeitcount']\n",
      "['sigh']\n",
      "['hurts']\n",
      "['hurts']\n"
     ]
    }
   ],
   "source": [
    "while line:  # 트위터 데이터 1줄씩 처리\n",
    "    processedTweet = processTweet(line)\n",
    "    featureVector = getFeatureVector(processedTweet)\n",
    "    print(featureVector)\n",
    "    line = fp.readline()\n",
    "fp.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([], 'positive'),\n",
       " ([], 'positive'),\n",
       " ([], 'positive'),\n",
       " (['twitter'], 'neutral'),\n",
       " ([], 'neutral'),\n",
       " (['makeitcount'], 'neutral'),\n",
       " (['sigh'], 'negative'),\n",
       " (['hurts'], 'negative'),\n",
       " (['hurts'], 'negative')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 트위터 데이터가 .CSV 인 경우\n",
    "# Tweets are read one by one and then processed.\n",
    "inpTweets = csv.reader(open('./data/sampleTweets.csv', 'r'), delimiter=',', quotechar='|')\n",
    "tweets = []\n",
    "for row in inpTweets:\n",
    "    sentiment = row[0]\n",
    "    tweet = row[1]\n",
    "    processedTweet = processTweet(tweet)\n",
    "    featureVector = getFeatureVector(processedTweet) #, stopWords)\n",
    "    tweets.append((featureVector, sentiment));\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 특징 추출하는 메서드\n",
    "def extract_features(tweet):\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in featureList:\n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 나이브베이즈 분류기로 감정분석\n",
    "NaiveBClassifier = nltk.NaiveBayesClassifier.train(train_sets)\n",
    "#processedTestTweet = processTweet(test_sets)\n",
    "# Testing the classifiertestTweet = 'I liked this book on Sentiment Analysis a lot.'\n",
    "# processedTestTweet = processTweet(test_sets)\n",
    "# NaiveBClassifier.classify(extract_features(getFeatureVector(processedTestTweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am so badly hurt'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testTweet = 'I am so badly hurt'\n",
    "processedTestTweet = processTweet(testTweet)\n",
    "processedTestTweet\n",
    "# NaiveBClassifier.classify(extract_features(getFeatureVector(processedTestTweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hurts']\n"
     ]
    }
   ],
   "source": [
    "featureVector = getFeatureVector(processedTweet)\n",
    "print(featureVector)\n",
    "# line = fp.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03  nltk.sentiment.sentiment_analyzer 예제\n",
    "http://www.nltk.org/howto/probability.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the movie begins in the past where a young boy named sam attempts to save celebi from a hunter . \\nemerging from the human psyche and showing characteristics of abstract expressionism , minimalism and russian constructivism , graffiti removal has secured its place in the history of modern art while being created by artists who are unconscious of their artistic achievements . \\nspurning her mother's insistence that she get on with her life , mary is thrown out of the house , rejected by joe , and e\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import subjectivity\n",
    "subjectivity.raw()[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj_docs : 100 (['smart', 'and', 'alert', ',', 'thirteen', 'conversations', 'about', 'one', 'thing', 'is', 'a', 'small', 'gem', '.'], 'subj')\n",
      "obj_docs  : 100 (['the', 'movie', 'begins', 'in', 'the', 'past', 'where', 'a', 'young', 'boy', 'named', 'sam', 'attempts', 'to', 'save', 'celebi', 'from', 'a', 'hunter', '.'], 'obj')\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "n_instances = 100\n",
    "\n",
    "# Each document is represented by a tuple (sentence, label)\n",
    "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
    "print('subj_docs :',len(subj_docs), subj_docs[0])\n",
    "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
    "print('obj_docs  :',len(obj_docs), obj_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 학습, test 데이터를 생성한다\n",
    "train_subj_docs = subj_docs[:80]\n",
    "test_subj_docs  = subj_docs[80:100]\n",
    "train_obj_docs  =  obj_docs[:80]\n",
    "test_obj_docs   =  obj_docs[80:100]\n",
    "\n",
    "training_docs = train_subj_docs + train_obj_docs\n",
    "testing_docs  = test_subj_docs + test_obj_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3799, ['smart', 'and', 'alert', ',', 'thirteen', 'conversations', 'about'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import mark_negation, extract_unigram_feats\n",
    "\n",
    "sentim_analyzer = SentimentAnalyzer()\n",
    "all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])\n",
    "len(all_words_neg), all_words_neg[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83, ['.', 'the', ',', 'a', 'and', 'of', 'to'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use simple unigram word features, handling negation:\n",
    "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
    "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams = unigram_feats)\n",
    "len(unigram_feats), unigram_feats[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n",
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy: 0.8\n",
      "F-measure [obj]: 0.8\n",
      "F-measure [subj]: 0.8\n",
      "Precision [obj]: 0.8\n",
      "Precision [subj]: 0.8\n",
      "Recall [obj]: 0.8\n",
      "Recall [subj]: 0.8\n"
     ]
    }
   ],
   "source": [
    "# We apply features to obtain a feature-value representation of our datasets:\n",
    "training_set = sentim_analyzer.apply_features(training_docs)\n",
    "test_set = sentim_analyzer.apply_features(testing_docs)\n",
    "\n",
    "# We can now train our classifier on the training set, and subsequently output the evaluation results:\n",
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentim_analyzer.train(trainer, training_set)\n",
    "# `Training classifier\n",
    "for key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
    "    print('{0}: {1}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04  nltk.sentiment.sentiment_analyzer  모듈 살펴보기\n",
    "http://www.nltk.org/howto/probability.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from __future__ import print_function\n",
    "from collections import defaultdict\n",
    "from nltk.classify.util import apply_features, accuracy as eval_accuracy\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import (BigramAssocMeasures, precision as eval_precision, \n",
    "                          recall as eval_recall, f_measure as eval_f_measure)\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.sentiment.util import save_file, timer\n",
    "\n",
    "# 기계학습에 기반한 감정분석도구\n",
    "class SentimentAnalyzer(object):\n",
    "    def __init__(self, classifier=None):\n",
    "        self.feat_extractors = defaultdict(list)\n",
    "        self.classifier = classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # 텍스트에서 모든 (중복)단어를 반환\n",
    "    def all_words(self, documents, labeled=None):\n",
    "        all_words = []\n",
    "        if labeled is None:\n",
    "            labeled = documents and isinstance(documents[0], tuple)\n",
    "        if labeled == True:\n",
    "            for words, sentiment in documents:\n",
    "                all_words.extend(words)\n",
    "        elif labeled == False:\n",
    "            for words in documents:\n",
    "                all_words.extend(words)\n",
    "        return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # 특징 추출 함수 feature extraction function\n",
    "    def apply_features(self, documents, labeled=None):\n",
    "        return apply_features(self.extract_features, documents,labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # 단어의 특징을 반환하는 코드\n",
    "    def unigram_word_feats(self, words, top_n=None, min_freq=0):\n",
    "        unigram_feats_freqs = FreqDist(word for word in words)\n",
    "        return [w    for   w, f   in   unigram_feats_freqs.most_common(top_n) \n",
    "                     if   unigram_feats_freqs[w]  >  min_freq ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # bi-gram의 특징을 반환하는 코드\n",
    "    def bigram_collocation_feats(self, documents, top_n=None, min_freq=3, assoc_measure=BigramAssocMeasures.pmi):\n",
    "        finder = BigramCollocationFinder.from_documents(documents)\n",
    "        finder.apply_freq_filter(min_freq)\n",
    "        return finder.nbest(assoc_measure, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # 사용 가능한 특징세트를 사용하여, 주어진 인스턴스를 분류\n",
    "    def classify(self, instance):\n",
    "        instance_feats = self.apply_features([instance],labeled=False)\n",
    "        return self.classifier.classify(instance_feats[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # 텍스트에서 특징 추출을 위해 사용\n",
    "    def add_feat_extractor(self, function, **kwargs):\n",
    "        self.feat_extractors[function].append(kwargs)\n",
    "\n",
    "    def extract_features(self, document):\n",
    "        all_features = {}\n",
    "        for extractor in self.feat_extractors:\n",
    "            for param_set in self.feat_extractors[extractor]:\n",
    "                feats = extractor(document, **param_set)\n",
    "            all_features.update(feats)\n",
    "        return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # 훈련데이터를 훈련시키는 함수\n",
    "    def train(self, trainer, training_set, save_classifier = None, **kwargs):\n",
    "        print(\"Training classifier\")\n",
    "        self.classifier = trainer(training_set, **kwargs)\n",
    "        if save_classifier:\n",
    "            save_file(self.classifier, save_classifier)\n",
    "        return self.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # 테스트데이터를 사용한 분류기의 테스트 및 성능평가\n",
    "    def evaluate(self, test_set, classifier = None, accuracy = True, \n",
    "                 f_measure = True, precision = True, recall = True, verbose = False):\n",
    "\n",
    "        if classifier is None:  # 분류기에 아무것도 지정하지 않은 경우\n",
    "            classifier = self.classifier  # __init__의 초깃값을 불러와서 작업을 시작\n",
    "        print(\"Evaluating {0} results...\".format(type(classifier).__name__))\n",
    "        metrics_results = {}              # 출력 report dictionary를 생성\n",
    "\n",
    "        if accuracy == True:   # 정확도 측정옵션에 True를 입력시\n",
    "            accuracy_score = eval_accuracy(classifier, test_set) # 분류기 기준, 데이터를 test한다\n",
    "            metrics_results['Accuracy'] = accuracy_score         # report dictionary에 기록 \n",
    "\n",
    "        # 출처 : https://dongyeopblog.wordpress.com/2016/04/08/python-defaultdict-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0/\n",
    "        gold_results = defaultdict(set)   # key값 지정없어도, default로 key를 자동으로 지정한다\n",
    "        test_results = defaultdict(set)   \n",
    "        labels = set()\n",
    "        for i, (feats, label) in enumerate(test_set): # test 상세내용을 기록한다\n",
    "            labels.add(label)\n",
    "            gold_results[label].add(i)\n",
    "            observed = classifier.classify(feats)\n",
    "            test_results[observed].add(i)\n",
    "\n",
    "        for label in labels:  # test 결과 수집된 labels 에 따라, 평가함수로 계산을 한다\n",
    "            if precision == True:  # 정확도 측정\n",
    "                precision_score = eval_precision(gold_results[label], test_results[label])\n",
    "                metrics_results['Precision [{0}]'.format(label)] = precision_score\n",
    "            if recall == True:     # recall 측정\n",
    "                recall_score = eval_recall(gold_results[label], test_results[label])\n",
    "                metrics_results['Recall [{0}]'.format(label)] = recall_score\n",
    "            if f_measure == True:  # f-measure 측정\n",
    "                f_measure_score = eval_f_measure(gold_results[label], test_results[label])\n",
    "                metrics_results['F-measure [{0}]'.format(label)] = f_measure_score\n",
    "            if verbose == True:    # Data를 정렬\n",
    "                for result in sorted(metrics_results):\n",
    "                    print('{0}: {1}'.format(result, metrics_results[result]))\n",
    "        return metrics_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 NER 시스템의 평가\n",
    "Evaluation of the NER system"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
