{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 - Morphology 시작하기\n",
    "행태학 Getting Our Feet Wet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 1 행태학의 이해\n",
    "Morphology: 어간과 접사(접미사, 접두사, 삽입/외접사)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 Morphology\n",
    "어간과 접사(접미사, 접두사, 삽입/외접사)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 어간 (자립형태소) : 접사를 추가하지 않아도 존재가능  ex) believe\n",
    "# 접사 (의존형태소) : 항상 자립형태소와 함깨 존재     ex) anti-, un-, -able, -ly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 Stemming의 이해\n",
    "스테머 : 단어의 접사제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n",
      "happi\n"
     ]
    }
   ],
   "source": [
    "# PorterStemmer() : 단어들의 어간 지식을 보유한다\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmerporter = PorterStemmer()\n",
    "print(stemmerporter.stem('working'))\n",
    "print(stemmerporter.stem('happiness'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n",
      "happy\n"
     ]
    }
   ],
   "source": [
    "# LancasterStemmer() : 랭커스터대학교 개발, 감정단어에 특화\n",
    "\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmerlan = LancasterStemmer()\n",
    "print(stemmerlan.stem('working'))\n",
    "print(stemmerlan.stem('happiness'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n",
      "happiness\n",
      "pair\n"
     ]
    }
   ],
   "source": [
    "# RegexpStemmer('ing') : 'ing' 어근을 찾아서 삭제 (1개만 가능)\n",
    "\n",
    "from nltk.stem import RegexpStemmer\n",
    "stemmerregexp = RegexpStemmer('ing')\n",
    "print(stemmerregexp.stem('working'))\n",
    "print(stemmerregexp.stem('happiness'))\n",
    "print(stemmerregexp.stem('pairing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n",
      "com\n",
      "mang\n"
     ]
    }
   ],
   "source": [
    "# SnowballStemmer() : 스페인어, 불어등의 외국어 어근처리\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "print(SnowballStemmer.languages)\n",
    "spanishstemmer=SnowballStemmer('spanish')\n",
    "print(spanishstemmer.stem('comiendo'))\n",
    "frenchstemmer=SnowballStemmer('french')\n",
    "print(frenchstemmer.stem('manger'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" It is an interface that helps to eliminate morphological affixes from\n",
    "the tokens and the process is known as stemming. \"\"\"\n",
    "class StemmerI(object):\n",
    "    # Eliminate affixes from token and stem is returned.\n",
    "    def stem(self, token):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok length : 435806 \n",
      "tokens is  : ['\\ufeffThe', 'Project', 'Gutenberg', 'EBook', 'of', 'An', 'Inquiry', 'into', 'the', 'Nature']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "def obtain_tokens():\n",
    "    with open('./data/adam_smith.txt') as stem: \n",
    "        tokens = nltk.word_tokenize(stem.read())\n",
    "    return tokens\n",
    "\n",
    "tok = obtain_tokens()\n",
    "print('tok length :' ,len(tok), '\\ntokens is  :', tok[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After stemming is : ['\\ufeffthe', 'project', 'gutenberg', 'ebook', 'of', 'An', 'inquiri', 'into', 'the', 'natur']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "def stemming(filtered):\n",
    "    stem=[]\n",
    "    for x in filtered:\n",
    "        stem.append(PorterStemmer().stem(x))\n",
    "    return stem\n",
    "\n",
    "stem_tokens = stemming(tok)\n",
    "# result = dict(zip(tok,stem_tokens))  \n",
    "print(\"After stemming is :\",stem_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 2  원형복원 이해\n",
    "lemmatization : 다른 단어범주의 형태로 단어를 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working\n",
      "work\n",
      "work\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer_output = WordNetLemmatizer()\n",
    "print(lemmatizer_output.lemmatize('working'))\n",
    "print(lemmatizer_output.lemmatize('working',pos='v')) # pos='v' : 음성 카테고리\n",
    "print(lemmatizer_output.lemmatize('works'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to Stemmer : happi\n",
      "to Word    : happiness\n"
     ]
    }
   ],
   "source": [
    "# to stem\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer_output = PorterStemmer()\n",
    "print('to Stemmer :' ,stemmer_output.stem('happiness'))\n",
    "\n",
    "# to lemmatize (원형복원)  \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer_output = WordNetLemmatizer()\n",
    "print('to Word    :' ,lemmatizer_output.lemmatize('happiness'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 3 비영어 언어의 스테머 개발\n",
    "Morpho에서 지원하는 언어들 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pyicu 설치로 인한 오류가 잔존\n",
    "# https://stackoverflow.com/questions/40940188/error-installing-pip-pyicu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from polyglot.downloader import downloader\n",
    "#print(downloader.supported_languages_table(\"morph2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 4 형태소 분석기\n",
    "형태소기반, 어휘기반, 단어기반\n",
    "\n",
    "http://pythonhosted.org/pyenchant/tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install pyenchant\n",
    "# 알파벳 단어 확인모듈\n",
    "\n",
    "import enchant\n",
    "s = enchant.Dict(\"en_US\")\n",
    "s.check('ness')   # 해당 단어의 적합성 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = \"itismyfavouritebook\"\n",
    "\n",
    "def tokenize(st1):\n",
    "    result, i = [], 0\n",
    "    for j in range(len(st1),-1,-1):\n",
    "        if s.check(st1[0:j]):\n",
    "            st = st1[0:1]\n",
    "            print(st)\n",
    "            #result.append(st1[i:j])\n",
    "            #i = j\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenize(\"itismyfavouritebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 5 검색엔진\n",
    "PyStemmer 1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 불용어 제거 및 토근화 함수\n",
    "Stemmer 사용자 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 문맥 중심으로 중요단어를 선별\n",
    "def eliminatestopwords(self,list):\n",
    "    return [ word for word in list if word not in self.stopwords ]\n",
    "\n",
    "# 불용어와 token을 구분\n",
    "def tokenize(self,string):\n",
    "    Str=self.clean(str)\n",
    "    Words=str.split(\"\")\n",
    "    return [self.stemmer.stem(word,0,len(word)-1) for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 키워드 벡터 차원의 매핑\n",
    "mapping keywords into vector dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 문버 벡터에서 주어진 포지션에 대한 키워드를 생성\n",
    "def obtainvectorkeywordindex(self, documentList):\n",
    "    # 텍스트를 문자열로 매핑\n",
    "    vocabstring = \"\".join(documentList)\n",
    "    vocablist = self.parser.tokenise(vocabstring)\n",
    "    # 중요성이 없는 일반단어(common words)를 삭제\n",
    "    vocablist = self.parser.eliminatestopwords(vocablist)\n",
    "    uniqueVocablist = util.removeDuplicates(vocablist)\n",
    "    vectorIndex = {}\n",
    "    offset = 0\n",
    "    # 차원/ 매칭을 수행하는 키워드의 index값 추출\n",
    "    for word in uniqueVocablist:\n",
    "        vectorIndex[word] = offset\n",
    "        offset += 1\n",
    "    return vectorIndex #(keyword:position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 심플 턴 카운트 모델\n",
    "Simple term count model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simple Term Count Model is used\n",
    "def constructVector(self, wordString):\n",
    "    # 벡터 초기화\n",
    "    Vector_val = [0] * len(self.vectorKeywordIndex)\n",
    "    tokList = self.parser.tokenize(tokString)\n",
    "    tokList = self.parser.eliminatestopwords(tokList)\n",
    "    for word in toklist:\n",
    "        vector[self.vectorKeywordIndex[word]] += 1;\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04 cos() 각을 찾아서 문사유사도 확인\n",
    "코사인 값이 1 (각도  0, 벡터 평행) : 문서 관련성 O\n",
    "\n",
    "코사인 값이 0 (각도 90, 벡터 수직) : 문서 관련성 X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cosine = ( X * Y ) / ||X|| x ||Y||\n",
    "def cosine(vec1, vec2):\n",
    "    return float(dot(vec1,vec2) / (norm(vec1) * norm(vec2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05 키워드와 벡터 공간의 매핑\n",
    "cos() 각을 찾아서 문사유사도 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 키워드와 벡터 공간의 매핑을 수행\n",
    "def searching(self,searchinglist):\n",
    "    askVector = self.buildQueryVector(searchinglist)\n",
    "    ratings = [util.cosine(askVector, textVector) \n",
    "               for textVector in self.documentVectors]\n",
    "    ratings.sort(reverse = True)\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06 소스 텍스트의 언어를 탐지하는 모델을 실행\n",
    "nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
     ]
    }
   ],
   "source": [
    "import nltk, sys\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "print(SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 다른언어여부 확인{'german': 2, 'french': 4, 'english': 1}\n",
    "# nltk.wordpunct_tokenize() : 모든 문장부호를 token으로 생성\n",
    "def _calculate_languages_ratios(text):\n",
    "    languages_ratios = {}\n",
    "    tok = wordpunct_tokenize(text)\n",
    "    words = [word.lower() for word in tok]\n",
    "    # 텍스트에서 고유 불용어의 발생을 계산한다\n",
    "    for language in stopwords.fileids():\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(words)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "        languages_ratios[language] = len(common_elements)\n",
    "    return languages_ratios # language \"score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 다른언어의 포함확률을 계산하고, 텍스트내 고유 불용어를 계산한다\n",
    "def detect_language(text):\n",
    "    ratios = _calculate_languages_ratios(text)\n",
    "    most_rated_language = max(ratios, key=ratios.get)\n",
    "    return most_rated_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = '''\n",
    "All over this cosmos, most of the people believe that there is\n",
    "an invisible supreme power that is the creator and the runner of\n",
    "this world. Human being is supposed to be the most intelligent and\n",
    "loved creation by that power and that is being searched by human\n",
    "beings in different ways into different things. As a result people\n",
    "reveal His assumed form as per their own perceptions and beliefs.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english\n"
     ]
    }
   ],
   "source": [
    "language = detect_language(text)\n",
    "print(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Artikel 26\n",
    "1. Jeder hat das Recht auf Bildung. Die Bildung ist unentgeltlich, zum mindesten \n",
    "der Grundschulunterricht und die grundlegende Bildung. Der Grundschulunterricht \n",
    "ist obligatorisch. Fach- und Berufsschulunterricht müssen allgemein verfügbar \n",
    "gemacht werden, und der Hochschulunterricht muß allen gleichermaßen entsprechend \n",
    "ihren Fähigkeiten offenstehen.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "german\n"
     ]
    }
   ],
   "source": [
    "language = detect_language(text)\n",
    "print(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = '''Suzanne et Joseph étaient nés dans les deux premières années de leur \n",
    "arrivée à la colonie. Après la naissance de Suzanne, la mère abandonna l’enseignement \n",
    "d’état. Elle ne donna plus que des leçons particulières de français. Son mari avait \n",
    "été nommé directeur d’une école indigène et, disaient-elle, ils avaient vécu très \n",
    "largement malgré la charge de leurs enfants.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "french\n"
     ]
    }
   ],
   "source": [
    "language = detect_language(text)\n",
    "print(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = '''Лорем ипсум долор сит амет, ан темпорибус сцрибентур сед, дуо ут омиттам \n",
    "форенсибус омиттантур. Но вим яуис дицо елаборарет. Ех сеа дицтас тациматес салутанди, \n",
    "яуис цоммуне фастидии ет иус, ид вих яуаестио сенсибус патриояуе. Ад инани цонсеяуат вих.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "russian\n"
     ]
    }
   ],
   "source": [
    "language = detect_language(text)\n",
    "print(language)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
