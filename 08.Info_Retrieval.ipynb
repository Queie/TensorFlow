{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8 - Information Retrieval\n",
    "정보검색 - 정보접속"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 정보검색 소개\n",
    "Introducing information retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 불용어 제거\n",
    "Stop-Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.735240435097661"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 주어진 단어중 불용어 비해당 확률\n",
    "def not_stopwords(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w    for w in text    if w.lower() not in stopwords]\n",
    "    return len(content) / len(text)\n",
    "\n",
    "not_stopwords(nltk.corpus.reuters.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens : 63 ['', 'a', 'about', 'above', 'across', 'after', 'again', 'against', 'all', 'almost']\n"
     ]
    }
   ],
   "source": [
    "import nltk ,string\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def get_tokens():\n",
    "    with open('./data/stopwords.txt') as stopl:\n",
    "        tokens = nltk.word_tokenize(stopl.read().lower().translate(string.punctuation))\n",
    "    return tokens\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "tokens = get_tokens()\n",
    "tokens = tokens[0].split('+')\n",
    "len(tokens)\n",
    "\n",
    "print(\"tokens :\", len(tokens), tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: len(count1)  62\n"
     ]
    }
   ],
   "source": [
    "count1 = Counter(tokens)\n",
    "print(\"before: len(count1) \",len(count1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered1 : ['', 'across', 'almost', 'alone', 'along']\n"
     ]
    }
   ],
   "source": [
    "filtered1 = [w for w in tokens if not w in stopwords.words('english')]\n",
    "print(\"filtered1 :\",filtered1[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after: len(count1) : 39\n"
     ]
    }
   ],
   "source": [
    "count1 = Counter(filtered1)\n",
    "print(\"after: len(count1) :\",len(count1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 2),\n",
       " ('across', 1),\n",
       " ('almost', 1),\n",
       " ('alone', 1),\n",
       " ('along', 1),\n",
       " ('already', 1),\n",
       " ('also', 1),\n",
       " ('although', 1),\n",
       " ('always', 1),\n",
       " ('among', 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count1.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltk.pos_tag(filtered1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 벡터 공간 모델을 사용한 정보 검색\n",
    "Term Frequency (용어 빈도) Inverse Document Frequency\n",
    "\n",
    "https://gist.github.com/himzzz/4105717"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 문서의 토큰화\n",
    "# 2 벡터 공간 모델의 계산\n",
    "# 3 각 문서에 대한 TF-IDF를 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$TF(t,d) = 0.5 + (0.5 * f(t,d)) / max {f(w,d) : w \\in d}$\n",
    "\n",
    "$Mark Down Signs$\n",
    "https://en.wikipedia.org/wiki/Help:Displaying_a_formula#Formatting_using_TeX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 문서에서 각 용어에 대한 TF-IDF를 계산하는 코드\n",
    "import re, nltk, math\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import bigrams, trigrams\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "tokenizer = RegexpTokenizer(\"[\\w’]+\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def freq(word, doc):\n",
    "    return doc.count(word)\n",
    "\n",
    "def word_count(doc):\n",
    "    return len(doc)\n",
    "\n",
    "def tf(word, doc):\n",
    "    return (freq(word, doc) / float(word_count(doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def num_docs_containing(word, list_of_docs):\n",
    "    count = 0\n",
    "    for document in list_of_docs:\n",
    "        if freq(word, document) > 0:\n",
    "            count += 1\n",
    "    return 1 + count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def idf(word, list_of_docs):\n",
    "    return math.log(len(list_of_docs) /\n",
    "            float(num_docs_containing(word, list_of_docs)))\n",
    "\n",
    "def tf_idf(word, doc, list_of_docs):\n",
    "    return (tf(word, doc) * idf(word, list_of_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 각 용어의 빈도를 계산한다\n",
    "vocabulary, all_tips = [], []\n",
    "docs = {}\n",
    "for tip in (['documment 1', 'documment 2']):\n",
    "    tokens = tokenizer.tokenize(tip)#.text)\n",
    "    bi_tokens = bigrams(tokens)\n",
    "    tri_tokens = trigrams(tokens)\n",
    "    tokens = [token.lower() for token in tokens if len(token) > 2]\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    bi_tokens = [' '.join(token).lower() for token in bi_tokens]\n",
    "    bi_tokens = [token for token in bi_tokens if token not in stopwords]\n",
    "    tri_tokens = [' '.join(token).lower() for token in tri_tokens]\n",
    "    tri_tokens = [token for token in tri_tokens if token not in stopwords]\n",
    "    final_tokens = []\n",
    "    final_tokens.extend(tokens)\n",
    "    final_tokens.extend(bi_tokens)\n",
    "    final_tokens.extend(tri_tokens)\n",
    "    docs[tip] = {'freq': {}, 'tf': {}, 'idf': {},\n",
    "                        'tf-idf': {}, 'tokens': []}\n",
    "    for token in final_tokens:\n",
    "        docs[tip]['freq'][token] = freq(token, final_tokens) # 각 팁에 대한 빈도를 계산\n",
    "        docs[tip]['tf'][token] = tf(token, final_tokens)     # 용어의 빈도를 계산(정규화)\n",
    "        docs[tip]['tokens'] = final_tokens\n",
    "    vocabulary.append(final_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    for token in docs[doc]['tf']:\n",
    "        docs[doc]['idf'][token]    = idf(token, vocabulary) # The Inverse-Document-Frequency\n",
    "        docs[doc]['tf-idf'][token] = tf_idf(token, docs[doc]['tokens'], vocabulary) # The tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documment 1\n",
      "documment -0.20273255405408222\n",
      "documment 1 0.0\n",
      "documment 2\n",
      "documment -0.20273255405408222\n",
      "documment 2 0.0\n"
     ]
    }
   ],
   "source": [
    "# tf-idf로 가장 관련된 단어를 찾아본다\n",
    "words = {}\n",
    "for doc in docs:\n",
    "    for token in docs[doc]['tf-idf']:\n",
    "        if token not in words:\n",
    "            words[token] = docs[doc]['tf-idf'][token]\n",
    "        else:\n",
    "            if docs[doc]['tf-idf'][token] > words[token]:\n",
    "                words[token] = docs[doc]['tf-idf'][token]\n",
    "    print (doc)\n",
    "    for token in docs[doc]['tf-idf']:\n",
    "        print (token, docs[doc]['tf-idf'][token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000000 <= documment 1\n",
      "0.000000 <= documment 2\n",
      "-0.202733 <= documment\n"
     ]
    }
   ],
   "source": [
    "for item in sorted(words.items(), key=lambda x: x[1], reverse=True):\n",
    "    print (\"%f <= %s\" % (item[1], item[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 벡터 공간 스코링 및 질의 연산자의 상호작용\n",
    "Vector space scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 텍스트의 요약\n",
    "Text summarization : 긴 텍스트의 요약을 생성한다\n",
    "\n",
    "Luhn의 The AutoMatic Creation of Literature Abstracts (1958) : Naive Summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "class Summarize_Frequency:   \n",
    "    # 텍스트 요약자를 초기화 한다 ,cut_min 보다 작거나 / cut_max 보다 크면 제외\n",
    "    def __init__(self, cut_min=0.2, cut_max=0.8):\n",
    "        self._cut_min = cut_min\n",
    "        self._cut_max = cut_max\n",
    "        self._stopwords = set(stopwords.words('english') +\n",
    "        list(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # 각 빈도를 계산, input : word_sent // output : freq, freq[w]\n",
    "    def _compute_frequencies(self, word_sent): \n",
    "        freq = defaultdict(int)\n",
    "        for s in word_sent:\n",
    "            for word in s:\n",
    "                if word not in self._stopwords:\n",
    "                    freq[word] += 1\n",
    "        # 빈도 정규화 및 필터링\n",
    "        m = float(max(freq.values()))\n",
    "        for w in freq.keys():\n",
    "            freq[w] = freq[w]/m\n",
    "            if freq[w] >= self._cut_max or freq[w] <= self._cut_min:\n",
    "                del freq[w]\n",
    "        return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # 문장(n)의 목록을 반환한다\n",
    "    def summarize(self, text, n): \n",
    "        sents = sent_tokenize(text)\n",
    "        assert n <= len(sents)\n",
    "        word_sent = [word_tokenize(s.lower()) for s in sents]\n",
    "        self._freq = self._compute_frequencies(word_sent)\n",
    "        ranking = defaultdict(int)\n",
    "        for i,sent in enumerate(word_sent):\n",
    "            for w in sent:\n",
    "                if w in self._freq:\n",
    "                    ranking[i] += self._freq[w]\n",
    "        sents_idx = self._rank(ranking, n)\n",
    "        return [sents[j] for j in sents_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # 순위가 가장 높은 첫번쨰 n 문장을 반환\n",
    "    def _rank(self, ranking, n):\n",
    "        return nlargest(n, ranking, key=ranking.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 질의 응답 시스템\n",
    "Question-answering system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['numerical',\n",
       " 'short',\n",
       " ',',\n",
       " 'intended',\n",
       " 'statistic',\n",
       " 'frequency–inverse',\n",
       " 'frequency',\n",
       " 'document',\n",
       " 'tf–idf',\n",
       " 'term']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk ,string\n",
    "from nltk import *\n",
    "# question = input()\n",
    "question = \"\"\"tf–idf, short for term frequency–inverse document frequency is \n",
    "a numerical statistic that is intended\"\"\"\n",
    "question = question.lower()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "cont = nltk.word_tokenize(question)\n",
    "analysis_keywords = list( set(cont) - set(stopwords) ) # 불용어 제거 후 token\n",
    "analysis_keywords"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
