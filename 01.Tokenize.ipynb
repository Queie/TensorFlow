{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 문자열을 사용한 작업\n",
    "Working with Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 1 토큰화\n",
    "텍스트를 token이라는 작은 부분으로 분할"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 Sentence - tokenize\n",
    "sent_tokenize() : text 를 문장으로 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Welcome readers.', 'I hope you find it interesting.', 'Please do reply.']\n"
     ]
    }
   ],
   "source": [
    "# 문장부호를 기준으로 문장을 나눈다\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text=\" Welcome readers. I hope you find it interesting. Please do reply.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02  Word tokenize\n",
    "word_tokenize() : 문장을 단어로 나누기\n",
    "\n",
    "1) TreebankWordTokenizer : Treebank를 활용해서 정교하나 느리다\n",
    "\n",
    "2) RegexpTokenizer(정규식활용) - WordPunctTokenizer - WhitespaceTokenizer : 속도 빠르지만 결과는 거칠다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) TreebankWordTokenizer 를 사용한 토큰화\n",
    "TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Have', 'a', 'nice', 'day.', 'I', 'hope', 'you', 'find', 'the', 'book', 'interesting']\n"
     ]
    }
   ],
   "source": [
    "# Penn Treebank Corpus 에 따른 기준을 사용하여, 문법별로 나눈다\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize(\"Have a nice day. I hope you find the book interesting\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\" Don't hesitate to ask questions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) RegexpTokenizer 를 사용한 토큰화\n",
    "RegexpTokenizer(\"정규식\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She', 'She']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re 모듈을 통해서도 같은 결과를 도출가능\n",
    "# re.findall() , re.split() 함수로도 구현이 가능하다\n",
    "import re\n",
    "sent = \" She secured 90.56 % in class X . She is a meritorious student\"\n",
    "re.findall('[A-Z]\\w+', sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She', 'She']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RegexpTokenizer : 정규식을 기준으로 token 변환\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "capt = RegexpTokenizer('[A-Z]\\w+')  # 대문자로 시작하는 단어를 기준으로 tokenize\n",
    "capt.tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "# WordPunctTokenizer : white-space를 정규식으로 token 생성 (빠르다)\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "print(tokenizer.tokenize(\" Don't hesitate to ask questions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) BlanklineTokenizer/ WhitespaceTokenizer 를 사용한 토큰화\n",
    "Whitespace를 기준으로 Token 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이썬 기본함수를 통해서도 같은 결과를 출력가능하다\n",
    "sent=\" She secured 90.56 % in class X . She is a meritorious student\"\n",
    "print(sent.split())     # 기본 분할기준은 WhiteSpace\n",
    "print(sent.split(' '))  # 명시적 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "sent=\" She secured 90.56 % in class X . She is a meritorious student\"\n",
    "print(WhitespaceTokenizer().tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 Token's index tuple\n",
    "token의 offset index 값을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 4), (5, 12), (13, 18), (19, 20), (21, 23), (24, 29), (30, 31), (32, 34), (35, 38), (39, 41), (42, 43), (44, 55), (56, 64)]\n"
     ]
    }
   ],
   "source": [
    "# string_span_tokenize()    cf) (30, 31) '\\n.' (32, 34) 로 2개로 인식\n",
    "\n",
    "sent=\" She secured 90.56 % in class X \\n. She is a meritorious student\\n\"\n",
    "from nltk.tokenize.util import string_span_tokenize\n",
    "print(list(string_span_tokenize(sent, \" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 4), (5, 12), (13, 18), (19, 20), (21, 23), (24, 29), (30, 31), (33, 34), (35, 38), (39, 41), (42, 43), (44, 55), (56, 63)]\n"
     ]
    }
   ],
   "source": [
    "# WhitespaceTokenizer()     cf) (30, 31) '\\n.' (33, 34) 로 1개로 앞의 \n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "print(list(WhitespaceTokenizer().span_tokenize(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 3), (1, 7), (1, 5), (1, 1), (1, 2), (1, 5), (1, 1), (2, 1), (1, 3), (1, 2), (1, 1), (1, 11), (1, 7)]\n"
     ]
    }
   ],
   "source": [
    "# spans_to_relative()   상대 offset index를 반환   cf) '\\n' (2, 1)\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize.util import spans_to_relative\n",
    "print(list(spans_to_relative(WhitespaceTokenizer().span_tokenize(sent))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 2 정규화\n",
    "전처리 작업 : 텍스트의 대문자변환, 숫자를 단어로 변환, 약어전개, text의 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 문장 부호 제거\n",
    "token내부 불필요한 부호들을 관리한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'[\\!\\\"\\#\\$\\%\\&\\\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^_\\`\\{\\|\\}\\~]',\n",
       "re.UNICODE)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장부호 목록을 Re 모듈에서 호출\n",
    "import re, string\n",
    "x = re.compile('[%s]' % re.escape(string.punctuation)); x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Food', 'was', '@', 'tasty', '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = [\" It is a pleasant evening.\",\n",
    "        \"Guests!!!, who came from ###US arrived at the venue??\",\n",
    "        \"Food was @tasty.\"]\n",
    "\n",
    "tokenized_docs = [word_tokenize(doc) for doc in text]\n",
    "tokenized_docs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['It', 'is', 'a', 'pleasant', 'evening'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Food', 'was', 'tasty']]\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for review in tokenized_docs:\n",
    "    # x의 부호 앞의 u''가 있으면 제거, 없으면 그대로 저장\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = x.sub(u'', token) \n",
    "        if not new_token == u'': new_review.append(new_token)\n",
    "    result.append(new_review)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 불용어 처리\n",
    "stop words : 문장 전체에 기여하지 않는 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'kazakh', 'norwegian', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "# 불용어 언어팩 (아쉽게도 한글은 없다)\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords : 153 \n",
      "ex) ['shan', 'very', 'do', 'how', 'than', 'but', 'does', 'once', 'up']\n"
     ]
    }
   ],
   "source": [
    "# 불용어에 비해당 단어만 출력\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "print('stopwords :', len(stops),'\\nex)', list(stops)[::18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text \n",
      " [['It', 'is', 'a', 'pleasant', 'evening'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Food', 'was', 'tasty']] \n",
      " \n",
      " Filtered :\n",
      "['It', 'pleasant', 'evening']\n",
      "['Guests', 'came', 'US', 'arrived', 'venue']\n",
      "['Food', 'tasty']\n"
     ]
    }
   ],
   "source": [
    "print('Original Text \\n' ,result,'\\n', '\\n Filtered :')\n",
    "for words in result:\n",
    "    print([word    for word in words  if word not in stops])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 3 Token의 대체\n",
    "오타 및 동의어 등을 이유로 알파벳을 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 알파벳 반복시 전처리\n",
    "오타등의 이유로 반복 알파벳(중요도가 낮은)을 축약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "class RepeatReplacer(object):\n",
    "    # oooohhhh -> (ooo)(oh)(hhh) -> (o)(oh)(h) -> (oo)(hh) -> (o)(h)\n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "        \n",
    "    def replace(self, word):\n",
    "        # wordnet에 포함단어는 그대로 출력\n",
    "        if wordnet.synsets(word): return word\n",
    "        # 그렇지 않으면 축약을 실행\n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "        # 축약후 결과를 wordnet과 비교\n",
    "        if repl_word != word:  return self.replace(repl_word)\n",
    "        else:                  return repl_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lot\n",
      "oh\n",
      "ooh\n",
      "happy\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "replacer=RepeatReplacer()\n",
    "print(replacer.replace('lotttt'))\n",
    "print(replacer.replace('ohhhhh'))\n",
    "print(replacer.replace('ooohhhhh'))\n",
    "print(replacer.replace('happy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 알파벳 반복시 전처리\n",
    "오타등의 이유로 반복 알파벳(중요도가 낮은)을 축약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 파이썬 기본명령인 .replace() 메소드를 활용\n",
    "class WordReplacer(object):\n",
    "    def __init__(self, word_map):\n",
    "        self.word_map = word_map\n",
    "\n",
    "    def replace(self, word):\n",
    "        return self.word_map.get(word, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language ToolKit\n",
      "mathmatics\n"
     ]
    }
   ],
   "source": [
    "replacer = WordReplacer({'nltk':'Natural Language ToolKit', 'math':'mathmatics'})\n",
    "print(replacer.replace('nltk'))\n",
    "print(replacer.replace('math'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 4 Text에 Zipf's 법칙 적용하기\n",
    "출현빈도 1위 단어는 1/1에 비례하고\n",
    "\n",
    "출현빈도 2위 단어는 1/2=0.5에 비례하며\n",
    "\n",
    "출현빈도 3위 단어는 1/3=0.33에 비례한다.\n",
    "\n",
    "즉 어떤 요소가 차지하는 비율은 1/K에 비례한다는 어림짐작으로 제타분포의 특별한 형태이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('she', 2),\n",
       " ('secured', 1),\n",
       " ('90.56', 1),\n",
       " ('%', 1),\n",
       " ('in', 1),\n",
       " ('class', 1),\n",
       " ('x', 1),\n",
       " ('.', 1),\n",
       " ('is', 1),\n",
       " ('a', 1),\n",
       " ('meritorious', 1),\n",
       " ('student', 1)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FreqDist() : token의 출현횟수 계산\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "sent = \" She secured 90.56 % in class X . She is a meritorious student\"\n",
    "fdist = FreqDist()\n",
    "for word in word_tokenize(sent):\n",
    "    fdist[word.lower()] += 1\n",
    "fdist.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.corpus import gutenberg\n",
    "# from nltk.probability import FreqDist\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# matplotlib.use('TkAgg')\n",
    "# fd = FreqDist()  # frequency distributions 빈도측정\n",
    "# for text in gutenberg.fileids():\n",
    "#     for word in gutenberg.words(text):\n",
    "#         #fd.ine() : Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranks ,freqs = [], []\n",
    "# for rank, word in enumerate(fd):\n",
    "#     ranks.append(rank+1)\n",
    "#     freqs.append(fd[word])\n",
    "# plt.loglog(ranks, freqs)\n",
    "# plt.xlabel('frequency(f)', fontsize=14, fontweight='bold')\n",
    "# plt.ylabel('rank(r)', fontsize=14, fontweight='bold')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 5 유사도 측정\n",
    "편집거리 알고리즘/ 자카드계수 유사척도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset : {'OTHER', 'PERSON', 'ORGANIZATION'} \n",
      "testing  : ['PERSON', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER']\n"
     ]
    }
   ],
   "source": [
    "training = 'PERSON OTHER PERSON OTHER OTHER ORGANIZATION'.split()\n",
    "testing = 'PERSON OTHER OTHER OTHER OTHER OTHER'.split()\n",
    "trainset = set(training)\n",
    "testset = set(testing)\n",
    "print('trainset :', trainset,'\\ntesting  :', testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision : 1.0\n",
      "f_measure : 0.8\n",
      "accuracy  : 0.6666666666666666\n",
      "recall    : 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from nltk.metrics import precision, accuracy, recall, f_measure\n",
    "print('precision :', precision(trainset,testset))\n",
    "print('f_measure :', f_measure(trainset,testset))\n",
    "print('accuracy  :', accuracy(training,testing))\n",
    "print('recall    :', recall(trainset,testset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 편집거리 알고리즘\n",
    "Edit Distance(Levenshtein edit distance)\n",
    "\n",
    "삭제/ 삽입 : 비용 1, 대체/ 복사 : 비용 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# 두 단어/ 문장이 같기 위해선 편집을 몇번 해야 하는지를 수치로 표시\n",
    "import nltk\n",
    "from nltk.metrics import edit_distance\n",
    "print(edit_distance(\"relate\",\"relation\"))\n",
    "print(edit_distance(\"suggestion\",\"calculation\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 자카드 계수를 사용한 유사척도\n",
    "두 set의 overlap을 통해서 유사도 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jacc_similarity(query, document):\n",
    "    first = set(query).intersection(set(document))\n",
    "    second = set(query).union(set(document))\n",
    "    return len(first)/len(second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacc_similarity(trainset, testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 이진거리 매트릭\n",
    "두 라벨의 동일성을 0 ~ 1 사이의 숫자로 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 이진거리 매트릭 함수\n",
    "def binary_distance(label1, label2):\n",
    "    return 0.0 if label1 == label2 else 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_distance(trainset, testset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
