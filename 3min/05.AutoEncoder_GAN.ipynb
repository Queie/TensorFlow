{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# <strong>Tensor Flow Deep Learning\n",
    "<strong>Unsupervised Learning\n",
    "\n",
    "<strong>Chapter 7 : 합성곱 신경망 (Convolution Neural Network)</strong>\n",
    "\n",
    "<strong>RBM(Restricted Boltzmann Machine)<strong>\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/neural_networks/plot_rbm_logistic_classification.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## <strong>1 AutoEncoder 개념\n",
    "<strong>입력값과 출력값이 동일한 신경망</strong>으로, 가운데 Node의 갯수가 입력보다 적게 설계되어\n",
    "\n",
    "입력데이터를 압축 후 복원함으로써 <strong>노이즈를 제거하는데 효과적</strong>으로 알려져 있다\n",
    "1. 변이형 오토인코더 (Variational AutoEncoder)\n",
    "1. 잡음제거 오토인코더 (Denoising AutoEncoder) \n",
    "1. 기타 다양한 오토인코더로 구성\n",
    "1. 실무에선 <strong>RBM(Restricted Boltzmann Machine)</strong>을 대신 활용하기도 한다\n",
    "1. <strong>오토인코더</strong>는 피처를 축소, <strong>RBM은 확률분포</strong>에 기반하여 visible 변수, hidden 변수간 상호작용을 파악\n",
    "1. http://khanrc.tistory.com/entry/Autoencoder-vs-RBM-vs-CNN\n",
    "\n",
    "<img src=\"http://fastforwardlabs.github.io/blog-images/miriam/imgs_code/vae.4.png\" align=\"left\" width='400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## <strong>2 AutoEncoder 구현\n",
    "<strong>MNIST 데이터를 활용\n",
    "\n",
    "(참고) Optimizers 최적화 알고리즘\n",
    "1. SGD with Momentum\n",
    "1. RMS propagation   &nbsp;&nbsp;&nbsp;<--  using in codes\n",
    "1. Adagrad\n",
    "1. Adadelta\n",
    "1. Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markbaum/Python/python36/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# 대표적인 비지도(Unsupervised) 학습 방법인 Autoencoder 를 구현해봅니다.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./data/mnist/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. 파라미터의 설정\n",
    "learning_rate = 0.01  # Activation Function의 파라미터\n",
    "training_epoch = 20   # 전체 데이터를 학습하는 총 횟수\n",
    "batch_size = 100      # 1번 학습시 호출할 이미지 데이터 숫자 (100개씩 랜덤추출)\n",
    "\n",
    "# 1) 신경망 레이어 구성 옵션\n",
    "n_hidden = 256        # 히든 레이어의 뉴런 갯수\n",
    "n_input  = 28*28      # 입력값 크기 - 이미지 픽셀수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. 신경망 모델 구성\n",
    "# 1) 입력값을 Y로 사용하므로, 별도 Y를 설정하지 않는다\n",
    "X = tf.placeholder(tf.float32, [None, n_input])\n",
    "\n",
    "# 2) 인코더 레이어와 디코더 레이어의 가중치와 편향 변수를 설정 \n",
    "# input -> encode -> decode -> output\n",
    "W_encode = tf.Variable(tf.random_normal([n_input, n_hidden]))\n",
    "b_encode = tf.Variable(tf.random_normal([n_hidden]))\n",
    "# cf) sigmoid 함수를 이용해 신경망 레이어를 구성시 : sigmoid(X * W + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3. 모델을 정의한다\n",
    "# 1) 인코더 레이어 구성\n",
    "# encode는 입력값보다 작은값으로 정보를 압축, decode 출력풋은 입력과 동일한 크기로 설정\n",
    "encoder = tf.nn.sigmoid(tf.add(tf.matmul(X, W_encode), b_encode))\n",
    "W_decode = tf.Variable(tf.random_normal([n_hidden, n_input]))\n",
    "b_decode = tf.Variable(tf.random_normal([n_input]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2) 디코더 레이어 구성 : 디코더가 최종 모델이 된다\n",
    "decoder = tf.nn.sigmoid(\n",
    "                tf.add(tf.matmul(encoder, W_decode), b_decode))\n",
    "\n",
    "# 3) 손실함수 정의\n",
    "# 디코더는 인풋과 최대한 같은 결과를 내야 하므로, 디코딩한 결과를 평가하기 위해\n",
    "# 입력 값인 X 를 실측 결과 값으로하여 decoder 와의 차이를 손실함수를 정의한다\n",
    "cost = tf.reduce_mean(tf.pow(X - decoder, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Avg.cost = 0.1906\n",
      "Epoch: 0005 Avg.cost = 0.0311\n",
      "Epoch: 0009 Avg.cost = 0.0267\n",
      "Epoch: 0013 Avg.cost = 0.0243\n",
      "Epoch: 0017 Avg.cost = 0.0232\n",
      "최적화 완료!\n"
     ]
    }
   ],
   "source": [
    "# 4. 신경망 모델 학습\n",
    "# 1) 모델학습 초기화\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "total_batch = int(mnist.train.num_examples/batch_size)\n",
    "# 2) 모델의 훈련\n",
    "for epoch in range(training_epoch):\n",
    "    total_cost = 0\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, cost_val = sess.run([optimizer, cost], feed_dict={X: batch_xs})\n",
    "        total_cost += cost_val\n",
    "    if epoch % 4 == 0:\n",
    "        print('Epoch:', '%04d'%(epoch+1),\n",
    "              'Avg.cost =', '{:.4f}'.format(total_cost / total_batch))\n",
    "print('최적화 완료!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAACNCAYAAACT6v+eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd6ea5961d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5. 모델의 성능평가\n",
    "sample_size = 10\n",
    "samples = sess.run(decoder,\n",
    "                   feed_dict = {X: mnist.test.images[ :sample_size]})\n",
    "fig, ax = plt.subplots(2, sample_size, figsize=(sample_size, 2))\n",
    "for i in range(sample_size):\n",
    "    # 원본 이미지를 출력\n",
    "    ax[0][i].set_axis_off()\n",
    "    ax[0][i].imshow(np.reshape(mnist.test.images[i], (28, 28)))\n",
    "    # AutoEncoder로 생성한 이미지를 출력\n",
    "    ax[1][i].set_axis_off()\n",
    "    ax[1][i].imshow(np.reshape(samples[i], (28, 28)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# <strong>Tensor Flow Deep Learning\n",
    "<strong>Unsupervised Learning\n",
    "\n",
    "<strong>Chapter 8 : GAN (Generative Adversarial Network)</strong>\n",
    "\n",
    "대립하는 두 신경망을 경쟁시킴으로써 결과물 생성하는 학습모듈\n",
    "1. Discriminator(구분자)에게 이미지가 진짜임을 판단하게 한다\n",
    "1. Generator(생성자)를 통해 노이즈로 부터 임의 이미지를 만들고, 이를 진짜 이미지여부를 판단한다\n",
    "1. <strong>생성자가 가짜 이미지</strong>를 만들고, <strong>구분자가 이를 구분하는 훈련</strong>을 통해서 진짜 이미지 구별능력을 향상시킨다\n",
    "\n",
    "<strong>ex) 위조지폐 범죄자와, 경찰을 경쟁시켜 완벽한 위조지폐/ 원본화폐를 찾는 학습모듈을 생성\n",
    "\n",
    "<img src=\"https://oshearesearch.com/wp-content/uploads/2016/07/mnist_gan.png\" align='left' width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## <strong>1 GNA Deep Learning\n",
    "https://arxiv.org/abs/1406.2661\n",
    "\n",
    "Generator(생성자)가 실제와 상당히 비슷한 이미지를 생성하는 능력을 키운다\n",
    "1. 이미지에서 고흐풍의 <strong>이미지를 재현</strong>\n",
    "1. 테두리만 있는 <strong>이미지를 자동채색</strong>\n",
    "1. <strong>모자이크 이미지를 복원</strong> 등에서 탁월한 성능을 나타낸다\n",
    "1. 기타 <strong>자연어 문장 생성</strong>의 연구에서도 활발한 성능을 나타낸다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## <strong>2 GNA 기본 모델의 구현\n",
    "<strong>Unsupervised 학습인 Generative Adversarial Network (GAN) 2016 을 구현한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./data/mnist/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. 파라미터 설정\n",
    "# 1) 옵션 설정\n",
    "total_epoch = 50\n",
    "batch_size = 100\n",
    "learning_rate = 0.0002\n",
    "\n",
    "# 2) 신경망 레이어 구성 옵션\n",
    "n_hidden = 256\n",
    "n_input = 28 * 28\n",
    "n_noise = 128  # 생성기의 입력값으로 사용할 노이즈의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. 신경망의 파라미터를 설정\n",
    "# 1) 입력/ 출력부분 설정\n",
    "# GAN 도 Unsupervised 학습이므로 Y 를 사용하지 않는다\n",
    "X = tf.placeholder(tf.float32, [None, n_input])\n",
    "# 대신 노이즈 Z를 추가 입력값으로 사용\n",
    "Z = tf.placeholder(tf.float32, [None, n_noise])\n",
    "\n",
    "# 2) Generator : 생성기 신경망의 변수를 설정\n",
    "G_W1 = tf.Variable(tf.random_normal([n_noise, n_hidden], stddev=0.01)) # 입력부분\n",
    "G_b1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "G_W2 = tf.Variable(tf.random_normal([n_hidden, n_input], stddev=0.01))\n",
    "G_b2 = tf.Variable(tf.zeros([n_input]))\n",
    "\n",
    "# 3) Discriminator : 판별기 신경망에 사용하는 변수를 설정\n",
    "D_W1 = tf.Variable(tf.random_normal([n_input, n_hidden], stddev=0.01))\n",
    "D_b1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "# 판별기의 최종 결과값이 진짜와 얼마나 가까운지를 판단 : Scalar\n",
    "D_W2 = tf.Variable(tf.random_normal([n_hidden, 1], stddev=0.01)) # 출력부분\n",
    "D_b2 = tf.Variable(tf.zeros([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3. 신경망의 모델을 정의한다\n",
    "# 생성기(G) 신경망 정의\n",
    "def generator(noise_z):\n",
    "    hidden = tf.nn.relu(tf.matmul(noise_z, G_W1) + G_b1)\n",
    "    output = tf.nn.sigmoid(tf.matmul(hidden, G_W2) + G_b2)\n",
    "    return output\n",
    "\n",
    "# 판별기(D) 신경망 정의\n",
    "def discriminator(inputs):\n",
    "    hidden = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)\n",
    "    output = tf.nn.sigmoid(tf.matmul(hidden, D_W2) + D_b2)\n",
    "    return output\n",
    "\n",
    "# 랜덤한 노이즈(Z)를 생성\n",
    "def get_noise(batch_size, n_noise):\n",
    "    return np.random.normal(size=(batch_size, n_noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4. 노이즈를 이용해 랜덤한 이미지를 생성\n",
    "G = generator(Z)\n",
    "# 노이즈를 이용해 생성한 이미지가 진짜 이미지인지 판별\n",
    "D_gene = discriminator(G)\n",
    "# 진짜 이미지를 이용해 판별한 결과를 출력\n",
    "D_real = discriminator(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5. 이미지 판별기/ 생성기의 코스트 함수\n",
    "# GAN 모델의 최적화는 loss_G, loss_D 를 최대로 갖는 모델을 설계한다\n",
    "# 하지만 loss_D와 loss_G는 경쟁관계로 서로 반대로 움직인다\n",
    "# 논문의 수식에서는 loss_D 를 최대화하기 위해, D_gene 를 최소화 한다\n",
    "# tf.log(D_real) : 판별기에 진짜 이미지를 넣었을 때 최대값을 출력한다\n",
    "# tf.log(1 - D_gene) : 가짜 이미지를 넣었을 때에도 최대값을 출력한다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 이것은 판별기는 생성기가 만들어낸 이미지가 가짜라고 판단하도록 판별기 신경망을 학습시킵니다.\n",
    "loss_D = tf.reduce_mean(tf.log(D_real) + tf.log(1 - D_gene))\n",
    "# loss_G 를 최대로 하기 위해서는, D_gene 도 같이 최대가 되어야 한다\n",
    "# cf) 논문에서는 loss_D 와 같은 수식으로 최소화 하는 생성기를 찾는다\n",
    "loss_G = tf.reduce_mean(tf.log(D_gene))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss_D 는 판별기 신경망에 사용되는 변수를 사용하고,\n",
    "# loss_G 는 생성기 신경망에 사용되는 변수만 사용하여 최적화를 합니다.\n",
    "D_var_list = [D_W1, D_b1, D_W2, D_b2]\n",
    "G_var_list = [G_W1, G_b1, G_W2, G_b2]\n",
    "\n",
    "# GAN 논문의 수식에 따르면 loss 를 극대화 해야하지만, minimize 하는 최적화 함수를 사용하기 때문에\n",
    "# 최적화 하려는 loss_D 와 loss_G 에 음수 부호를 붙여줍니다.\n",
    "train_D = tf.train.AdamOptimizer(learning_rate).minimize(-loss_D,\n",
    "                                                         var_list=D_var_list)\n",
    "train_G = tf.train.AdamOptimizer(learning_rate).minimize(-loss_G,\n",
    "                                                         var_list=G_var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 6. 신경망 모델의 학습\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "total_batch = int(mnist.train.num_examples/batch_size)\n",
    "loss_val_D, loss_val_G = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 D loss: -0.606 \n",
      "G loss: -1.934\n",
      "Epoch: 0005 D loss: -0.1099 \n",
      "G loss: -2.862\n",
      "Epoch: 0010 D loss: -0.1909 \n",
      "G loss: -3.102\n",
      "Epoch: 0015 D loss: -0.3385 \n",
      "G loss: -2.42\n",
      "Epoch: 0020 D loss: -0.5163 \n",
      "G loss: -2.257\n",
      "Epoch: 0025 D loss: -0.2836 \n",
      "G loss: -2.827\n",
      "Epoch: 0030 D loss: -0.5213 \n",
      "G loss: -2.422\n",
      "Epoch: 0035 D loss: -0.4926 \n",
      "G loss: -2.769\n",
      "Epoch: 0040 D loss: -0.59 \n",
      "G loss: -2.432\n",
      "Epoch: 0045 D loss: -0.4532 \n",
      "G loss: -2.273\n",
      "최적화 완료!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(total_epoch):\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        noise = get_noise(batch_size, n_noise)\n",
    "        # 판별기와 생성기 신경망을 각각 학습시킵니다.\n",
    "        _, loss_val_D = sess.run([train_D, loss_D], feed_dict={X: batch_xs, Z: noise})\n",
    "        _, loss_val_G = sess.run([train_G, loss_G], feed_dict={Z: noise})\n",
    "    if epoch % 5 == 0 :\n",
    "        print('Epoch:', '%04d' % epoch,\n",
    "              'D loss: {:.4}  \\nG loss: {:.4}'.format(loss_val_D,loss_val_G))\n",
    "\n",
    "    # 7. 학습이 되어가는 모습을 보기 위해 주기적으로 이미지를 생성하여 저장\n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        sample_size = 10\n",
    "        noise = get_noise(sample_size, n_noise)\n",
    "        samples = sess.run(G, feed_dict={Z: noise})\n",
    "        fig, ax = plt.subplots(1, sample_size, figsize=(sample_size, 1))\n",
    "        for i in range(sample_size):\n",
    "            ax[i].set_axis_off()\n",
    "            ax[i].imshow(np.reshape(samples[i], (28, 28)))\n",
    "        # samples 폴더에, 이미지 생성결과를 저장한다\n",
    "        plt.savefig('./data/gan_sample/{}.png'.format(str(epoch).zfill(3)), bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "print('최적화 완료!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## <strong>3 원하는 숫자의 이미지 생성모델 구현\n",
    "<strong>특정한 숫자 이미지를 생성하는 모델을 설계한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# GAN 모델을 이용해 원하는 손글씨 숫자를 생성하는 모델을 생성\n",
    "# (흑백 사진을 컬러로 변경, 선화의 채색등의 응용이 가능)\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./data/mnist/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. 파라미터 설정\n",
    "total_epoch = 100\n",
    "batch_size = 100\n",
    "n_hidden = 256\n",
    "n_input = 28 * 28\n",
    "n_noise = 128\n",
    "n_class = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. 신경망 모델 구성\n",
    "X = tf.placeholder(tf.float32, [None, n_input])\n",
    "Y = tf.placeholder(tf.float32, [None, n_class]) # 노이즈 추가를 위한 매개변수\n",
    "Z = tf.placeholder(tf.float32, [None, n_noise])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (1) GAN 모델의 생성 1 (생성자) : tf.variable_scope()로 생성자를 정의한다\n",
    "def generator(noise, labels):\n",
    "    with tf.variable_scope('generator') :                    \n",
    "        inputs = tf.concat([noise, labels], 1)                                # noise 에 labels 정보를 추가한다\n",
    "        hidden = tf.layers.dense(inputs, n_hidden, activation = tf.nn.relu)   # 신경망 모델 : TF 유틸리티 함수를 활용\n",
    "        output = tf.layers.dense(hidden, n_input, activation = tf.nn.sigmoid) \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (2) GAN 모델의 생성 2 (구분자) : tf.variable_scope()로 구분자를 정의한다\n",
    "# tf.variable_scope.reuse_variables() : 이전에 사용된 매개변수를 재사용 !!!\n",
    "# cf) 앞의 코드들은 '생성자', '구분자' 개별 매개변수를 사용하여 불필요한 반복을 요한다\n",
    "def discriminator(inputs, labels, reuse = None):\n",
    "    with tf.variable_scope('discriminator') as scope:\n",
    "        if reuse:               \n",
    "            scope.reuse_variables()\n",
    "        inputs = tf.concat([inputs, labels], 1)\n",
    "        hidden = tf.layers.dense(inputs, n_hidden, activation = tf.nn.relu)\n",
    "        output = tf.layers.dense(hidden, 1, activation = None)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (3) 노이즈를 균등분포로 생성한다\n",
    "def get_noise(batch_size, n_noise):\n",
    "    return np.random.uniform(-1., 1., size=[batch_size, n_noise])\n",
    "\n",
    "# '임의생성 이미지 데이터'의 '구분자'를 하나씩 정의한다\n",
    "# generator() : 매개변수 생성\n",
    "# discriminator() : 생성, 판별모델에 Y (labels 정보)를 추가한다 (labels 해당 이미지를 생성도록 유도)\n",
    "# discriminator(  , reuse = True) : '구분자'의 변수들을 재사용\n",
    "G = generator(Z, Y)\n",
    "D_real = discriminator(X, Y)\n",
    "D_gene = discriminator(G, Y, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3. 손실함수 (GAN 논문과는 변형을 주었다) - http://bamos.github.io/2016/08/09/deep-completion/\n",
    "# D_real (진짜를 구분)는 1에 가깝도록, D_gene (가짜를 구분)는 0에 가깝도록 한다\n",
    "loss_D_real = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                        logits=D_real, labels=tf.ones_like(D_real)))\n",
    "loss_D_gene = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                        logits=D_gene, labels=tf.zeros_like(D_gene)))\n",
    "\n",
    "# loss_D_real 과 loss_D_gene 를 더한 값을, 최소화로 하는 최적값을 계산한다\n",
    "loss_D = loss_D_real + loss_D_gene \n",
    "# '생성자' 학습을 위한 손실함수를 정의한다 (D_gene 를 1에 가깝도록 손실함수를 정의)\n",
    "loss_G = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                        logits=D_gene, labels=tf.ones_like(D_gene)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4. 개별 매개변수와 함수를 종합하여 모델을 완성한다\n",
    "# tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='') : 유틸리티 함수로 변수들을 쉽게 가져올 수 있다.\n",
    "vars_D = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='discriminator')\n",
    "train_D = tf.train.AdamOptimizer().minimize(loss_D, var_list=vars_D)\n",
    "vars_G = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator')\n",
    "train_G = tf.train.AdamOptimizer().minimize(loss_G, var_list=vars_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 D loss: 0.004718 \n",
      "G loss: 7.511\n",
      "Epoch: 0001 D loss: 0.005956 \n",
      "G loss: 6.989\n",
      "Epoch: 0002 D loss: 0.003316 \n",
      "G loss: 9.771\n",
      "Epoch: 0003 D loss: 0.001612 \n",
      "G loss: 9.064\n",
      "Epoch: 0004 D loss: 0.002711 \n",
      "G loss: 10.04\n",
      "Epoch: 0005 D loss: 0.002952 \n",
      "G loss: 10.48\n",
      "Epoch: 0006 D loss: 0.008215 \n",
      "G loss: 13.08\n",
      "Epoch: 0007 D loss: 0.0004396 \n",
      "G loss: 11.48\n",
      "Epoch: 0008 D loss: 9.919e-05 \n",
      "G loss: 14.86\n",
      "Epoch: 0009 D loss: 4.344e-06 \n",
      "G loss: 15.24\n",
      "Epoch: 0010 D loss: 6.894e-07 \n",
      "G loss: 16.6\n",
      "Epoch: 0011 D loss: 3.969e-06 \n",
      "G loss: 16.5\n",
      "Epoch: 0012 D loss: 7.196e-06 \n",
      "G loss: 18.12\n",
      "Epoch: 0013 D loss: 2.113e-07 \n",
      "G loss: 18.65\n",
      "Epoch: 0014 D loss: 1.36e-07 \n",
      "G loss: 17.39\n",
      "Epoch: 0015 D loss: 2.18e-07 \n",
      "G loss: 18.33\n",
      "Epoch: 0016 D loss: 2.403e-06 \n",
      "G loss: 18.66\n",
      "Epoch: 0017 D loss: 3.006e-08 \n",
      "G loss: 19.05\n",
      "Epoch: 0018 D loss: 1.003e-07 \n",
      "G loss: 17.97\n",
      "Epoch: 0019 D loss: 7.549e-08 \n",
      "G loss: 18.67\n",
      "Epoch: 0020 D loss: 9.887e-08 \n",
      "G loss: 17.04\n",
      "Epoch: 0021 D loss: 8.253e-05 \n",
      "G loss: 15.23\n",
      "Epoch: 0022 D loss: 3.538e-05 \n",
      "G loss: 16.25\n",
      "Epoch: 0023 D loss: 1.819e-05 \n",
      "G loss: 15.2\n",
      "Epoch: 0024 D loss: 4.562e-10 \n",
      "G loss: 25.45\n",
      "Epoch: 0025 D loss: 1.041e-06 \n",
      "G loss: 15.96\n",
      "Epoch: 0026 D loss: 2.456e-06 \n",
      "G loss: 15.3\n",
      "Epoch: 0027 D loss: 1.683e-07 \n",
      "G loss: 16.27\n",
      "Epoch: 0028 D loss: 1.088e-07 \n",
      "G loss: 16.99\n",
      "Epoch: 0029 D loss: 5.43e-08 \n",
      "G loss: 17.68\n",
      "Epoch: 0030 D loss: 1.244e-07 \n",
      "G loss: 17.26\n",
      "Epoch: 0031 D loss: 1.641e-05 \n",
      "G loss: 18.71\n",
      "Epoch: 0032 D loss: 2.212e-05 \n",
      "G loss: 20.65\n",
      "Epoch: 0033 D loss: 7.432e-07 \n",
      "G loss: 34.86\n",
      "Epoch: 0034 D loss: 8.613e-06 \n",
      "G loss: 15.12\n",
      "Epoch: 0035 D loss: 5.375e-06 \n",
      "G loss: 17.02\n",
      "Epoch: 0036 D loss: 7.46e-05 \n",
      "G loss: 14.44\n",
      "Epoch: 0037 D loss: 2.57e-06 \n",
      "G loss: 14.49\n",
      "Epoch: 0038 D loss: 2.199e-06 \n",
      "G loss: 19.65\n",
      "Epoch: 0039 D loss: 0.004298 \n",
      "G loss: 8.291\n",
      "Epoch: 0040 D loss: 2.755e-07 \n",
      "G loss: 16.92\n",
      "Epoch: 0041 D loss: 2.407e-07 \n",
      "G loss: 16.05\n",
      "Epoch: 0042 D loss: 3.101e-07 \n",
      "G loss: 16.06\n",
      "Epoch: 0043 D loss: 1.516e-07 \n",
      "G loss: 16.43\n",
      "Epoch: 0044 D loss: 5.063e-08 \n",
      "G loss: 17.28\n",
      "Epoch: 0045 D loss: 4.526e-07 \n",
      "G loss: 17.51\n",
      "Epoch: 0046 D loss: 6.364e-09 \n",
      "G loss: 19.56\n",
      "Epoch: 0047 D loss: 1.366e-08 \n",
      "G loss: 18.98\n",
      "Epoch: 0048 D loss: 9.898e-10 \n",
      "G loss: 21.77\n",
      "Epoch: 0049 D loss: 1.138e-09 \n",
      "G loss: 21.89\n",
      "Epoch: 0050 D loss: 3.966e-10 \n",
      "G loss: 22.31\n",
      "Epoch: 0051 D loss: 2.927e-09 \n",
      "G loss: 21.11\n",
      "Epoch: 0052 D loss: 4.367e-10 \n",
      "G loss: 23.46\n",
      "Epoch: 0053 D loss: 3.316e-10 \n",
      "G loss: 23.41\n",
      "Epoch: 0054 D loss: 3.652e-10 \n",
      "G loss: 23.14\n",
      "Epoch: 0055 D loss: 2.958e-10 \n",
      "G loss: 24.42\n",
      "Epoch: 0056 D loss: 8.486e-11 \n",
      "G loss: 24.83\n",
      "Epoch: 0057 D loss: 1.54e-08 \n",
      "G loss: 24.24\n",
      "Epoch: 0058 D loss: 6.525e-06 \n",
      "G loss: 14.76\n",
      "Epoch: 0059 D loss: 4.084e-08 \n",
      "G loss: 19.19\n",
      "Epoch: 0060 D loss: 1.149e-05 \n",
      "G loss: 18.89\n",
      "Epoch: 0061 D loss: 1.634e-06 \n",
      "G loss: 16.06\n",
      "Epoch: 0062 D loss: 4.556e-07 \n",
      "G loss: 16.29\n",
      "Epoch: 0063 D loss: 1.996e-08 \n",
      "G loss: 18.06\n",
      "Epoch: 0064 D loss: 3.731e-08 \n",
      "G loss: 18.7\n",
      "Epoch: 0065 D loss: 1.665e-08 \n",
      "G loss: 18.6\n",
      "Epoch: 0066 D loss: 6.384e-09 \n",
      "G loss: 19.45\n",
      "Epoch: 0067 D loss: 3.857e-08 \n",
      "G loss: 18.16\n",
      "Epoch: 0068 D loss: 9.362e-11 \n",
      "G loss: 43.81\n",
      "Epoch: 0069 D loss: 8.919e-08 \n",
      "G loss: 17.6\n",
      "Epoch: 0070 D loss: 1.213e-08 \n",
      "G loss: 18.95\n",
      "Epoch: 0071 D loss: 2.246e-07 \n",
      "G loss: 16.77\n",
      "Epoch: 0072 D loss: 1.085e-07 \n",
      "G loss: 17.37\n",
      "Epoch: 0073 D loss: 3.662e-16 \n",
      "G loss: 50.47\n",
      "Epoch: 0074 D loss: 7.788e-09 \n",
      "G loss: 21.14\n",
      "Epoch: 0075 D loss: 4.94e-08 \n",
      "G loss: 74.21\n",
      "Epoch: 0076 D loss: 1.161e-08 \n",
      "G loss: 21.23\n",
      "Epoch: 0077 D loss: 6.105e-09 \n",
      "G loss: 20.26\n",
      "Epoch: 0078 D loss: 6.091e-11 \n",
      "G loss: 26.26\n",
      "Epoch: 0079 D loss: 3.915e-09 \n",
      "G loss: 22.06\n",
      "Epoch: 0080 D loss: 2.36e-09 \n",
      "G loss: 22.12\n",
      "Epoch: 0081 D loss: 2.666e-09 \n",
      "G loss: 22.57\n",
      "Epoch: 0082 D loss: 3.994e-08 \n",
      "G loss: 19.53\n",
      "Epoch: 0083 D loss: 1.076e-08 \n",
      "G loss: 20.38\n",
      "Epoch: 0084 D loss: 5.676e-08 \n",
      "G loss: 17.73\n",
      "Epoch: 0085 D loss: 2.886e-09 \n",
      "G loss: 20.44\n",
      "Epoch: 0086 D loss: 1.368e-09 \n",
      "G loss: 21.15\n",
      "Epoch: 0087 D loss: 2.595e-07 \n",
      "G loss: 18.13\n",
      "Epoch: 0088 D loss: 8.524e-09 \n",
      "G loss: 18.99\n",
      "Epoch: 0089 D loss: 5.653e-15 \n",
      "G loss: 42.15\n",
      "Epoch: 0090 D loss: 3.953e-08 \n",
      "G loss: 18.91\n",
      "Epoch: 0091 D loss: 2.683e-07 \n",
      "G loss: 16.96\n",
      "Epoch: 0092 D loss: 3.451e-09 \n",
      "G loss: 21.79\n",
      "Epoch: 0093 D loss: 1.604e-08 \n",
      "G loss: 19.68\n",
      "Epoch: 0094 D loss: 9.206e-08 \n",
      "G loss: 17.83\n",
      "Epoch: 0095 D loss: 1.43e-17 \n",
      "G loss: 72.8\n",
      "Epoch: 0096 D loss: 1.68e-08 \n",
      "G loss: 25.66\n",
      "Epoch: 0097 D loss: 1.011e-05 \n",
      "G loss: 18.09\n",
      "Epoch: 0098 D loss: 3.303e-07 \n",
      "G loss: 16.69\n",
      "Epoch: 0099 D loss: 1.496e-07 \n",
      "G loss: 17.57\n",
      "최적화 완료!\n"
     ]
    }
   ],
   "source": [
    "# 5. 신경망 모델 학습 (running Session)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "loss_val_D, loss_val_G = 0, 0\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        noise = get_noise(batch_size, n_noise)\n",
    "        _, loss_val_D = sess.run([train_D, loss_D],\n",
    "                                 feed_dict={X: batch_xs, Y: batch_ys, Z: noise})\n",
    "        _, loss_val_G = sess.run([train_G, loss_G],\n",
    "                                 feed_dict={Y: batch_ys, Z: noise})\n",
    "    print('Epoch:', '%04d' % epoch, 'D loss: {:.4} \\nG loss: {:.4}'.format(loss_val_D,loss_val_G))\n",
    "\n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:     # 학습과정을 기록, 주기적으로 레이블에 이미지를 저장\n",
    "        sample_size = 10\n",
    "        noise = get_noise(sample_size, n_noise)\n",
    "        samples = sess.run(G,\n",
    "                           feed_dict={Y: mnist.test.labels[:sample_size],\n",
    "                                      Z: noise})\n",
    "        \n",
    "        fig, ax = plt.subplots(2, sample_size, figsize=(sample_size, 2))\n",
    "        for i in range(sample_size):\n",
    "            ax[0][i].set_axis_off()\n",
    "            ax[1][i].set_axis_off()\n",
    "            ax[0][i].imshow(np.reshape(mnist.test.images[i], (28, 28)))\n",
    "            ax[1][i].imshow(np.reshape(samples[i], (28, 28)))\n",
    "        plt.savefig('./data/gan_sample2/{}.png'.format(str(epoch).zfill(3)), bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "print('최적화 완료!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
