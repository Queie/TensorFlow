{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chapter 4 - Web Mining Techniques\n",
    "웹 마이닝 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 1 웹 구조 마이닝\n",
    "Web structure mining\n",
    "\n",
    "### 01 웹 크롤러 - Spyder\n",
    "### 02 인덱서- DB\n",
    "### 03 페이지 랭킹 알고리즘 - Ranking (명성)\n",
    "page I's in-link : 외부 페이지에서 I page 연결 수\n",
    "\n",
    "page I's out-link : 페이지 I 가 연결하는 외부페이지 수\n",
    "1. P(i) : i 페이지 방문확률\n",
    "2. P(j) : j 페이지 방문확률\n",
    "3. Aji: 노트 j에서 노드 i로 전이될 확률 \n",
    "$$ P(i) = \\sum_j A_{ji} P(j) $$\n",
    "\n",
    "### 04 rank sing (랭크싱크) \n",
    "서로 링크한 경우에는 Loop에 빠져서, 랜덤한 점프 전이행렬을 추가\n",
    "1. (1-d) 페이지를 임의로 방문할 확률\n",
    "2. A는 확룰로써 총 합은 1이다\n",
    "$$ P = (\\frac{(1-d)E}{N}+dA^T)P $$\n",
    "3. 아래의 식의 P벡터를 $e^T P=N $으로 정규화를 하면 식이 간단해진다\n",
    "$$ P = (1-d)e + dA^TP \\to P(i) = (1-d)+d\\sum_{j=1}A_{ji}P(j) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 2 웹 콘텐츠 마이닝\n",
    "### 01 Parsing\n",
    "파싱\n",
    "### 02 자연어 처리\n",
    "NLTK\n",
    "\n",
    "Stemming : 단어를 어간으로 축소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"\"\"On 1 January 2007, Irish became a full EU official language, \n",
    "with a temporary derogation for a renewable period of five years (see Council \n",
    "Regulation (EC) No 920/2005 of 13 June 2005 (OJ L 156, 18.6.2005, p. 3)) stating \n",
    "that 'the institutions of the European Union shall not be bound by the obligation\n",
    "to draft all acts in Irish and to publish them in that language in the Official \n",
    "Journal of the European Union \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words length :  91 \n",
      "\n",
      " ['On', '1', 'January', '2007', ',', 'Irish', 'became', 'a', 'full', 'EU', 'official', 'language', ',', 'with', 'a', 'temporary', 'derogation', 'for', 'a', 'renewable']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tknzr = WordPunctTokenizer()\n",
    "\n",
    "words = tknzr.tokenize(text)\n",
    "print('Words length : ', len(words), '\\n\\n', words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean words length :  64 \n",
      "\n",
      " ['on', '1', 'january', '2007', ',', 'irish', 'became', 'full', 'eu', 'official', 'language', ',', 'temporary', 'derogation', 'renewable']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "words_clean = [ w.lower()   for w in words   if w not in stopwords]\n",
    "print('Clean words length : ', len(words_clean), '\\n\\n', words_clean[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean words stem length :  64 \n",
      "\n",
      " ['on', '1', 'januari', '2007', ',', 'irish', 'becam', 'full', 'eu', 'offici', 'languag', ',', 'temporari', 'derog', 'renew']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words_clean_stem = [stemmer.stem(w)  for w in words_clean]\n",
    "print('Clean words stem length : ', len(words_clean_stem), '\\n\\n', words_clean_stem[:15])\n",
    "# token 의 갯수는 동일, 접사를 제거한 나머지를 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 3 정보 검색 모델\n",
    "Information retrieval models\n",
    "\n",
    "모델목록 : Boolean Model(불리언), Vector space model(벡터), Probabilistic model(확률)\n",
    "\n",
    "Vector 모델 목록\n",
    "1. TF-IDF(Term Frequency-Inverse Document Frequency) 단어빈도 - 역문서 빈도\n",
    "2. LSA(Latent Semantic Analysis) 잠재의미분석\n",
    "3. Doc2Vec (Word2Vec)\n",
    "\n",
    "쿼리 단어간의 벡터로 표현하고, <strong>쿼리 벡터</strong>와 <strong>각 문서</strong>간의 <strong>$\\cos \\theta$ 유사도</strong> 를 측정, 비교한다 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 TF-IDF\n",
    "아주 <strong>많은 문서에 빈번</strong>하게 등장하는 단어는 <strong>중요도가 낮고</strong>, \n",
    "<strong>일부문서에 빈번</strong>한 단어의 <strong>중요도가 높다</strong>\n",
    "$$ W_{ij} = tf_{ij}*idf_j $$\n",
    "$ tf_{ij} = \\frac {f_{ij}}{max f_{i1}...f_{iV}}$  문서 i에서 <strong>단어 j의 정규화 빈도</strong>\n",
    "\n",
    "$ idf_j = log\\frac{N}{df} $ <strong>역문서 빈도</strong>로 $idf_j$는 단어 j를 포함하는 웹페이지 갯수 (N은 전체 웹페이지 갯수)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 잠재 의미 분석(LSA)\n",
    "단어와 문서를 효과적으로 설명할 수 있는 정형적인 잠재공간이 존재하고, 비슷한 의미의 단어는 비슷한 위치에 나타난다\n",
    "\n",
    "문서의 잠재공간 투영은 <strong>절단SVD</strong> (Singular Value Decomposition)를 이용한다\n",
    "$$ X = U_t \\sum_t V_t^T $$\n",
    "$U_t(V*t)$ : t 차원의 잠재공간에 투영된 단어의 행렬\n",
    "\n",
    "$\\sum_t V_t^T $ : 잠재공간에 투영된 문서의 전치행렬\n",
    "\n",
    "$\\sum_{t(t*t)}$ : 특기앖으로 이루어진 대각행렬\n",
    "$$ q_t = q^TU_t\\sum_t^{-1}$$ 쿼리벡터도 잠재공간에 투영된다\n",
    "\n",
    "각 문서는 <strong>$q_t^T$</strong>와 <strong>$\\cos\\theta$유사도</strong>로 비교될 수 있지만, \n",
    "\n",
    "실제는 $V_t^T$와 $q_t^T$간의 <strong>유사도</strong>로 계산한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 Word2Vec\n",
    "두가지 아키텍처 알고리즘 중 하나를 선택하려, N개의 뉴런(Neuron)(가중치)으로 된 Hidden Layer를 훈련시킨다.\n",
    "\n",
    "즉 N개의 뉴런(가중치)h를 갖는 단일계층 학습으로 얕은학습(shallow learning)방식이다 \n",
    "\n",
    "행렬W는 입력벡터를 hidden layer로 변환하며, 출력계층에서는 목표를 평가한다\n",
    "\n",
    "train단계에서 W와 W\"를 수정하기 위해서 확률적 내리막 경사법(Stochastic Gradient Descent)를 사용한다\n",
    "\n",
    "https://shuuki4.wordpress.com/2016/01/27/word2vec-%EA%B4%80%EB%A0%A8-%EC%9D%B4%EB%A1%A0-%EC%A0%95%EB%A6%AC/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><strong>연속단어 주머니 CBOW (Continuous Bag Of Words)</strong></h4>\n",
    "<h5>ex) 편의점에서 아이스크림을 사 먹었는데, ___ 시려서 힘들었다. (의 빈칸에 알맞은 단어찾기)</h5>\n",
    "주어진 단어 앞 뒤 C/2개 씩 <strong>총 C개 단어를 Input</strong>으로 사용, \n",
    "\n",
    "<strong>주어진 단어를 맞추기 위한 네트워크</strong>를 만든다\n",
    "<h4>Input Layer 는</h4> 모든 단어들이 공통적으로 사용하는 VxN \n",
    "(N:사용벡터의 길이(Projection Layer길이)) 크기의 Projection Matrix $W$가 있고\n",
    "<h4>Projection Layer에서 Output Layer로 갈 때는</h4> NxV 크기의 Weight Matrix W’ 가 있다.  \n",
    "(주의해야할 점은, 두 행렬은 transpose이 아닌, 별개의 행렬이라는 점이다)\n",
    "<h4>Process 정리</h4>\n",
    "1. 즉 단어들을 one-hot encoding으로 입력\n",
    "2. 여러단어를 각각 projection 시킨 후, 벡터들의 평균을 구해서 Projection Layer에 보낸다. \n",
    "3. Weight Matrix를 곱해서 Output Layer로 보내고 softmax 계산을 한 후, \n",
    "4. 이 결과를 진짜 단어의 one-hot encoding과 비교하여 에러를 계산한다.\n",
    "\n",
    "<h4>CBOW 모델에서 하나의 단어 처리 계산량</h4>\n",
    "1. C개의 단어를 Projection 하는 데에 C x N\n",
    "2. Projection Layer에서 Output Layer로 가는 데에 N x V\n",
    "전체 계산량은 CxN + NxV가 되는데, V를 ln V로 줄이는 테크닉을 사용하면 \n",
    "\n",
    "전체 계산량이 CxN + N x lnV가 된다.\n",
    "\n",
    "결국 <strong>Projection Layer의 크기 N</strong>과 <strong>log-사전크기의 lnV</strong>의 크기의 곱에 비례하게 된다.\n",
    "\n",
    "C=10, N=500, V=1,000,000으로 잡아도 500 x (10+ln(1,000,000)) = 약 10000의 계산량밖에 들지 않는 것이다. \n",
    "\n",
    "이는 앞서 확인한 NNLM이나 RNNLM에 비해 정말 엄청나게 줄어든 계산량이라는 것을 확인할 수 있다.\n",
    "\n",
    "<img src=\"https://shuuki4.files.wordpress.com/2016/01/cbow.png?w=260&h=300\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><strong>스킵그램(Skip-gram)</strong></h4>\n",
    "\n",
    "주어진 단어의, 주위에 등장하는 나머지 몇 가지의 단어들의 등장 여부를 유추하는 것으로\n",
    "\n",
    "‘가까이 위치한 단어일 수록 관련이 더 많다’ 라는 생각을 적용하기 위해 \n",
    "\n",
    "\n",
    "멀리 떨어져있는 단어일수록 낮은 확률로 택하는 방법을 사용한다. \n",
    "\n",
    "나머지 구조는 CBOW와 방향만 반대일 뿐 굉장히 유사하다\n",
    "\n",
    "<h4>Skip-gram에서 하나의 단어를 처리하는 데에 드는 계산량</h4>\n",
    "\n",
    "1. C개의 단어를 샘플링했다고 할 때, 현재 단어를 Projection 하는 데에 N\n",
    "2. Output을 계산하는 데에 N x V, 테크닉을 사용하면 N x ln V\n",
    "3. 총 C개의 단어에 대해 진행해야 하므로 총 C배\n",
    "4. 총 C(N + N x lnV) 만큼의 연산이 필요하다. \n",
    "\n",
    "CBOW 모델같이 N x lnV에 비례하는 계산량을 가진 모델이기는 하지만, \n",
    "\n",
    "샘플링 단어 갯수에 따라서 계산량이 비례하여 올라가게 되므로 CBOW 비해서는 느리다\n",
    "\n",
    "실험 결과에서는 Skip-gram이 CBOW에 비해 전체적으로 다소 좋은 결과를 내는 추세를 보인다\n",
    "\n",
    "<img src=\"https://shuuki4.files.wordpress.com/2016/01/skip-gram.png?w=276&h=300\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04 Doc2Vec\n",
    "단어 $W_j$를 벡터 $V_w$로 표현하며, 단어가 나타나는 문서 $d_i$와는 독립적 벡터로 정의한다\n",
    "\n",
    "Word2Vec알고리즘을 <strong>신경망</strong>과 <strong>역전파</strong>를 확장 이용한 것으로\n",
    "\n",
    "문서를 단어의 벡터로 추가한 모델이다\n",
    "<h4><strong>분산 메모리 모델 (DM) Distributed Memory model</strong>로써</h4>\n",
    "문서 <strong>D</strong>벡터로 수집한 단어들이 학습모델과 공유되며, \n",
    "\n",
    "W와 W\"는 모든 문서에 공통으로 연산하게 되어서 최적값을 찾는다 \n",
    "<h4><strong>분산 단어 주머니(DBOW) distributed bag of words</strong> </h4>\n",
    "https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "\n",
    "입력계층에서는 문서 벡터만 고려하고, 출력계층은 문서 샘플링 Context의 단어 집합만 고려한다\n",
    "\n",
    "\n",
    "DM아키텍처는 Gensim 라이브러리에 기본모듈로써 구현되어 있다\n",
    "<h5>(아래는 3개 단어로 된 컨텍스트를 찾는 분산 메모리 모델의 예제이다)</h5>\n",
    "<img src=\"https://research.google.com/archive/paragraph_vector_icml2014/distributed_memory_model.png\" align=\"left\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br></br>\n",
    "## 4 영화 리뷰 쿼리분석 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 데이터 불러오기\n",
    "bs4 옵션설정 https://www.pythonanywhere.com/forums/topic/7002/  \n",
    "\n",
    "open() utf-8 오류설정 https://stackoverflow.com/questions/20578270/unicodedecodeerror-in-python-while-reading-utf-8-sql-file-from-english-wikipedia  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "movie_html_dir = './data/movie/'\n",
    "movie_dict = {} # 데이터 파일 목록 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web page : 27886 \n",
      "Crawling time : 37.3517\n"
     ]
    }
   ],
   "source": [
    "filenames = [f    for f in os.listdir(movie_html_dir)      if f[0]!='.'] ; t0 = time()\n",
    "for file in filenames:\n",
    "    id_html = file.split('.')[0]\n",
    "    # UTF-8 오류발생시 외부 메세지 처리없이 결과를 출력한다\n",
    "    f = open(movie_html_dir + file, encoding = \"utf8\", errors = 'replace')\n",
    "    s = f.read()    \n",
    "    parsed_html = BeautifulSoup(s,\"lxml\")\n",
    "    try:     title = parsed_html.body.h1.text\n",
    "    except:  title = 'none'\n",
    "    movie_dict[id_html] = title\n",
    "print('Web page :', len(filenames), '\\nCrawling time :',round(time()-t0, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 긍/부정 Text 사전처리\n",
    "nltk_movie_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/markbaum/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 모듈호출 및 불용어 목록 불러오기\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tknzr = WordPunctTokenizer()\n",
    "nltk.download('stopwords')\n",
    "stoplist = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 어간처리 모듈 불러오기\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def ListDocs(dirname):\n",
    "    docs, titles  = [], []\n",
    "    filenames = [f for f in os.listdir(dirname) if str(f)[0]!='.']\n",
    "    for filename in filenames:\n",
    "        f = open(dirname+'/'+filename,'r')\n",
    "        id_html = filename.split('.')[0].split('_')[1]\n",
    "        titles.append(movie_dict[id_html])\n",
    "        docs.append(f.read())\n",
    "    return docs, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'david schwimmer ( from the television series \" friends \" ) stars as a sensitive ( and slightly neurotic ) single guy who gets more than he expected from the grieving mother ( barbara hershey ) of a classmate he can\\'t remember . \\nhello mrs . robinson ! \\nthough quite cute as a romantic comedy , the pallbearer is paced like a funeral march . \\nthe characters act , react , and interact at half-speed , making for one * excruciatingly * long sit . \\n ( and what\\'s with the dreary lighting ? ) \\nco-writer/'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 리뷰 텍스트 모두 메모리에 불러오기\n",
    "dir = '/home/markbaum/nltk_data/corpora/movie_reviews/'\n",
    "pos_textreviews, pos_titles = ListDocs(dir+'pos/')\n",
    "neg_textreviews, neg_titles = ListDocs(dir+'neg/')\n",
    "tot_textreviews = pos_textreviews + neg_textreviews\n",
    "tot_titles = pos_titles + neg_titles\n",
    "\n",
    "neg_textreviews[2][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 저장 데이터 목록 \n",
    "# pos_titles\n",
    "# neg_titles\n",
    "# tot_titles\n",
    "\n",
    "# pos_textreviews\n",
    "# neg_textreviews\n",
    "# tot_textreviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 TF-IDF 구현하기 sklearn 모듈 활용\n",
    "TF-IDF model can be trained using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text 전처리 함수\n",
    "def Preprocess_Tf_idf(texts, stoplist = [], stem = False):\n",
    "    newtexts = []\n",
    "    for text in texts:\n",
    "        # 불용어 제거하기\n",
    "        if stem:   tmp = [w for w in tknzr.tokenize(text) if w not in stoplist]\n",
    "        # 유효한 단어 어간 처리 (접사 제거)\n",
    "        else:      tmp = [stemmer.stem(w) for w in [w for w in tknzr.tokenize(text) if w not in stoplist]]\n",
    "        newtexts.append(' '.join(tmp))\n",
    "    return newtexts\n",
    "\n",
    "# neg/pos 영화리뷰 데이터 전처리 하기\n",
    "processed_reviews = Preprocess_Tf_idf(tot_textreviews, stoplist, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "{ 00 : 6.40417775725 }\n",
      "{ 000 : 4.06880284143 }\n",
      "{ 0009f : 7.90825515402 }\n",
      "{ 007 : 6.29881724159 }\n",
      "{ 00s : 7.90825515402 }\n",
      "{ 03 : 7.90825515402 }\n"
     ]
    }
   ],
   "source": [
    "# tf-idf 모델 테스트 하기\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 1)            # tf-idf 모듈 활성화\n",
    "mod_tfidf = vectorizer.fit(processed_reviews)       # tf-idf 데이터 적용\n",
    "vec_tfidf = mod_tfidf.transform(processed_reviews)  # tf-idf 벡터 변환\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "\n",
    "print(mod_tfidf); i = 0\n",
    "# tfidf 살펴보기\n",
    "for item, value in tfidf.items():\n",
    "    print(\"{\",item,\":\",value,\"}\"); i += 1\n",
    "    if i > 5 : break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 39516)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import  pickle  # tf-idf 객체 저장하기\n",
    "print ('학습한 html 리뷰 수:',len(processed_reviews),'--> TF-IDF 저장객체 :', len(mod_tfidf.get_feature_names()))\n",
    "print ('TF-IDF :', mod_tfidf.get_feature_names()[::4000])\n",
    "v = mod_tfidf.transform( processed_reviews )\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 -- 39516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1x39516 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습한 객체를 저장한다\n",
    "with open('vectorizer.pk', 'wb') as file:\n",
    "      pickle.dump(mod_tfidf, file)\n",
    "\n",
    "# 저장한 객체를 불러온다\n",
    "load_tfidf = pickle.load( open(\"vectorizer.pk\", 'rb') )\n",
    "load_tfidf.transform(PreprocessTfidf([' '.join(['drama'])],stoplist,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04 gensim 의 LSA 모델 적용\n",
    "단어 쿼리를 잠재공간으로 변환시 필요한 U,V,S 행렬을 얻는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import models\n",
    "class GenSimCorpus(object):\n",
    "    def __init__(self, texts, stoplist=[],stem=False):\n",
    "        self.texts = texts\n",
    "        self.stoplist = stoplist\n",
    "        self.stem = stem\n",
    "        self.dictionary = gensim.corpora.Dictionary(self.iter_docs(texts, stoplist))\n",
    "    def __len__(self):  return len(self.texts)\n",
    "    def __iter__(self):\n",
    "        for tokens in self.iter_docs(self.texts, self.stoplist):\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "    def iter_docs(self,texts, stoplist):\n",
    "        for text in texts:\n",
    "            if self.stem: yield (stemmer.stem(w) for w in [x for x in tknzr.tokenize(text) if x not in stoplist])\n",
    "            else:         yield (x for x in tknzr.tokenize(text) if x not in stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus     : <__main__.GenSimCorpus object at 0x7f85f9f7e3c8>\n",
      "dict_corpus: Dictionary(26132 unique tokens: ['film', 'unexpect', ',', 'scari', 'origin']...)\n"
     ]
    }
   ],
   "source": [
    "# corpus 생성하기\n",
    "corpus = GenSimCorpus(tot_textreviews, stoplist, True)\n",
    "dict_corpus = corpus.dictionary\n",
    "print('corpus     :', corpus); print('dict_corpus:', dict_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26132, 10)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# U matrix 생성\n",
    "lsi =  models.LsiModel(corpus, num_topics = ntopics, id2word = dict_corpus)\n",
    "U = lsi.projection.u\n",
    "U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sigma 생성\n",
    "ntopics = 10\n",
    "Sigma = np.eye(ntopics) * lsi.projection.s\n",
    "Sigma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 10)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# V 행렬을 계산한다\n",
    "V = gensim.matutils.corpus2dense(lsi[corpus], len(lsi.projection.s)).T / lsi.projection.s\n",
    "V.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_word's length 26132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{',': 2,\n",
       " '.': 9,\n",
       " 'caught': 5,\n",
       " 'film': 0,\n",
       " 'guard': 6,\n",
       " 'loop': 8,\n",
       " 'origin': 4,\n",
       " 'scari': 3,\n",
       " 'threw': 7,\n",
       " 'unexpect': 1}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 쿼리에 포함된 단어의 색인을 생성한다\n",
    "dict_words = {}\n",
    "for i in range(len(dict_corpus)):\n",
    "    dict_words[dict_corpus[i]] = i\n",
    "    \n",
    "# http://hamait.tistory.com/803  # 반복 실행문의 부분실행 모듈\n",
    "import itertools as it\n",
    "print('dict_word\\'s length' : , len(dict_words))\n",
    "{k: dict_words[k] for k in it.islice(dict_words.keys(), 10) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "for i in islice(range(10), 5):\n",
    "    print i\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',': 2,\n",
       " '.': 9,\n",
       " 'caught': 5,\n",
       " 'film': 0,\n",
       " 'guard': 6,\n",
       " 'loop': 8,\n",
       " 'origin': 4,\n",
       " 'scari': 3,\n",
       " 'threw': 7,\n",
       " 'unexpect': 1}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "it.islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
