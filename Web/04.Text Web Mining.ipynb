{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chapter 4 - Web Mining Techniques\n",
    "웹 마이닝 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 1 웹 구조 마이닝\n",
    "Web structure mining\n",
    "\n",
    "### 01 웹 크롤러 - Spyder\n",
    "### 02 인덱서- DB\n",
    "### 03 페이지 랭킹 알고리즘 - Ranking (명성)\n",
    "page I's in-link : 외부 페이지에서 I page 연결 수\n",
    "\n",
    "page I's out-link : 페이지 I 가 연결하는 외부페이지 수\n",
    "1. P(i) : i 페이지 방문확률\n",
    "2. P(j) : j 페이지 방문확률\n",
    "3. Aji: 노트 j에서 노드 i로 전이될 확률 \n",
    "$$ P(i) = \\sum_j A_{ji} P(j) $$\n",
    "\n",
    "### 04 rank sing (랭크싱크) \n",
    "서로 링크한 경우에는 Loop에 빠져서, 랜덤한 점프 전이행렬을 추가\n",
    "1. (1-d) 페이지를 임의로 방문할 확률\n",
    "2. A는 확률로써 총 합은 1이다\n",
    "$$ P = (\\frac{(1-d)E}{N}+dA^T)P $$\n",
    "3. 아래의 식 P벡터를 정규화 하면 간단해진다. $e^T P=N $\n",
    "$$ P = (1-d)e + dA^TP \\to P(i) = (1-d)+d\\sum_{j=1}A_{ji}P(j) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 2 웹 콘텐츠 마이닝\n",
    "### 01 Parsing\n",
    "파싱\n",
    "### 02 자연어 처리\n",
    "NLTK\n",
    "\n",
    "Stemming : 단어를 어간으로 축소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"\"\"On 1 January 2007, Irish became a full EU official language, \n",
    "with a temporary derogation for a renewable period of five years (see Council \n",
    "Regulation (EC) No 920/2005 of 13 June 2005 (OJ L 156, 18.6.2005, p. 3)) stating \n",
    "that 'the institutions of the European Union shall not be bound by the obligation\n",
    "to draft all acts in Irish and to publish them in that language in the Official \n",
    "Journal of the European Union \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words length :  91 \n",
      "\n",
      " ['On', '1', 'January', '2007', ',', 'Irish', 'became', 'a', 'full', 'EU', 'official', 'language', ',', 'with', 'a', 'temporary', 'derogation', 'for', 'a', 'renewable']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tknzr = WordPunctTokenizer()\n",
    "\n",
    "words = tknzr.tokenize(text)\n",
    "print('Words length : ', len(words), '\\n\\n', words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean words length :  64 \n",
      "\n",
      " ['on', '1', 'january', '2007', ',', 'irish', 'became', 'full', 'eu', 'official', 'language', ',', 'temporary', 'derogation', 'renewable']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "words_clean = [ w.lower()   for w in words   if w not in stopwords]\n",
    "print('Clean words length : ', len(words_clean), '\\n\\n', words_clean[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean words stem length :  64 \n",
      "\n",
      " ['on', '1', 'januari', '2007', ',', 'irish', 'becam', 'full', 'eu', 'offici', 'languag', ',', 'temporari', 'derog', 'renew']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words_clean_stem = [stemmer.stem(w)  for w in words_clean]\n",
    "print('Clean words stem length : ', len(words_clean_stem), '\\n\\n', words_clean_stem[:15])\n",
    "# token 의 갯수는 동일, 접사를 제거한 나머지를 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 3 정보 검색 모델\n",
    "Information retrieval models\n",
    "\n",
    "모델목록 : Boolean Model(불리언), Vector space model(벡터), Probabilistic model(확률)\n",
    "\n",
    "Vector 모델 목록\n",
    "1. TF-IDF(Term Frequency-Inverse Document Frequency) 단어빈도 - 역문서 빈도\n",
    "2. LSA(Latent Semantic Analysis) 잠재의미분석\n",
    "3. Doc2Vec (Word2Vec)\n",
    "\n",
    "쿼리 단어간의 벡터로 표현하고, <strong>쿼리 벡터</strong>와 <strong>각 문서</strong>간의 <strong>$\\cos \\theta$ 유사도</strong> 를 측정, 비교한다 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 TF-IDF\n",
    "아주 <strong>많은 문서에 빈번</strong>하게 등장하는 단어는 <strong>중요도가 낮고</strong>, \n",
    "<strong>일부문서에 빈번</strong>한 단어의 <strong>중요도가 높다</strong>\n",
    "$$ W_{ij} = tf_{ij}*idf_j $$\n",
    "$ tf_{ij} = \\frac {f_{ij}}{max f_{i1}...f_{iV}}$  문서 i에서 <strong>단어 j의 정규화 빈도</strong>\n",
    "\n",
    "<strong>역문서 빈도</strong>의 $idf_j$는 단어 j를 포함하는 웹페이지 수, N은 전체 웹페이지 수이다 $ idf_j = log\\frac{N}{df} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 잠재 의미 분석(LSA)\n",
    "단어와 문서를 효과적으로 설명할 수 있는 정형적인 잠재공간이 존재하고, 비슷한 의미의 단어는 비슷한 위치에 나타난다\n",
    "\n",
    "문서의 잠재공간 투영은 <strong>절단SVD</strong> (Singular Value Decomposition)를 이용한다\n",
    "$$ X = U_t \\sum_t V_t^T $$\n",
    "$U_t(V*t)$ : t 차원의 잠재공간에 투영된 단어의 행렬\n",
    "\n",
    "$\\sum_t V_t^T $ : 잠재공간에 투영된 문서의 전치행렬\n",
    "\n",
    "$\\sum_{t(t*t)}$ : 특기앖으로 이루어진 대각행렬\n",
    "$$ q_t = q^TU_t\\sum_t^{-1}$$ 쿼리벡터도 잠재공간에 투영된다\n",
    "\n",
    "각 문서는 <strong>$q_t^T$</strong>와 <strong>$\\cos\\theta$유사도</strong>로 비교될 수 있지만, \n",
    "\n",
    "실제는 $V_t^T$와 $q_t^T$간의 <strong>유사도</strong>로 계산한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 Word2Vec\n",
    "두가지 아키텍처 알고리즘 중 하나를 선택하여, \n",
    "\n",
    "N개의 뉴런(Neuron)(가중치)으로 된 Hidden Layer를 훈련시킨다.\n",
    "\n",
    "즉 N개의 뉴런(가중치) 'h'를 갖는 <strong>단일계층 학습</strong>으로 얕은학습(shallow learning)이다 \n",
    "\n",
    "행렬W는 <strong>입력벡터를 hidden layer</strong>로 변환하며, <strong>출력계층에서는 Target</strong>을 평가한다\n",
    "\n",
    "train에서 W와 W\"의 수정은 <strong>확률적 내리막 경사법(Stochastic Gradient Descent)</strong>를 사용한다\n",
    "\n",
    "https://shuuki4.wordpress.com/2016/01/27/word2vec-%EA%B4%80%EB%A0%A8-%EC%9D%B4%EB%A1%A0-%EC%A0%95%EB%A6%AC/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><strong>연속단어 주머니 CBOW (Continuous Bag Of Words)</strong></h4>\n",
    "<h5>ex) 아이스크림을 사 먹었는데, ___ 시려서 힘들었다. (빈칸 알맞은 단어 찾기)</h5>\n",
    "주어진 단어 앞 뒤 C/2개 씩 <strong>총 C개 단어를 Input</strong>으로 사용, \n",
    "\n",
    "<strong>주어진 단어를 맞추기 위한 네트워크</strong>를 만든다\n",
    "<h4>Input Layer 는</h4> \n",
    "모든 단어들이 공통적으로 사용하는 VxN (N:사용벡터의 길이(Projection Layer길이)) \n",
    "\n",
    "크기의 Projection Matrix $W$가 있고\n",
    "<h4>Projection Layer에서 Output Layer로 갈 때는</h4> NxV 크기의 Weight Matrix W’ 가 있다.  \n",
    "(주의해야할 점은, 두 행렬은 transpose이 아닌, 별개의 행렬이라는 점이다)\n",
    "<h4>Process 정리</h4>\n",
    " 1. 즉 단어들을 one-hot encoding으로 입력\n",
    "2. 여러단어를 각각 projection 시킨 후, 벡터들의 평균을 구해서 Projection Layer에 보낸다. \n",
    "3. Weight Matrix를 곱해서 Output Layer로 보내고 softmax 계산을 한 후, \n",
    "4. 이 결과를 진짜 단어의 one-hot encoding과 비교하여 에러를 계산한다.\n",
    "\n",
    "<h4>CBOW 모델에서 하나의 단어 처리 계산량</h4>\n",
    "1. C개의 단어를 Projection 하는 데에 C x N\n",
    "2. Projection Layer에서 Output Layer로 가는 데에 N x V, 전체 계산량은 CxN + NxV\n",
    "3. 2.식은 V를 ln V로 줄이는 테크닉을 사용하면 전체 계산량이 CxN + N x lnV가 된다.\n",
    "\n",
    "결국 <strong>Projection Layer의 크기 N</strong>과 <strong>log-사전크기의 lnV</strong>의 크기의 곱에 비례하게 된다.\n",
    "\n",
    "C=10, N=500, V=1,000,000으로 잡아도 500 x (10+ln(1,000,000)) = 약 10000의 계산량밖에 들지 않는다. \n",
    "\n",
    "이는 앞서 확인한 NNLM이나 RNNLM에 비해 정말 엄청나게 줄어든 계산량이라는 것을 확인할 수 있다.\n",
    "\n",
    "<img src=\"https://shuuki4.files.wordpress.com/2016/01/cbow.png?w=260&h=300\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><strong>스킵그램(Skip-gram)</strong></h4>\n",
    "\n",
    "주어진 단어 주위에 등장하는, 나머지 단어들의 등장 여부를 유추하는 것으로\n",
    "\n",
    "<strong>‘가까이 위치한 단어일 수록 관련이 더 많다’</strong> 라는 생각을 적용하기 위해 \n",
    "\n",
    "멀리 떨어져있는 단어일수록 낮은 확률로 택하는 방법을 사용한다. \n",
    "\n",
    "나머지 구조는 CBOW와 방향만 반대일 뿐 굉장히 유사하다\n",
    "\n",
    "<h4>Skip-gram에서 하나의 단어를 처리하는 데에 드는 계산량</h4>\n",
    "\n",
    "1. C개의 단어를 샘플링했다고 할 때, 현재 단어를 Projection 하는 N\n",
    "2. Output을 계산하는 데에 N x V, 테크닉을 사용하면 N x ln V\n",
    "3. 총 C개의 단어에 대해 진행해야 하므로 총 C배\n",
    "4. 총 C(N + N x lnV) 만큼의 연산이 필요하다\n",
    "\n",
    "CBOW 모델같이 N x lnV에 비례하는 계산량을 가진 모델이기는 하지만, \n",
    "\n",
    "샘플링 단어 갯수에 따라서 계산량이 비례하여 올라가게 되므로 CBOW 비해서는 느리다\n",
    "\n",
    "실험결과, Skip-gram이 다소 좋은결과를 내는 추세를 보인다\n",
    "\n",
    "<img src=\"https://shuuki4.files.wordpress.com/2016/01/skip-gram.png?w=276&h=300\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04 Doc2Vec\n",
    "단어 $W_j$를 벡터 $V_w$로 표현하며, 단어가 나타나는 문서 $d_i$와는 독립적 벡터로 정의한다\n",
    "\n",
    "Word2Vec알고리즘을 <strong>신경망</strong>과 <strong>역전파</strong>를 확장 이용한 것으로\n",
    "\n",
    "문서를 단어의 벡터로 추가한 모델이다\n",
    "<h4><strong>분산 메모리 모델 (DM) Distributed Memory model</strong>로써</h4>\n",
    "문서 <strong>D</strong>벡터로 수집한 단어들이 학습모델과 공유되며, \n",
    "\n",
    "W와 W\"는 모든 문서에 공통으로 연산하게 되어서 최적값을 찾는다 \n",
    "<h4><strong>분산 단어 주머니(DBOW) distributed bag of words</strong> </h4>\n",
    "https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "\n",
    "입력계층에서는 문서 벡터만 고려하고, \n",
    "\n",
    "출력계층은 문서 샘플링 Context의 단어 집합만 고려한다\n",
    "\n",
    "DM아키텍처는 Gensim 라이브러리에 기본모듈로써 구현되어 있다\n",
    "<h5>(아래는 3개 단어로 된 컨텍스트를 찾는 분산 메모리 모델의 예제이다)</h5>\n",
    "<img src=\"https://research.google.com/archive/paragraph_vector_icml2014/distributed_memory_model.png\" align=\"left\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br></br>\n",
    "## 4 영화 리뷰 쿼리분석 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 데이터 불러오기\n",
    "bs4 옵션설정 https://www.pythonanywhere.com/forums/topic/7002/  \n",
    "\n",
    "open() utf-8 오류설정 https://stackoverflow.com/questions/20578270/unicodedecodeerror-in-python-while-reading-utf-8-sql-file-from-english-wikipedia  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "movie_html_dir = './data/movie/'\n",
    "movie_dict = {} # 데이터 파일 목록 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web page : 27886 \n",
      "Crawling time : 42.491\n"
     ]
    }
   ],
   "source": [
    "filenames = [f    for f in os.listdir(movie_html_dir)      if f[0]!='.'] ; t0 = time()\n",
    "for file in filenames:\n",
    "    id_html = file.split('.')[0]\n",
    "    # UTF-8 오류발생시 외부 메세지 처리없이 결과를 출력한다\n",
    "    f = open(movie_html_dir + file, encoding = \"utf8\", errors = 'replace')\n",
    "    s = f.read()    \n",
    "    parsed_html = BeautifulSoup(s,\"lxml\")\n",
    "    try:     title = parsed_html.body.h1.text\n",
    "    except:  title = 'none'\n",
    "    movie_dict[id_html] = title\n",
    "print('Web page :', len(filenames), '\\nCrawling time :',round(time()-t0, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 긍/부정 Text 사전처리\n",
    "nltk_movie_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/markbaum/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 모듈호출 및 불용어 목록 불러오기\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tknzr = WordPunctTokenizer()\n",
    "nltk.download('stopwords')\n",
    "stoplist = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 어간처리 모듈 불러오기\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def ListDocs(dirname):\n",
    "    docs, titles  = [], []\n",
    "    filenames = [f for f in os.listdir(dirname) if str(f)[0]!='.']\n",
    "    for filename in filenames:\n",
    "        f = open(dirname+'/'+filename,'r')\n",
    "        id_html = filename.split('.')[0].split('_')[1]\n",
    "        titles.append(movie_dict[id_html])\n",
    "        docs.append(f.read())\n",
    "    return docs, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'david schwimmer ( from the television series \" friends \" ) stars as a sensitive ( and slightly neurotic ) single guy who gets more than he expected from the grieving mother ( barbara hershey ) of a classmate he can\\'t remember . \\nhello mrs . robinson ! \\nthough quite cute as a romantic comedy , the pallbearer is paced like a funeral march . \\nthe characters act , react , and interact at half-speed , making for one * excruciatingly * long sit . \\n ( and what\\'s with the dreary lighting ? ) \\nco-writer/'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 리뷰 텍스트 모두 메모리에 불러오기\n",
    "dir = '/home/markbaum/nltk_data/corpora/movie_reviews/'\n",
    "pos_textreviews, pos_titles = ListDocs(dir+'pos/')\n",
    "neg_textreviews, neg_titles = ListDocs(dir+'neg/')\n",
    "tot_textreviews = pos_textreviews + neg_textreviews\n",
    "tot_titles = pos_titles + neg_titles\n",
    "\n",
    "neg_textreviews[2][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 저장 데이터 목록 \n",
    "# pos_titles\n",
    "# neg_titles\n",
    "# tot_titles\n",
    "\n",
    "# pos_textreviews\n",
    "# neg_textreviews\n",
    "# tot_textreviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 TF-IDF 구현하기 sklearn 모듈 활용\n",
    "TF-IDF model can be trained using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text 전처리 함수\n",
    "def Preprocess_Tf_idf(texts, stoplist = [], stem = False):\n",
    "    newtexts = []\n",
    "    for text in texts:\n",
    "        # 불용어 제거하기\n",
    "        if stem:   tmp = [w for w in tknzr.tokenize(text) if w not in stoplist]\n",
    "        # 유효한 단어 어간 처리 (접사 제거)\n",
    "        else:      tmp = [stemmer.stem(w) for w in [w for w in tknzr.tokenize(text) if w not in stoplist]]\n",
    "        newtexts.append(' '.join(tmp))\n",
    "    return newtexts\n",
    "\n",
    "# neg/pos 영화리뷰 데이터 전처리 하기\n",
    "processed_reviews = Preprocess_Tf_idf(tot_textreviews, stoplist, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf's length : 39516 \n",
      " time : 1.3149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'00': 6.4041777572475143,\n",
       " '000': 4.0688028414304771,\n",
       " '0009f': 7.908255154023788,\n",
       " '007': 6.2988172415896875,\n",
       " '00s': 7.908255154023788,\n",
       " '03': 7.908255154023788,\n",
       " '04': 7.908255154023788,\n",
       " '05': 7.2151079734638426,\n",
       " '05425': 7.908255154023788,\n",
       " '10': 3.3649603717537842}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf-idf 모델 테스트 하기\n",
    "t0 = time()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 1)            # tf-idf 모듈 활성화\n",
    "mod_tfidf = vectorizer.fit(processed_reviews)       # tf-idf 데이터 적용\n",
    "vec_tfidf = mod_tfidf.transform(processed_reviews)  # tf-idf 벡터 변환\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "\n",
    "# http://hamait.tistory.com/803  # 반복 실행문의 부분실행 모듈\n",
    "import itertools as it\n",
    "print('tfidf\\'s length :' , len(tfidf), '\\n time :',round(time()-t0, 4))\n",
    "{k: tfidf[k] for k in it.islice(tfidf.keys(), 10) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습한 html 리뷰 수: 2000 \n",
      "--> TF-IDF 저장객체 : 39516\n",
      "\n",
      "TF-IDF : ['00', 'blas', 'courier', 'establishment', 'hayakawa', 'lavishly', 'notoriety', 'rains', 'skim', 'transposes']\n",
      "(2000, 39516)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1x39516 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf-idf 객체 저장하기\n",
    "print ('학습한 html 리뷰 수:',len(processed_reviews),'\\n--> TF-IDF 저장객체 :', len(mod_tfidf.get_feature_names()))\n",
    "print ('\\nTF-IDF :', mod_tfidf.get_feature_names()[::4000])\n",
    "v = mod_tfidf.transform( processed_reviews )\n",
    "print(v.shape)\n",
    "\n",
    "# 학습한 객체를 저장한다\n",
    "import  pickle  \n",
    "with open('vectorizer.pk', 'wb') as file:\n",
    "      pickle.dump(mod_tfidf, file)\n",
    "\n",
    "# 저장한 객체를 불러온다\n",
    "load_tfidf = pickle.load( open(\"vectorizer.pk\", 'rb') )\n",
    "load_tfidf.transform(Preprocess_Tf_idf([' '.join(['drama'])],stoplist,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04 gensim 의 LSA 모델 적용\n",
    "단어 쿼리를 잠재공간으로 변환시 필요한 U,V,S 행렬을 얻는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Gensim 모델 만들기\n",
    "import gensim\n",
    "from gensim import models\n",
    "class GenSimCorpus(object):\n",
    "    def __init__(self, texts, stoplist=[],stem=False):\n",
    "        self.texts = texts\n",
    "        self.stoplist = stoplist\n",
    "        self.stem = stem\n",
    "        self.dictionary = gensim.corpora.Dictionary(self.iter_docs(texts, stoplist))\n",
    "    def __len__(self):  return len(self.texts)\n",
    "    def __iter__(self):\n",
    "        for tokens in self.iter_docs(self.texts, self.stoplist):\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "    def iter_docs(self,texts, stoplist):\n",
    "        for text in texts:\n",
    "            if self.stem: yield (stemmer.stem(w) for w in [x for x in tknzr.tokenize(text) if x not in stoplist])\n",
    "            else:         yield (x for x in tknzr.tokenize(text) if x not in stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus      : <__main__.GenSimCorpus object at 0x7f79761a8eb8> \n",
      "dict_corpus : Dictionary(26132 unique tokens: ['film', 'unexpect', ',', 'scari', 'origin']...)\n"
     ]
    }
   ],
   "source": [
    "# corpus 생성하기\n",
    "corpus = GenSimCorpus(tot_textreviews, stoplist, True)\n",
    "dict_corpus = corpus.dictionary\n",
    "print('corpus      :', corpus, '\\ndict_corpus :', dict_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26132, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# U matrix 생성\n",
    "n_topics = 10\n",
    "lsi =  models.LsiModel(corpus, num_topics = n_topics, id2word = dict_corpus)\n",
    "U = lsi.projection.u\n",
    "U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sigma 생성\n",
    "Sigma = np.eye(n_topics) * lsi.projection.s\n",
    "Sigma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# V 행렬을 계산한다\n",
    "V = gensim.matutils.corpus2dense(lsi[corpus], len(lsi.projection.s)).T / lsi.projection.s\n",
    "V.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_word's length : 26132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{',': 2,\n",
       " '.': 9,\n",
       " 'caught': 5,\n",
       " 'film': 0,\n",
       " 'guard': 6,\n",
       " 'loop': 8,\n",
       " 'origin': 4,\n",
       " 'scari': 3,\n",
       " 'threw': 7,\n",
       " 'unexpect': 1}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 쿼리에 포함된 단어들의 색인'dict_words'을 생성한다\n",
    "dict_words = {}\n",
    "\n",
    "for i in range(len(dict_corpus)):\n",
    "    dict_words[dict_corpus[i]] = i\n",
    "    \n",
    "print('dict_word\\'s length :' , len(dict_words))\n",
    "{k: dict_words[k] for k in it.islice(dict_words.keys(), 10) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05 gensim 의 Doc2Vec 를 생성 1\n",
    "전처리 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 전처리 함수\n",
    "# token화, 어근처리(stemming), 불용어 제거\n",
    "def Preprocess_Doc2Vec(text, stop = [], stem = False):\n",
    "    words = tknzr.tokenize(text)  # text의 token 변화\n",
    "    if stem:                      # 어근처리 (stemming)\n",
    "        words_clean = [stemmer.stem(w)    for w in [ i.lower()    \n",
    "                                                    for i in words   \n",
    "                                                    if i not in stop]] # 불용어 제거\n",
    "    else:\n",
    "        words_clean = [i.lower()   for i in words   if i not in stop]  # 불용어 제거\n",
    "    return words_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "dir = '/home/markbaum/nltk_data/corpora/movie_reviews/'\n",
    "Review = namedtuple('Review', 'words tags')\n",
    "\n",
    "# 긍정적인 리뷰 데이터 수집하기\n",
    "reviews_pos, cnt = [], 0  # 데이터 초기화 설정\n",
    "do2vec_stem = False       # doc2vec의 어근처리 비활성화 (전처리 함수에서 미리 수행)\n",
    "\n",
    "for filename in [f    for f in os.listdir(dir + 'pos/')    if str(f)[0]!='.']:\n",
    "    f = open(dir + 'pos/' + filename, 'r')\n",
    "    reviews_pos.append(Review(Preprocess_Doc2Vec(f.read(), stoplist, do2vec_stem), ['pos_' + str(cnt)]))\n",
    "    cnt += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 부정적인 리뷰 데이터 수집하기\n",
    "reviews_neg, cnt = [], 0  # 데이터 초기화 설정\n",
    "\n",
    "for filename in [f    for f in os.listdir(dir + 'neg/')    if str(f)[0]!='.']:\n",
    "    f = open(dir + 'neg/' + filename, 'r')\n",
    "    reviews_neg.append(Review(Preprocess_Doc2Vec(f.read(), stoplist, do2vec_stem), ['neg_' + str(cnt)]))\n",
    "    cnt += 1\n",
    "tot_reviews = reviews_pos + reviews_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 결과 \n",
    "# reviews_pos  # 긍정 리뷰 1000개 모음\n",
    "# reviews_neg  # 부정 리뷰 1000개 모음\n",
    "# tot_reviews  # 모든 리뷰 모음\n",
    "# word2vec 의 경우에는 do2vec_stem = False 즉 어근처리 않한경우에 더 결과가 좋은 경우가 많다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06 gensim 의 Doc2Vec 를 생성 2\n",
    "Train 작업 by Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Doc2Vec 모델을 파라미터 설정\n",
    "from gensim.models import Doc2Vec\n",
    "import multiprocessing\n",
    "\n",
    "numepochs= 20   # 훈련은 20시대 동안 진행된다\n",
    "vec_size = 500  # 총 벡터의 수 : 은닉 레이어 갯수가 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# windos : 10개 단어의 관계망을 설정\n",
    "# min_count : 최소 1번 언급된 단어는 모두 데이터를 생성\n",
    "# negative  : 부정 샘플링\n",
    "# hs : 계층적 소프트맥스 설정\n",
    "cores = multiprocessing.cpu_count()\n",
    "model_d2v = Doc2Vec(dm = 1, dm_concat = 0, size = vec_size, \n",
    "                    window = 10, negative = 0, hs = 0, min_count = 1, workers =cores)\n",
    "\n",
    "# Doc2Vec 모델에 전체 리뷰데이터를 입력\n",
    "model_d2v.build_vocab(tot_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 17.7211\n",
      "epoch 2 20.2661\n",
      "epoch 4 20.0804\n",
      "epoch 6 21.5727\n",
      "epoch 8 19.9699\n",
      "epoch 10 20.041\n",
      "epoch 12 19.9743\n",
      "epoch 14 20.2278\n",
      "epoch 16 20.0998\n",
      "epoch 18 20.1269\n",
      "word2vec train ends.. 20 4\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/RaRe-Technologies/gensim/issues/1284\n",
    "# .train : 개체의 갯수와, Epoch의 수를 명시적 입력\n",
    "t0 = time\n",
    "for epoch in range(numepochs):\n",
    "    try:\n",
    "        t0 = time()\n",
    "        model_d2v.train(tot_reviews, total_examples = len(tot_reviews), epochs = numepochs)\n",
    "        model_d2v.alpha *= 0.99  # 학습률\n",
    "        model_d2v.min_alpha = model_d2v.alpha\n",
    "        if epoch % 2 == 0: print ('epoch %d' % (epoch), round(time()-t0,3), 'sec')\n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        break\n",
    "print(\"word2vec train ends..\", round(time()-t0)*10,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 07 TF-IDF 를 활용한 쿼리문 실행\n",
    "mod_tfidf - Train 데이터 활용하기 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim: 0.216511203046  title: Usual Suspects, The (1995)\n",
      "sim: 0.199655873219  title: Jane Austen's Mafia! (1998)\n",
      "sim: 0.137757636005  title: Reservoir Dogs (1992)\n",
      "sim: 0.135541633859  title: Score, The (2001)\n",
      "sim: 0.12848238199  title: Mystery Men (1999)\n",
      "sim: 0.127533806889  title: Se7en (1995)\n",
      "sim: 0.12296714611  title: L.A. Confidential (1997)\n",
      "sim: 0.113695734249  title: Lost Highway (1997)\n",
      "sim: 0.107419256584  title: Very Bad Things (1998)\n",
      "sim: 0.103080438187  title: True Crime (1999)\n"
     ]
    }
   ],
   "source": [
    "# tf-idf를 활용하여 비슷한 웹페이지를 출력\n",
    "query = ['drama','mystery','crime'] # 웹문서를 수집을 위한 query\n",
    "\n",
    "# Similar tf-idf 결과값이 무척 작은경우 matrix 를 vector로 변환한다\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 벡터를 정규 벡터로 변환\n",
    "query_vec = mod_tfidf.transform(Preprocess_Tf_idf ([' '.join(query)], stoplist,  True))  \n",
    "sims = cosine_similarity(query_vec, vec_tfidf)[0]  # tf-idf의 cos 유사도 측정 \n",
    "indxs_sims = sims.argsort()[::-1]                  # 데이터 순서를 뒤집는다 (내림차순 정렬)\n",
    "for d in list(indxs_sims)[:10]:\n",
    "    print('sim:', sims[d],' title:', tot_titles[d])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 08 LSA의 $q_k$로 변환 쿼리문 활용하기\n",
    "dict_words 목록 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q length : 26132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 20.05895731,   0.52786705,  -1.35532556,   0.53508898,\n",
       "         2.49284268,   1.80272746,  -0.34347263,  -0.6973278 ,\n",
       "         1.64073059,  -0.82348602])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 웹문서를 수집을 위한 query\n",
    "query = ['drama','mystery','crime']\n",
    "\n",
    "# 쿼리문을 LSA의 qk로 변환\n",
    "def Transform_WordsList_to_Query_Vec(wordslist, dict_words, stem = False):\n",
    "    q = np.zeros(len( dict_words.keys()) )\n",
    "    for w in wordslist:\n",
    "        if stem:  q[dict_words[ stemmer.stem(w) ]] = 1.\n",
    "        else:     q[dict_words[ w ]] = 1.\n",
    "    return q\n",
    "\n",
    "q  = Transform_WordsList_to_Query_Vec(query, dict_words, True)\n",
    "print('q length :', len(q))\n",
    "qk = np.dot(np.dot(q,U),Sigma); qk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim :  1.59359220917  doc :  Rocky Horror Picture Show, The (1975)\n",
      "sim :  1.51608878503  doc :  Alien³ (1992)\n",
      "sim :  1.41083646704  doc :  Wild Things (1998)\n",
      "sim :  1.40695314744  doc :  Star Wars: Episode I - The Phantom Menace (1999)\n",
      "sim :  1.3501999207  doc :  Boogie Nights (1997)\n",
      "sim :  1.29669576899  doc :  Deep Impact (1998)\n",
      "sim :  1.2916246418  doc :  Black Cauldron, The (1985)\n",
      "sim :  1.26846220112  doc :  Antz (1998)\n",
      "sim :  1.26571395457  doc :  Starship Troopers (1997)\n",
      "sim :  1.23622768048  doc :  Species II (1998)\n"
     ]
    }
   ],
   "source": [
    "# 벡터를 정규벡터로 변환 뒤, 코사인 유사도를 계산\n",
    "sims = np.zeros(len( tot_textreviews ))\n",
    "for d in range(len(V)):\n",
    "    sims[d]=np.dot(qk, V[d])\n",
    "\n",
    "indxs_sims = np.argsort(sims)[::-1]  \n",
    "for d in list(indxs_sims)[:10]:\n",
    "    print ('sim : ', sims[d],' doc : ',tot_titles[d] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09 doc2vec 모델에서 쿼리문 활용하기\n",
    "model_d2v 모델링 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pos_418', 0.14885319769382477),\n",
       " ('neg_924', 0.14184680581092834),\n",
       " ('pos_48', 0.1287953108549118),\n",
       " ('neg_315', 0.11485865712165833),\n",
       " ('pos_197', 0.11204808950424194)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 웹문서를 수집을 위한 query\n",
    "query = ['love','sex','nudy']\n",
    "\n",
    "# doc2vec query : force inference to get the same result\n",
    "model_d2v.random = np.random.RandomState(1)\n",
    "query_docvec = model_d2v.infer_vector(Preprocess_Tf_idf(' '.join(query), stoplist, do2vec_stem))\n",
    "reviews_related = model_d2v.docvecs.most_similar([query_docvec], topn = 5)\n",
    "reviews_related\n",
    "\n",
    "# for review in reviews_related:\n",
    "#     print('relevance:',review[1],'  title:',tot_titles[review[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과\n",
    "\n",
    "고급 알고리즘 보다 tf-idf 가 더 좋은 결과를 보여준다\n",
    "\n",
    "Doc2Vec 등은 오히려 훈련데이터가 너무 작기 때문에 결과가 좋지 않다\n",
    "\n",
    "http://www.cs.cornell.edu/people/pabo/movie-review-data/ \n",
    "는 10억개의 데이터로 훈련된 데이터를 릴리즈 하였다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br></br>\n",
    "## 5 잠재 디리클레 할당 - (사후 처리정보 1)\n",
    "LDA (Latent Dirichlet allocation)\n",
    "\n",
    "수집 데이터를 바탕으로, 다양한 정보를 추출하는 자연어 알고리즘\n",
    "\n",
    "입력단어 <strong>(관측변수)</strong>는, <strong>잠재된 미관측변수(주제)</strong>에 의해 설명이 되고,\n",
    "\n",
    "<strong>미관측 변수(주제)</strong>는 <strong>관측 데이터와 유사여부</strong>를 근거로 제공한다\n",
    "\n",
    "즉 LDA는, 입력한 텍스트의 <strong>잠재주제 단어</strong>를 자동으로 찾고,\n",
    "\n",
    "이를 바탕으로 문서의 <strong>공통주제 추출</strong>이 가능하다\n",
    "\n",
    "이는 <strong>알고리즘의 '사후확률의 최대화'</strong>를 통해서 구현이 가능하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 데이터 수집 및 목록 만들기\n",
    "train, DB 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.797 sec\n"
     ]
    }
   ],
   "source": [
    "# train 데이터 불러오기\n",
    "# import os; import numpy as np\n",
    "# from bs4 import BeautifulSoup\n",
    "moviehtmldir = './data/movie/'\n",
    "movie_dict = {}; t0=time()\n",
    "\n",
    "for filename in [f for f in os.listdir(moviehtmldir) if f[0]!='.']:\n",
    "    id_ = filename.split('.')[0]\n",
    "    f = open(moviehtmldir+'/'+filename, encoding = \"utf8\", errors = 'replace')\n",
    "    parsed_html = BeautifulSoup(f.read(), \"lxml\")\n",
    "    try: title = parsed_html.body.h1.text\n",
    "    except: title = 'none'\n",
    "    movie_dict[id_] = title\n",
    "print(round(time()-t0, 4) , 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 분석의 바탕이 되는 DB의 데이터 목록 불러오기\n",
    "def ListDocs(dirname):\n",
    "    docs, titles = [], []\n",
    "    for filename in [f    for f in os.listdir(dirname)   if str(f)[0]!='.']:\n",
    "        f = open(dirname + '/' + filename, 'r')\n",
    "        id_file = filename.split('.')[0].split('_')[1]\n",
    "        titles.append(movie_dict[id_file])\n",
    "        docs.append(f.read())\n",
    "    return docs, titles\n",
    "\n",
    "dir = '/home/markbaum/nltk_data/corpora/movie_reviews/'\n",
    "pos_textreviews,  pos_titles = ListDocs(dir + 'pos/')\n",
    "neg_textreviews,  neg_titles = ListDocs(dir + 'neg/')\n",
    "tot_textreviews = pos_textreviews + neg_textreviews\n",
    "tot_titles = pos_titles + neg_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10378': 'Postman, The (1997)',\n",
       " '10807': 'Titanic (1997)',\n",
       " '11832': 'Lost in Space (1998)',\n",
       " '13684': 'Saving Private Ryan (1998)',\n",
       " '13764': 'Mark of Zorro, The (1920)',\n",
       " '1458': 'Revenge of the Nerds III: The Next Generation (1992) (TV)',\n",
       " '14726': 'Kundun (1997)',\n",
       " '29117': 'Others, The (2001)',\n",
       " '6389': 'Star Trek: First Contact (1996)',\n",
       " '6872': 'Azúcar amarga (1996)'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결과\n",
    "# movie_dict : 색인용 목록 (위 for문과 아래의 dirlist 명령에 공통적으로 작업결과를 기록)\n",
    "# http://hamait.tistory.com/803  # 반복 실행문의 부분실행 모듈\n",
    "{k: movie_dict[k] for k in it.islice(movie_dict.keys(), 10) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 데이터 전처리 및 LDA 모델 만들기\n",
    "LDA 분석을 위한 전처리를 시행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LDA 분석을 위한 전처리를 시행\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tknzr = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps = True) # 전처리시 제거용 token 생성\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 불용어 제거함수\n",
    "class GenSimCorpus(object):\n",
    "    def __init__(self, texts, stoplist = [], bestwords = [], stem = False):\n",
    "        self.texts = texts\n",
    "        self.stoplist = stoplist\n",
    "        self.stem = stem\n",
    "        self.bestwords = bestwords\n",
    "        self.dictionary = gensim.corpora.Dictionary(self.iter_docs(texts, stoplist))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __iter__(self):\n",
    "        for tokens in self.iter_docs(self.texts, self.stoplist):\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "    def iter_docs(self,texts, stoplist):\n",
    "        for text in texts:\n",
    "            if self.stem: # 불용어를 제거한다\n",
    "                yield (stemmer.stem(w) for w in [x for x in tknzr.tokenize(text) if x not in stoplist])\n",
    "            else:\n",
    "                if len(self.bestwords)>0:\n",
    "                    yield (x for x in tknzr.tokenize(text) if x in self.bestwords)\n",
    "                else:\n",
    "                    yield (x for x in tknzr.tokenize(text) if x not in stoplist)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370.3987 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(7,\n",
       "  '0.000*\" \" + 0.000*\"\\n\" + 0.000*\"\\'\" + 0.000*\"-\" + 0.000*\"one\" + 0.000*\"film\" + 0.000*\"movie\" + 0.000*\"like\" + 0.000*\"much\" + 0.000*\"good\"'),\n",
       " (9,\n",
       "  '0.206*\" \" + 0.015*\"\\n\" + 0.012*\"=\" + 0.004*\"\\'\" + 0.003*\"-\" + 0.001*\"92s\" + 0.001*\"one\" + 0.001*\"film\" + 0.001*\"classic\" + 0.000*\"movie\"'),\n",
       " (2,\n",
       "  '0.726*\" \" + 0.037*\"\\n\" + 0.017*\"\\'\" + 0.009*\"-\" + 0.003*\"film\" + 0.002*\"one\" + 0.002*\"movie\" + 0.001*\"like\" + 0.001*\"even\" + 0.001*\"time\"')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA 모델을 훈련시킨다\n",
    "num_topics = 10; t0 = time()\n",
    "corpus = GenSimCorpus(tot_textreviews, stoplist, [], False) # text 를 전처리 후 token을 걸러낸다\n",
    "dict_lda = corpus.dictionary                                # 색인 용이한 {dict}로 객체를 변경한다\n",
    "\n",
    "from gensim import models\n",
    "lda = models.LdaModel(corpus, \n",
    "                      num_topics = num_topics, \n",
    "                      id2word = dict_lda,\n",
    "                      passes = 10,\n",
    "                      iterations = 50)\n",
    "print(round(time()-t0, 4), 'sec')\n",
    "lda.show_topics(num_topics = 3) # 완성된 LDA 모델을 test 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03  LDA 모델 다듬기\n",
    "이상치 데이터 솎아내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(18480 unique tokens: ['unexpected', 'scary', 'original', 'caught', 'guard']...)\n"
     ]
    }
   ],
   "source": [
    "# 출현빈도 이상치를 갖는 데이터는 제거한다 (1000보다 많이 노출, 3보다 적게 노출된 데이터는 제거)\n",
    "out_ids = [tokenid    for tokenid, docfreq  in  dict_lda.dfs.items() \n",
    "                      if docfreq > 1000 or docfreq < 3 ]\n",
    "import copy\n",
    "dict_lfq = copy.deepcopy(dict_lda)\n",
    "dict_lfq.filter_tokens(out_ids)\n",
    "dict_lfq.compactify()\n",
    "print(dict_lfq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: 2000 \n",
      " [(0, 1), (1, 2), (2, 5), (3, 1), (4, 1), (5, 1), (6, 1), (7, 2), (8, 6), (9, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dict_lfq.doc2bow( tknzr.tokenize( text ))      for text in tot_textreviews]\n",
    "print('Corpus:',len(corpus),'\\n', corpus[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03  LDA 모델 훈련 시키기\n",
    "특정 주제에 해당되는 token 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic  0   words:  0.003*\"characters\" + 0.003*\"life\" + 0.003*\"see\" + 0.003*\"films\" + 0.003*\"love\" + 0.003*\"best\" + 0.002*\"little\" + 0.002*\"know\" + 0.002*\"really\" + 0.002*\"never\"\n",
      "topic  1   words:  0.007*\"/\" + 0.004*\"really\" + 0.004*\"people\" + 0.004*\"see\" + 0.003*\"bad\" + 0.003*\"scene\" + 0.003*\"plot\" + 0.003*\"go\" + 0.003*\"another\" + 0.003*\"something\"\n",
      "topic  2   words:  0.010*\"/\" + 0.004*\"see\" + 0.004*\"10\" + 0.004*\"life\" + 0.003*\"characters\" + 0.003*\"man\" + 0.003*\"scene\" + 0.003*\"little\" + 0.003*\"plot\" + 0.002*\"something\"\n",
      "topic  3   words:  0.003*\"action\" + 0.003*\"plot\" + 0.003*\"scenes\" + 0.003*\"life\" + 0.003*\"/\" + 0.002*\"characters\" + 0.002*\"however\" + 0.002*\"great\" + 0.002*\"man\" + 0.002*\"best\"\n",
      "topic  4   words:  0.003*\"films\" + 0.003*\"plot\" + 0.003*\"never\" + 0.003*\"many\" + 0.003*\"new\" + 0.003*\"life\" + 0.003*\"really\" + 0.003*\"could\" + 0.003*\"seems\" + 0.003*\"`\"\n",
      "topic  5   words:  0.004*\"new\" + 0.004*\"characters\" + 0.003*\"star\" + 0.003*\"little\" + 0.003*\"life\" + 0.003*\"disney\" + 0.003*\"family\" + 0.003*\"movies\" + 0.003*\"people\" + 0.002*\"films\"\n",
      "topic  6   words:  0.011*\"`\" + 0.003*\"see\" + 0.003*\"bad\" + 0.003*\"really\" + 0.003*\"people\" + 0.002*\"could\" + 0.002*\"action\" + 0.002*\"know\" + 0.002*\"never\" + 0.002*\"characters\"\n",
      "topic  7   words:  0.003*\"love\" + 0.003*\"effects\" + 0.003*\"titanic\" + 0.003*\"/\" + 0.003*\"best\" + 0.003*\"people\" + 0.003*\"ship\" + 0.003*\"tarantino\" + 0.003*\"great\" + 0.002*\"see\"\n",
      "topic  8   words:  0.003*\"characters\" + 0.003*\"bad\" + 0.003*\"people\" + 0.003*\"action\" + 0.003*\"director\" + 0.003*\"little\" + 0.003*\"man\" + 0.003*\"scene\" + 0.003*\"could\" + 0.003*\"plot\"\n",
      "topic  9   words:  0.004*\"alien\" + 0.003*\"films\" + 0.003*\"characters\" + 0.003*\"big\" + 0.003*\"really\" + 0.003*\"see\" + 0.003*\"movies\" + 0.003*\"star\" + 0.002*\"bad\" + 0.002*\"funny\"\n",
      "256.932 sec\n"
     ]
    }
   ],
   "source": [
    "# 10개 주제에 관련되어 모델을 훈련시킨다\n",
    "# 결과는 주제별 나타날 확률이 가장높은 10개의 단어를 반환한다.\n",
    "t0 = time()\n",
    "lda_lfq = models.LdaModel(corpus, \n",
    "                          num_topics = num_topics,\n",
    "                          id2word = dict_lfq,\n",
    "                          passes = 10,\n",
    "                          iterations = 50,\n",
    "                          alpha = 0.01,\n",
    "                          eta = 0.01)\n",
    "\n",
    "for t in range(num_topics):\n",
    "    print ('topic ',t,'  words: ',lda_lfq.print_topic(t,topn=10))\n",
    "print(round(time()-t0, 4), 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.004*\"alien\" + 0.003*\"films\" + 0.003*\"characters\" + 0.003*\"big\" + 0.003*\"really\" + 0.003*\"see\" + 0.003*\"movies\" + 0.003*\"star\" + 0.002*\"bad\" + 0.002*\"funny\"'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_lfq.print_topic(t, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Rain (1998)\n",
      "Roommates (1995)\n",
      "Matrix, The (1999)\n",
      "Saving Private Ryan (1998)\n",
      "Deuce Bigalow: Male Gigolo (1999)\n",
      "Maximum Risk (1996)\n"
     ]
    }
   ],
   "source": [
    "# 실제 가장 높은 확률의 데이터 6위내 영화 쿼리를 추출한다\n",
    "def GenerateDistrArrays(corpus):\n",
    "    for i, dist in enumerate(corpus[:100]):  # corpus[:100] 모집단 데이터를 크게하면 된다 \n",
    "        dist_array = np.zeros(num_topics)\n",
    "        for d in dist:\n",
    "            dist_array[d[0]] = d[1]\n",
    "        if dist_array.argmax() == 6:\n",
    "            print(tot_titles[i])\n",
    "\n",
    "corpus_lda = lda_lfq[corpus]\n",
    "GenerateDistrArrays(corpus_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LDA 알고리즘의 군집화 결과를 보면, \n",
    "# 나름 유사한 영화끼리 군집화가 잘 되어있음을 알 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br></br>\n",
    "## 6 오피니언 마이닝 (감성분석) - (사후 처리정보 2)\n",
    "글쓴 사람의 의견(긍정/ 중립/ 부정)을 추출하는 도구 : Opinion mining (sentiment analysis)\n",
    "\n",
    "분류 알고리즘으로, 단어집의 갯수만큼 특징(feature)이 구성되므로, \n",
    "\n",
    "SVM/ Naives Bayes 등의 알고리즘을 사용할 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 데이터 수집 및 목록 만들기\n",
    "DB 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tknzr = WordPunctTokenizer()\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tknzr = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "def PreprocessReviews(text, stop = [], stem = False): #print profile\n",
    "    words = tknzr.tokenize(text)\n",
    "    if stem: words_clean = [stemmer.stem(w)   for w in [ i.lower() \n",
    "                                                         for i in words \n",
    "                                                         if i not in stop] ]\n",
    "    else:    words_clean = [i.lower()    for i in words \n",
    "                                         if i not in stop ]\n",
    "    return words_clean\n",
    "\n",
    "Review = namedtuple('Review', 'words title tags')\n",
    "dir = '/home/markbaum/nltk_data/corpora/movie_reviews/'\n",
    "do2vecstem = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 긍정리뷰 데이터 수집하기\n",
    "reviews_pos, cnt = [], 0\n",
    "for filename in [f     for f in os.listdir(dir+'pos/')    if str(f)[0] != '.']:\n",
    "    f = open(dir + 'pos/' + filename, 'r')\n",
    "    id = filename.split('.')[0].split('_')[1]\n",
    "    reviews_pos.append( Review( PreprocessReviews( f.read(),\n",
    "                                                  stoplist, do2vecstem),\n",
    "                                movie_dict[id],  ['pos_' + str(cnt)] ) )\n",
    "    cnt += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 부정리뷰 데이터 수집하기\n",
    "reviews_neg, cnt = [], 0\n",
    "for filename in [f      for f in os.listdir(dir+'neg/')    if str(f)[0] != '.']:\n",
    "    f = open(dir + 'neg/' + filename, 'r')\n",
    "    id = filename.split('.')[0].split('_')[1]\n",
    "    reviews_neg.append( Review( PreprocessReviews( f.read(),\n",
    "                                                  stoplist, do2vecstem),\n",
    "                               movie_dict[id], ['neg_'+str(cnt)] ) )\n",
    "    cnt += 1\n",
    "\n",
    "tot_reviews = reviews_pos + reviews_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 수집 데이터로 Train/ Test 데이터 분류하기\n",
    "Train/ Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 --> 800\n",
      "1600\n"
     ]
    }
   ],
   "source": [
    "# Train(80%)/ Test(20%) 데이터로 분리하기\n",
    "def word_features(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "neg_features = [(word_features(r.words), 'neg') for r in reviews_neg]\n",
    "pos_features = [(word_features(r.words), 'pos') for r in reviews_pos]\n",
    "portion_pos = int( len(pos_features) * 0.8 )\n",
    "portion_neg = int( len(neg_features) * 0.8 )\n",
    "print (portion_pos,'-->',portion_neg)\n",
    "train_features = neg_features[:portion_neg] + pos_features[:portion_pos]\n",
    "print (len(train_features))\n",
    "test_features  = neg_features[portion_neg:] + pos_features[portion_pos:]\n",
    "#shuffle(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test on:  400\n",
      "error rate:  0.2875\n"
     ]
    }
   ],
   "source": [
    "# 나이브 베이즈 훈련모델 테스트\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_features)\n",
    "\n",
    "print ('test on: ', len(test_features)); err = 0\n",
    "for r in test_features:\n",
    "    sent = classifier.classify(r[0])\n",
    "    if sent != r[1]:    err += 1.\n",
    "print ('error rate: ', err/float(len(test_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X^2 테스트를 통해서 높은 빈도로 발생한는 bi-gram '연어(collocation)' 목록을 추출한다 \n",
    "# cf) 자연어에서의 '연어(collocation)' : 특정한 뜻을 나타낼 떄 함께 쓰이는 단어의 결합\n",
    "# train 결과 오류가 28%로 상당히 높다\n",
    "# 성능을 개선할 필요가 존재한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 $X^2$ 테스트를 활용\n",
    "발생빈도가 높은 500개의 바이그램을 선별 후, Naive Bayes 분류기로 훈련시킨다\n",
    "\n",
    "단어(i,j)로 된 bi-gram의 평균빈도 : $ O_{ij}$\n",
    "\n",
    "bi-gram(i,j)의 기대빈도 : $ E_{ij}$\n",
    "$$ X^2 = \\frac{\\sum_{i=0,j=0}^{1,1}(O_{ij}-E_{ij})^2}{E_{ij}} $$\n",
    "ex) bi-gram 의 $X^2$을 계산한 경우 : $E = (\\frac{O_{00}+O_{01}}{N} * \\frac{O_{00}+O_{10}}{N})N $\n",
    "\n",
    "case1) $X^2$이 커지면 : 평균빈도와 $ O_{ij}$ 기대빈도가 $ E_{ij}$ 벌어져서 \n",
    "\n",
    "<strong>영가설이 기각</strong>되고, 이는 <strong>더 많은 정보를 포함한다</strong>는 결론에 도달한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from random import shuffle\n",
    "\n",
    "# Bi-gram 선별기\n",
    "def bigrams_words_features(words, nbigrams = 200, measure = BigramAssocMeasures.chi_sq):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(measure, nbigrams)\n",
    "    return dict([(ngram, True) for ngram in itertools.chain(words, bigrams)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos_data: 800 ---> Nag_data: 800 Train : 1600\n"
     ]
    }
   ],
   "source": [
    "# bi-gram 선별기로 token의 데이터 선별\n",
    "neg_features = [(bigrams_words_features(r.words,500), 'neg') for r in reviews_neg]\n",
    "pos_features = [(bigrams_words_features(r.words,500), 'pos') for r in reviews_pos]\n",
    "portion_pos = int(len(pos_features)*0.8)\n",
    "portion_neg = int(len(neg_features)*0.8)\n",
    "\n",
    "train_features = neg_features[:portion_pos ] + pos_features[:portion_neg ]\n",
    "print ('Pos_data:', portion_pos,'---> Nag_data:', portion_neg, 'Train :', len(train_features))\n",
    "classifier = NaiveBayesClassifier.train(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test on:  400\n",
      "error rate:  0.2025\n"
     ]
    }
   ],
   "source": [
    "## test bigram\n",
    "test_features = neg_features[portion_neg:] + pos_features[portion_pos:]\n",
    "\n",
    "from random import shuffle\n",
    "shuffle(test_features)\n",
    "err = 0\n",
    "print( 'test on: ',len(test_features))\n",
    "for r in test_features:\n",
    "    sent = classifier.classify(r[0])\n",
    "    #print r[1],'-pred: ',sent\n",
    "    if sent != r[1]: err +=1.\n",
    "print('error rate: ',err/float(len(test_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 테스트 결과\n",
    "# 빈도가 높은 500개를 선별해서 훈련한 결과\n",
    "# 오차율이 20% 초반으로 낮아졌다, 즉 성능의 개선이 이루어졌다\n",
    "# X^2 테스트는 가장 유익한 단어의 추출시 유용성이 증명되었다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X^2 에 대한 정리\n",
    "# 단어의 중요도를 점수화 하기 위한 단어의 빈도가\n",
    "# 긍정문서와 부정문서에서 어떻게 다른지를 측정한다\n",
    "# ex) X^2값이 긍정리뷰서 높고, 부정리뷰서 낮은경우, 긍정리뷰의 정보에 해당\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04 Corpus 를 정제후 테스트 실행\n",
    "수집한 데이터 중 가장 유용한 10,000개 단어를 추출 후 학습한다\n",
    "\n",
    "전체 corpus 데이터에서 총빈도, 긍정빈도, 부정빈도의 계산결과로 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.604 sec\n"
     ]
    }
   ],
   "source": [
    "import nltk.classify.util, nltk.metrics\n",
    "tot_poswords = [val for l in [r.words for r in reviews_pos] for val in l]\n",
    "tot_negwords = [val for l in [r.words for r in reviews_neg] for val in l]\n",
    "\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "word_fd = FreqDist()\n",
    "label_word_fd = ConditionalFreqDist(); t0 = time()\n",
    " \n",
    "for word in tot_poswords:\n",
    "    word_fd[word.lower()] +=1\n",
    "    label_word_fd['pos'][word.lower()] +=1\n",
    "\n",
    "for word in tot_negwords:\n",
    "    word_fd[word.lower()] +=1\n",
    "    label_word_fd['neg'][word.lower()] +=1\n",
    "pos_words = len(tot_poswords)\n",
    "neg_words = len(tot_negwords)\n",
    "tot_words = pos_words + neg_words ; print(round(time()-t0,3), 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "759.081 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bake',\n",
       " 'biograph',\n",
       " 'formal',\n",
       " 'gosselaar',\n",
       " 'gottfri',\n",
       " 'hatchett',\n",
       " 'mash',\n",
       " 'purpos',\n",
       " 'sensat',\n",
       " 'underdevelop'}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 주요한 긍정, 부정 데이터 중 주요정보를 갖는 corpus 10,000개를 선별한다\n",
    "word_scores = {}; t0 = time()\n",
    " \n",
    "for word, freq in word_fd.items():\n",
    "    pos_score = BigramAssocMeasures.chi_sq(label_word_fd['pos'][word],\n",
    "                (freq, pos_words), tot_words)\n",
    "    neg_score = BigramAssocMeasures.chi_sq(label_word_fd['neg'][word],\n",
    "                (freq, neg_words), tot_words)\n",
    "    word_scores[word] = pos_score + neg_score\n",
    "print ('total: ',len(word_scores))\n",
    "best = sorted(word_scores.items(), key = lambda x : x[1], reverse=True)[:10000]\n",
    "best_words = set([w for w, s in best]) ; print(round(time()-t0,3), 'sec')\n",
    "{k for k in it.islice(best_words, 10) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 - 800 ==> 1600\n"
     ]
    }
   ],
   "source": [
    "# 정제한 결과를 활용하여 Naive Bayes 분류기를 훈련시킨다\n",
    "# 훈련을 위해서 긍/부정 데이터 분류\n",
    "def best_words_features(words):\n",
    "    return dict([(word, True) for word in words if word in bestwords])\n",
    "\n",
    "neg_features = [(best_words_features(r.words), 'neg') for r in reviews_neg]\n",
    "pos_features = [(best_words_features(r.words), 'pos') for r in reviews_pos]\n",
    "portion_pos = int(len(pos_features)*0.8)\n",
    "portion_neg = int(len(neg_features)*0.8)\n",
    "train_features = neg_features[:portion_pos] + pos_features[:portion_neg]\n",
    "print (portion_pos,'-',portion_neg,'==>',len(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test on:  400\n",
      "error rate:  0.1125\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier.train(train_features)\n",
    "\n",
    "# 데이터를 '카이제곱 분포'로 test 한다\n",
    "test_features = neg_features[portion_neg:] + pos_features[portion_pos:]\n",
    "shuffle(test_features)\n",
    "err = 0\n",
    "print ('test on: ',len(test_features))\n",
    "for r in test_features:\n",
    "    sent = classifier.classify(r[0])\n",
    "    # print(r[1],'-pred: ',sent)\n",
    "    if sent != r[1]:  err +=1.\n",
    "print ('error rate: ',err/float(len(test_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 결과\n",
    "# 테스트 결과 10,000개의 데이터 임에도, 오차율은 11%로 반이상 줄었다\n",
    "# 이를 보다 신뢰하기 위한 결과를 위해서는, 교차 검증 방법을 활용해야 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05 정제 데이터를 활용한 Doc2Vec 분류\n",
    "model_d2v.docvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Doc2Vec 모델을 파라미터 설정\n",
    "from gensim.models import Doc2Vec\n",
    "import multiprocessing\n",
    "\n",
    "num_epochs = 20\n",
    "vec_size  = 500\n",
    "shuffle(tot_reviews)\n",
    "cores = multiprocessing.cpu_count()\n",
    "model_d2v = Doc2Vec(dm = 1, dm_concat = 0, size = vec_size, window = 5, \n",
    "                    negative = 0, hs = 0, min_count = 1, workers = cores)\n",
    "\n",
    "# 정의한 모듈에 데이터를 연셜하여 모델을 생성\n",
    "model_d2v.build_vocab(tot_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 20.554 sec\n",
      "epoch : 2 62.735 sec\n",
      "epoch : 4 104.905 sec\n",
      "epoch : 6 146.174 sec\n",
      "epoch : 8 187.566 sec\n",
      "epoch : 10 229.758 sec\n",
      "epoch : 12 271.316 sec\n",
      "epoch : 14 312.834 sec\n",
      "epoch : 16 354.581 sec\n",
      "epoch : 18 396.102 sec\n",
      "word2vec train ends.. 4170.0 4\n"
     ]
    }
   ],
   "source": [
    "# 훈련의 시작 \n",
    "# https://github.com/RaRe-Technologies/gensim/issues/1284\n",
    "t0 = time()\n",
    "for epoch in range(numepochs):\n",
    "    try:\n",
    "        model_d2v.train(tot_reviews, total_examples = len(tot_reviews), epochs = num_epochs)\n",
    "        model_d2v.alpha *= 0.99\n",
    "        model_d2v.min_alpha = model_d2v.alpha\n",
    "        if epoch % 2 == 0: print ('epoch :',epoch, round(time()-t0,3), 'sec')\n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        break\n",
    "print(\"word2vec train ends..\", round(time()-t0)*(num_epochs/2), 4)\n",
    "model_d2v.docvecs  # 훈련 데이터 저장객체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train (80%), Test (20%) 데이터 분리하기\n",
    "training_size = 2 * int( len( reviews_pos ) * 0.8)\n",
    "train_d2v = np.zeros( (training_size, vec_size))\n",
    "train_labels = np.zeros(training_size)\n",
    "\n",
    "test_size = len(tot_reviews)-training_size\n",
    "test_d2v = np.zeros((test_size, vec_size))\n",
    "test_labels = np.zeros(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnt_train, cnt_test = 0, 0\n",
    "for r in reviews_pos:\n",
    "    name_pos = r.tags[0]\n",
    "    if int(name_pos.split('_')[1]) >= int(training_size/2.):\n",
    "        test_d2v[cnt_test] = model_d2v.docvecs[name_pos]\n",
    "        test_labels[cnt_test] = 1\n",
    "        cnt_test +=1\n",
    "    else:\n",
    "        train_d2v[cnt_train] = model_d2v.docvecs[name_pos]\n",
    "        train_labels[cnt_train] = 1\n",
    "        cnt_train +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for r in reviews_neg:\n",
    "    name_neg = r.tags[0]\n",
    "    if int(name_neg.split('_')[1])>= int(training_size/2.):\n",
    "        test_d2v[cnt_test] = model_d2v.docvecs[name_neg]\n",
    "        test_labels[cnt_test] = 0\n",
    "        cnt_test +=1\n",
    "    else:\n",
    "        train_d2v[cnt_train] = model_d2v.docvecs[name_neg]       \n",
    "        train_labels[cnt_train] = 0\n",
    "        cnt_train +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticReg</th>\n",
       "      <td>0.5275</td>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.5400</td>\n",
       "      <td>2.266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC(linear)</th>\n",
       "      <td>0.5275</td>\n",
       "      <td>2.089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Score   time\n",
       "Name                      \n",
       "LogisticReg  0.5275  0.037\n",
       "SVC          0.5400  2.266\n",
       "SVC(linear)  0.5275  2.089"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train log regre\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "functions = [('LogisticReg', LogisticRegression() ), \n",
    "             ('SVC', SVC()),\n",
    "             ('SVC(linear)', SVC(kernel = 'linear')),]\n",
    "\n",
    "result = []\n",
    "for name, clf in functions:\n",
    "    t0 = time()\n",
    "    clf.fit(train_d2v, train_labels)\n",
    "    result.append([name, clf.score( test_d2v, test_labels), round(time()-t0, 3)])\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(result, columns=['Name', 'Score', 'time'])\n",
    "df.set_index('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 결과\n",
    "# 훈련결과 정확도 값들이 낮은데\n",
    "# 이는 훈련데이터 집합의 크기가 작아서 나타난 결과이다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
