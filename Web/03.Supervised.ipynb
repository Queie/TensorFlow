{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chapter 3 -  Supervised Learning\n",
    "머신 러닝 기법 : 지도학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 1 Train으로 생성한 모델 평가\n",
    "bias : 예측 결과의 편향 (알고리즘의 가정의 문제로 발생)\n",
    "\n",
    "variance : 분산 (포인트 $X_1$ 에 대한 잘못 예측된 레이블)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 Mean Square Error (MSE : 평균 제곱 오차)\n",
    "오차 제곱의 평균 : $ MSE(\\sigma^-) ={1 \\over N} \\sum d(y_t-y_t^p)^2  $\n",
    "\n",
    "자율학습 방법의 평가척도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 Generalized linear models (일반화 선형모델)\n",
    "Function ($E_i는 모델의 오차$) : $X$는 1개</br>\n",
    "<p>$ y_i = \\sum \\sigma_j x_j^i + E_i $</br>\n",
    "<p>$ = h_\\sigma(x^i) + E_i $   \n",
    "\n",
    "Cost : Stochastic Gradient Descent (확률 내리막 경사법)</br>\n",
    "<p>$\\sigma_i$ 값에 따라 최종값 근처의 진폭이 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 linear regression (선형회귀)\n",
    "가장 단순한 알고리즘 = 다변량 선형 회귀모델 (univariate liner regression)\n",
    "\n",
    "선형 모델 회귀식 : '선형예측함수'를 사용해 회귀식을 모델링, 알려지지 않은 파라미터는 데이터로부터 추정</br>\n",
    "<p> $ h_\\sigma(x^i) = \\sigma_0 + \\sigma_1x_1^i + \\sigma_2x_2^i +...$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04 ridge regression (리지회귀) \n",
    "Tikhonov regularization (타코노프 정규화)\n",
    "\n",
    "비용함수에 \"정규화 항\"을 추가, 모델에 제약(Constraint)을 부과\n",
    "\n",
    "과적합 모델을 방지 : $ J = { 1 \\over 2}\\sum (y_i - h_\\sigma(x^i))^2 + { \\Gamma \\over 2}\\sum_j^2 $</p></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05 Lasso regression(라소 회귀)\n",
    "<strong>정규화 항이 \"파라미터 절대값\"의 합</strong> 만 제외한, 나머지는 '04.리지회귀' 와 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06 Logistic regression(로지스틱 회귀)\n",
    "분류를 위한 '조건부 확률'로 로지스틱 함수를 사용 : \n",
    "\n",
    "#### 1) function\n",
    "Target 값이 범주형 (0,1) 의 값\n",
    "\n",
    "$ h_\\sigma(x^i) $ 는 0과 1 사이의 연속된 값을 갖는다  (cf> $ h_\\sigma(x^i) $ 는 확률적 분류기(Probabilistic  classifier))\n",
    "<p>$ h_\\sigma(x^i) = \\begin{matrix} > 0.5 : 1\\\\ < 0.5 : 0\\\\\\end{matrix}$   \n",
    "\n",
    "#### 2) Cost function (일반화 선형 모델에 대한 확률적 해석)\n",
    "우도를 최대화 하는 것이, 비용함수를 최소화 한다 (내리막 경사법 함께 적용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 07 K-근접 이웃법\n",
    "숫자형 데이터간의 거리를 측정\n",
    "\n",
    "민코스키 방법(유클리드, 맨해튼 함수를 일반화한 공식) </br>\n",
    "<p>$ (\\sum(|x_j^k - x_j^t|)^q )^{1\\over q}$    ...cf) q = 1 (맨해튼) , 2 (유클리드), 무한대 (슈프림)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 08. 나이브 베이즈 (베이지안 정리)\n",
    "나이브 베이즈 (베이지안 정리) : 사후확률을 최대화 해주는 레이블 $P$를 찾는다 <br/>\n",
    "\n",
    "  <p> $ P = argmax  P(y|x_0, x_1..... x_M) $  - 종속변수 y 가 최대가 되는 독립변수 x를 찾는 함수</p></br>\n",
    "\n",
    "다항 분포 나이브 베이즈 \n",
    "\n",
    "가우시안 나이브 베이즈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09 의사결정트리\n",
    "- Decision Node : 결정 노드 (2개 이상의 가지가 있고, 의사결정이 나뉠 떄)\n",
    "- Leaf Node : 잎 노드 (데이터를 분류하는 노드)\n",
    "- 최적의 분할 규칙 : <strong>불순도 함수 $I$</strong>를 최소화 하는 것\n",
    "    $$ (t_k^q, q) = argmin.I(t_k^j, j) $$\n",
    "- Cost 함수 \n",
    "    1. Entropy(엔트로피) :  $ H(S_b) = - \\sum P_b \\log_2 P_b $\n",
    "    2. gini impurity(지니불순도) : $ H(S_b) = - \\sum P_b (1- P_b) $\n",
    "    3. misclassification(오분류) : $ H(S_b) = 1 - max(P_b) $\n",
    "    4. mean squared error(평균제곱오차) : $ H(S_b) = { 1 \\over N_b} \\sum (y_i - \\sigma_b)^2 $회귀분석에서 사용\n",
    "- 집합이 클경우 일반화가 잘 안되는 단점 (차원 축소가 필요)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 서포트 벡터 머신\n",
    "데이터 포인트에서 결정 경계까지 거리를 최대화\n",
    "\n",
    "여백을 최대화 하는 $w, b$의 값을 찾는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11 커널 트릭\n",
    "데이터 집합이 선형으로 분리되지 않을 때, 다른 차원의 공간으로 매핑을 한다.\n",
    "\n",
    "커널함수 (2차원 분리가능) : $ K(x^i, x^j) = e^{-|x^i-x^j|^2 \\over 2\\sigma^2} $\n",
    "1. liner Kernel\n",
    "2. RBF Basis Kernel\n",
    "3. Polynomial Kernel\n",
    "4. Sigmoid Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 2 방법간의 비교\n",
    "Cross Validation 절차에 따라 모델을 평가한다\n",
    "1. k-1개 폴드를 훈련집합으로 모델을 훈련\n",
    "2. 나머지 1 폴드로 테스트\n",
    "3. 시작점에 정한 폴드 갯수 k만큼 반복\n",
    "4. 정확도는 k번 반복으로 얻은 정확도의 평균을 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 3 Regression Problem\n",
    "회귀분석 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd; import numpy as np\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Read_CSV & shuffle the data\n",
    "df = pd.read_csv('./data/housing.data',delim_whitespace=True ,header=None)\n",
    "df = df.iloc[np.random.permutation(len(df))]\n",
    "X = df[df.columns[:-1]].values\n",
    "Y = df[df.columns[-1]].values\n",
    "cv = 10  # Train 반복횟수\n",
    "X.shape   # 506 Index, 13 필드 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 01 선형회귀 분석\n",
    "LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [ LinearRegression(),  \n",
    "              Ridge(alpha = 1.), \n",
    "              Lasso(alpha = 0.1),  \n",
    "              DecisionTreeRegressor(random_state = 0),\n",
    "              RandomForestRegressor(n_estimators = 50, max_depth = None, random_state = 0),\n",
    "              svm.SVR(epsilon = 0.2, kernel = 'linear', C = 1),\n",
    "              svm.SVR(epsilon = 0.2, kernel = 'rbf', C = 1.),\n",
    "              KNeighborsRegressor(), ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for fun in functions:\n",
    "    regression = fun; t0 = time()\n",
    "    scores = model_selection.cross_val_score(regression, X, Y, cv = cv)\n",
    "    predicted = model_selection.cross_val_predict(regression, X,Y, cv = cv)\n",
    "    result.append([ scores.mean(),   scores.std() * 2,  mean_squared_error(Y, predicted), round(time()-t0,4) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean R^2</th>\n",
       "      <th>+/- R^2</th>\n",
       "      <th>MSE</th>\n",
       "      <th>time(sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>0.862999</td>\n",
       "      <td>0.180534</td>\n",
       "      <td>10.701370</td>\n",
       "      <td>2.558500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTree</th>\n",
       "      <td>0.746390</td>\n",
       "      <td>0.319024</td>\n",
       "      <td>20.277609</td>\n",
       "      <td>0.095400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear</th>\n",
       "      <td>0.702841</td>\n",
       "      <td>0.263919</td>\n",
       "      <td>23.778265</td>\n",
       "      <td>0.022800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.701164</td>\n",
       "      <td>0.273166</td>\n",
       "      <td>23.948166</td>\n",
       "      <td>0.061300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.688348</td>\n",
       "      <td>0.279082</td>\n",
       "      <td>25.062058</td>\n",
       "      <td>0.045300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM(linear)</th>\n",
       "      <td>0.676177</td>\n",
       "      <td>0.369228</td>\n",
       "      <td>26.036198</td>\n",
       "      <td>78.693300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.531184</td>\n",
       "      <td>0.282044</td>\n",
       "      <td>38.165521</td>\n",
       "      <td>0.024700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM(rbf)</th>\n",
       "      <td>-0.000911</td>\n",
       "      <td>0.076034</td>\n",
       "      <td>83.662161</td>\n",
       "      <td>0.324800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>: mean</th>\n",
       "      <td>0.613524</td>\n",
       "      <td>0.255379</td>\n",
       "      <td>31.453918</td>\n",
       "      <td>10.228262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              mean R^2   +/- R^2        MSE  time(sec)\n",
       "RandomForest  0.862999  0.180534  10.701370   2.558500\n",
       "DecisionTree  0.746390  0.319024  20.277609   0.095400\n",
       "Linear        0.702841  0.263919  23.778265   0.022800\n",
       "Ridge         0.701164  0.273166  23.948166   0.061300\n",
       "Lasso         0.688348  0.279082  25.062058   0.045300\n",
       "SVM(linear)   0.676177  0.369228  26.036198  78.693300\n",
       "KNN           0.531184  0.282044  38.165521   0.024700\n",
       "SVM(rbf)     -0.000911  0.076034  83.662161   0.324800\n",
       ": mean        0.613524  0.255379  31.453918  10.228262"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "result = pd.DataFrame(result, columns=['mean R^2','+/- R^2','MSE', 'time(sec)'],\n",
    "                      index = ['Linear', 'Ridge', 'Lasso', 'DecisionTree', \n",
    "                               'RandomForest', 'SVM(linear)', 'SVM(rbf)', 'KNN'])\n",
    "result_one = result.sort_values(by = 'mean R^2', ascending = False)     # mean R^2 (적합도) 를 기준으로 데이터를 정렬한다\n",
    "result_one.loc[': mean'] = result_one.mean()                       # 분석한 데이터의 평균을 맨 마지막 행에 추가한다\n",
    "result_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 결과\n",
    "# 가장 적합한 모델은 Random Forest 로 Mean R^2 값이 크고,  MSE 값이 작다\n",
    "# SVM('liner')는 나름 적합한 결과를 출력하나 (단 시간이 오래걸린다)\n",
    "# SVM('rbf')는 최악의 결과값을 출력한다 선정된다\n",
    "# 라소, 리지회귀는 거의 엇비슷한 결과를 출력한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 02 재귀적 특징 축소방법\n",
    "Recursive Feature Elimination Method : 특정 모델의 최적의 결과값을 출력한다\n",
    "\n",
    "<strong>가장 큰 절대 가중치를 갖는 속성</strong>을 고려하여, 원하는 갯수의 특징을 선택할때까지 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "best_features = 4\n",
    "\n",
    "functions = [ LinearRegression(),  \n",
    "              Ridge(alpha = 1.), \n",
    "              Lasso(alpha = 0.1),  \n",
    "              DecisionTreeRegressor(random_state = 0),\n",
    "              RandomForestRegressor(n_estimators = 50, max_depth = None, random_state = 0),\n",
    "              svm.SVR(epsilon = 0.2, kernel = 'linear', C = 1),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for regression in functions:\n",
    "    rfe_regression = RFE(regression, best_features).fit(X, Y) ; t0 = time()\n",
    "    mask = np.array( rfe_regression.support_ )\n",
    "    scores = model_selection.cross_val_score(regression, X[:,mask], Y, cv=cv)\n",
    "    predicted = model_selection.cross_val_predict(regression, X[:,mask],Y, cv=cv)\n",
    "    result.append([scores.mean(),  scores.std() * 2,  mean_squared_error(Y,predicted), round(time()-t0,4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean R^2</th>\n",
       "      <th>+/- R^2</th>\n",
       "      <th>MSE</th>\n",
       "      <th>time(sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>0.818305</td>\n",
       "      <td>0.230439</td>\n",
       "      <td>14.084401</td>\n",
       "      <td>1.67360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTree</th>\n",
       "      <td>0.703744</td>\n",
       "      <td>0.300101</td>\n",
       "      <td>24.236225</td>\n",
       "      <td>0.04070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.655983</td>\n",
       "      <td>0.309719</td>\n",
       "      <td>27.692462</td>\n",
       "      <td>0.02410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear</th>\n",
       "      <td>0.586910</td>\n",
       "      <td>0.492484</td>\n",
       "      <td>33.347803</td>\n",
       "      <td>0.02110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.586163</td>\n",
       "      <td>0.494563</td>\n",
       "      <td>33.420252</td>\n",
       "      <td>0.02320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM(linear)</th>\n",
       "      <td>0.574089</td>\n",
       "      <td>0.511495</td>\n",
       "      <td>34.276485</td>\n",
       "      <td>0.36680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>: mean</th>\n",
       "      <td>0.654199</td>\n",
       "      <td>0.389800</td>\n",
       "      <td>27.842938</td>\n",
       "      <td>0.35825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              mean R^2   +/- R^2        MSE  time(sec)\n",
       "RandomForest  0.818305  0.230439  14.084401    1.67360\n",
       "DecisionTree  0.703744  0.300101  24.236225    0.04070\n",
       "Lasso         0.655983  0.309719  27.692462    0.02410\n",
       "Linear        0.586910  0.492484  33.347803    0.02110\n",
       "Ridge         0.586163  0.494563  33.420252    0.02320\n",
       "SVM(linear)   0.574089  0.511495  34.276485    0.36680\n",
       ": mean        0.654199  0.389800  27.842938    0.35825"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame(result, columns=['mean R^2','+/- R^2','MSE', 'time(sec)'],\n",
    "                      index = ['Linear', 'Ridge', 'Lasso', 'DecisionTree', \n",
    "                               'RandomForest', 'SVM(linear)'])\n",
    "result_rfe = result.sort_values(by = 'mean R^2', ascending = False)     # mean R^2 (적합도) 를 기준으로 데이터를 정렬한다\n",
    "result_rfe.loc[': mean'] = result_rfe.mean()                       # 분석한 데이터의 평균을 맨 마지막 행에 추가한다\n",
    "result_rfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 결과\n",
    "# SVM는 선형모델만 가능, KNN은 가중치가 제공되지 않아서 불가능\n",
    "# 반복한 결과도 Random Forest 가 가장 효과적인 모델로 결론이 도출된다\n",
    "# 반복한 결과 약간의 결과값 향상이 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 4 Classification Problem\n",
    "분류 문제의 해결 : Precision(정확률), Recall(재현율), F-척도(f-measure)\n",
    "\n",
    "Data : https://archive.ics.uci.edu/ml/datasets/car+evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>high</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1  2  3      4     5      6\n",
       "0  vhigh  vhigh  2  2  small   low  unacc\n",
       "1  vhigh  vhigh  2  2  small   med  unacc\n",
       "2  vhigh  vhigh  2  2  small  high  unacc\n",
       "3  vhigh  vhigh  2  2    med   low  unacc\n",
       "4  vhigh  vhigh  2  2    med   med  unacc"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 모든 컬럼의 데이터 형식을 'category'로 변환\n",
    "df = pd.read_csv('./data/car.data', delimiter=',' ,header=None)\n",
    "for i in range(len(df.columns)):\n",
    "    df[i] = df[i].astype('category')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "구매가격 {'high': 0, 'low': 1, 'med': 2, 'vhigh': 3}\n",
      "유지비용 {'high': 0, 'low': 1, 'med': 2, 'vhigh': 3}\n",
      "문갯수 {'2': 0, '3': 1, '4': 2, '5more': 3}\n",
      "탑승인원 {'2': 0, '4': 1, 'more': 2}\n",
      "트렁크크기 {'big': 0, 'med': 1, 'small': 2}\n",
      "안정성 {'high': 0, 'low': 1, 'med': 2}\n",
      "추천(target) {'acc': 0, 'good': 1, 'unacc': 2, 'vgood': 3}\n"
     ]
    }
   ],
   "source": [
    "# 글자로 등급 데이터를 feature 숫자로 변환\n",
    "# map catgories to values\n",
    "map0 = dict( zip( df[0].cat.categories, range( len(df[0].cat.categories ))))\n",
    "map1 = dict( zip( df[1].cat.categories, range( len(df[1].cat.categories ))))\n",
    "map2 = dict( zip( df[2].cat.categories, range( len(df[2].cat.categories ))))\n",
    "map3 = dict( zip( df[3].cat.categories, range( len(df[3].cat.categories ))))\n",
    "map4 = dict( zip( df[4].cat.categories, range( len(df[4].cat.categories ))))\n",
    "map5 = dict( zip( df[5].cat.categories, range( len(df[5].cat.categories ))))\n",
    "map6 = dict( zip( df[6].cat.categories, range( len(df[6].cat.categories ))))\n",
    "\n",
    "names = ['구매가격', '유지비용', '문갯수', '탑승인원', '트렁크크기', '안정성', '추천(target)'] \n",
    "maps = [map0, map1, map2, map3, map4, map5, map6]\n",
    "for i in range(len(names)):\n",
    "    print(names[i], maps[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 5, 3, 6, 0, 8, 9, 2, 4, 1])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# table 의 내용을 숫자로 매핑한다\n",
    "cat_cols = df.select_dtypes(['category']).columns  # 컬럼을 추출\n",
    "df[cat_cols] = df[cat_cols].apply(lambda x: x.cat.codes)\n",
    "\n",
    "df = df.iloc[np.random.permutation(len(df))]       # index 를 뒤섞는다\n",
    "print(np.random.permutation(10))  # 순서 integer 를 흩트러트린다\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_f1 = pd.DataFrame(columns = ['method'] + sorted(map6, key = map6.get))\n",
    "df_precision = pd.DataFrame(columns = ['method'] + sorted(map6, key = map6.get))\n",
    "df_recall = pd.DataFrame(columns = ['method'] + sorted(map6, key = map6.get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def CalcMeasures(method,y_pred,y_true,df_f1=df_f1\n",
    "                 ,df_precision=df_precision,df_recall=df_recall):\n",
    "\n",
    "    df_f1.loc[len(df_f1)] = [method]+list(f1_score(y_pred,y_true,average=None))\n",
    "    df_precision.loc[len(df_precision)] = [method]+list(precision_score(y_pred,y_true,average=None))\n",
    "    df_recall.loc[len(df_recall)] = [method]+list(recall_score(y_pred,y_true,average=None))\n",
    "\n",
    "X = df[df.columns[:-1]].values\n",
    "Y = df[df.columns[-1]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markbaum/Python/python36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/markbaum/Python/python36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import model_selection\n",
    "#from sklearn import cross_validation (통합)\n",
    "\n",
    "cv = 10\n",
    "method = 'linear support vector machine'\n",
    "clf = svm.SVC(kernel='linear',C=50)\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method = 'rbf support vector machine'\n",
    "clf = svm.SVC(kernel='rbf',C=50)\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method = 'poly support vector machine'\n",
    "clf = svm.SVC(kernel='poly',C=50)\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "method = 'decision tree'\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "method = 'random forest'\n",
    "clf = RandomForestClassifier(n_estimators=50,random_state=0,max_features=None)\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markbaum/Python/python36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/markbaum/Python/python36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "method = 'naive bayes'\n",
    "clf = MultinomialNB()\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markbaum/Python/python36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/markbaum/Python/python36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "method = 'logistic regression'\n",
    "clf = LogisticRegression()\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>acc</th>\n",
       "      <th>good</th>\n",
       "      <th>unacc</th>\n",
       "      <th>vgood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>linear support vector machine</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.846591</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rbf support vector machine</td>\n",
       "      <td>0.996109</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999173</td>\n",
       "      <td>0.992248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>poly support vector machine</td>\n",
       "      <td>0.785808</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.938241</td>\n",
       "      <td>0.776860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>decision tree</td>\n",
       "      <td>0.968668</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.992984</td>\n",
       "      <td>0.961832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>random forest</td>\n",
       "      <td>0.962289</td>\n",
       "      <td>0.929577</td>\n",
       "      <td>0.990468</td>\n",
       "      <td>0.984848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>naive bayes</td>\n",
       "      <td>0.035533</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.825701</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>logistic regression</td>\n",
       "      <td>0.267559</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.822836</td>\n",
       "      <td>0.027027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>k nearest neighbours</td>\n",
       "      <td>0.778231</td>\n",
       "      <td>0.536082</td>\n",
       "      <td>0.952759</td>\n",
       "      <td>0.742857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          method       acc      good     unacc     vgood\n",
       "0  linear support vector machine  0.260870  0.000000  0.846591  0.000000\n",
       "1     rbf support vector machine  0.996109  1.000000  0.999173  0.992248\n",
       "2    poly support vector machine  0.785808  0.837209  0.938241  0.776860\n",
       "3                  decision tree  0.968668  0.911765  0.992984  0.961832\n",
       "4                  random forest  0.962289  0.929577  0.990468  0.984848\n",
       "5                    naive bayes  0.035533  0.000000  0.825701  0.000000\n",
       "6            logistic regression  0.267559  0.000000  0.822836  0.027027\n",
       "7           k nearest neighbours  0.778231  0.536082  0.952759  0.742857"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "method = 'k nearest neighbours'\n",
    "clf = KNeighborsClassifier(weights='distance',n_neighbors=5)\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)\n",
    "df_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>acc</th>\n",
       "      <th>good</th>\n",
       "      <th>unacc</th>\n",
       "      <th>vgood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>linear support vector machine</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.985124</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rbf support vector machine</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998347</td>\n",
       "      <td>0.984615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>poly support vector machine</td>\n",
       "      <td>0.778646</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.947934</td>\n",
       "      <td>0.723077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>decision tree</td>\n",
       "      <td>0.966146</td>\n",
       "      <td>0.898551</td>\n",
       "      <td>0.994215</td>\n",
       "      <td>0.969231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>random forest</td>\n",
       "      <td>0.963542</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.987603</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>naive bayes</td>\n",
       "      <td>0.018229</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.998347</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>logistic regression</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923140</td>\n",
       "      <td>0.015385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>k nearest neighbours</td>\n",
       "      <td>0.744792</td>\n",
       "      <td>0.376812</td>\n",
       "      <td>0.991736</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          method       acc      good     unacc     vgood\n",
       "0  linear support vector machine  0.171875  0.000000  0.985124  0.000000\n",
       "1     rbf support vector machine  1.000000  1.000000  0.998347  0.984615\n",
       "2    poly support vector machine  0.778646  0.782609  0.947934  0.723077\n",
       "3                  decision tree  0.966146  0.898551  0.994215  0.969231\n",
       "4                  random forest  0.963542  0.956522  0.987603  1.000000\n",
       "5                    naive bayes  0.018229  0.000000  0.998347  0.000000\n",
       "6            logistic regression  0.208333  0.000000  0.923140  0.015385\n",
       "7           k nearest neighbours  0.744792  0.376812  0.991736  0.600000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>acc</th>\n",
       "      <th>good</th>\n",
       "      <th>unacc</th>\n",
       "      <th>vgood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>linear support vector machine</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.742217</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rbf support vector machine</td>\n",
       "      <td>0.992248</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>poly support vector machine</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.928745</td>\n",
       "      <td>0.839286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>decision tree</td>\n",
       "      <td>0.971204</td>\n",
       "      <td>0.925373</td>\n",
       "      <td>0.991756</td>\n",
       "      <td>0.954545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>random forest</td>\n",
       "      <td>0.961039</td>\n",
       "      <td>0.904110</td>\n",
       "      <td>0.993350</td>\n",
       "      <td>0.970149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>naive bayes</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.703963</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>logistic regression</td>\n",
       "      <td>0.373832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.742193</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>k nearest neighbours</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.916730</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          method       acc      good     unacc     vgood\n",
       "0  linear support vector machine  0.540984  0.000000  0.742217  0.000000\n",
       "1     rbf support vector machine  0.992248  1.000000  1.000000  1.000000\n",
       "2    poly support vector machine  0.793103  0.900000  0.928745  0.839286\n",
       "3                  decision tree  0.971204  0.925373  0.991756  0.954545\n",
       "4                  random forest  0.961039  0.904110  0.993350  0.970149\n",
       "5                    naive bayes  0.700000  0.000000  0.703963  0.000000\n",
       "6            logistic regression  0.373832  0.000000  0.742193  0.111111\n",
       "7           k nearest neighbours  0.814815  0.928571  0.916730  0.975000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "acc       384\n",
       "good       69\n",
       "unacc    1210\n",
       "vgood      65\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_counts=df[6].value_counts()\n",
    "pd.Series(map6).map(labels_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Hidden Markov Model (히든 마르코프 모델)\n",
    "엄격한 의미의 지도는 아니지만, 분류와 매우 비슷하게 사용된다\n",
    "\n",
    "http://shineware.tistory.com/entry/HMM-Hidden-Markov-Model\n",
    "\n",
    "https://www.youtube.com/watch?v=O1U2NWaSYn4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import copy\n",
    "\n",
    "def MostLikelyStateSequence(observations):\n",
    "    #calc combinations:\n",
    "    N = self_A.shape[0]\n",
    "    T = len(observations)\n",
    "    sequences = [str(i) for i in range(N)]\n",
    "    probs = np.array([self_pi[i]*self_B[i,observations[0]] for i in range(N)])\n",
    "    print (probs)\n",
    "    for i in range(1,T):\n",
    "        newsequences = []\n",
    "        newprobs = np.array([])\n",
    "        for s in range(len(sequences)):\n",
    "            for j in range(N):\n",
    "                newsequences.append(sequences[s]+str(j))\n",
    "                bef = int(sequences[s][-1])\n",
    "                tTpprob = probs[s]*self_A[bef,j]*self_B[j,observations[i]]\n",
    "                newprobs = np.append(newprobs,[tTpprob]) \n",
    "                print (sequences[s]+str(j),'-',tTpprob)\n",
    "        sequences = newsequences\n",
    "        probs = newprobs\n",
    "    return max((probs[i],sequences[i]) for i in range(len(sequences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 비터비 알고리즘\n",
    "https://www.youtube.com/watch?v=gchgZ2zp8uo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ViterbiSequence(observations):\n",
    "    deltas = [{}]\n",
    "    seq = {}\n",
    "    N = self_A.shape[0]\n",
    "    states = [i for i in range(N)]\n",
    "    T = len(observations)\n",
    "    #initialization\n",
    "    for s in states:\n",
    "        deltas[0][s] = self_pi[s]*self_B[s,observations[0]]\n",
    "        seq[s] = [s]\n",
    "    #compute Viterbi\n",
    "    for t in range(1,T):\n",
    "        deltas.append({})\n",
    "        newseq = {}\n",
    "        for s in states:\n",
    "            (delta,state) = max((deltas[t-1][s0]*self_A[s0,s]*self_B[s,observations[t]],s0) for s0 in states)\n",
    "            deltas[t][s] = delta\n",
    "            newseq[s] = seq[state] + [s]\n",
    "        seq = newseq\n",
    "\n",
    "    (delta,state) = max((deltas[T-1][s],s) for s in states)\n",
    "    return  delta,' sequence: ', seq[state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 정확한 상태의 개수를 최대화하는 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxProbSequence(observations):\n",
    "    N = self_A.shape[0]\n",
    "    states = [i for i in range(N)]\n",
    "    T = len(observations)\n",
    "    M = self_B.shape[1]\n",
    "    # alpha_t(i) = P(O_1 O_2 ... O_t, q_t = S_i | hmm)\n",
    "    # Initialize alpha\n",
    "    alpha = np.zeros((N,T))\n",
    "    c = np.zeros(T) #scale factors\n",
    "    alpha[:,0] = pi.T * self_B[:,observations[0]]\n",
    "    c[0] = 1.0/np.sum(alpha[:,0])\n",
    "    alpha[:,0] = c[0] * alpha[:,0]\n",
    "    # Update alpha for each observation step\n",
    "    for t in range(1,T):\n",
    "        alpha[:,t] = np.dot(alpha[:,t-1].T, self_A).T * self_B[:,observations[t]]\n",
    "        c[t] = 1.0/np.sum(alpha[:,t])\n",
    "        alpha[:,t] = c[t] * alpha[:,t]\n",
    "\n",
    "    # beta_t(i) = P(O_t+1 O_t+2 ... O_T | q_t = S_i , hmm)\n",
    "    # Initialize beta\n",
    "    beta = np.zeros((N,T))\n",
    "    beta[:,T-1] = 1\n",
    "    beta[:,T-1] = c[T-1] * beta[:,T-1]\n",
    "    # Update beta backwards froT end of sequence\n",
    "    for t in range(len(observations)-1,0,-1):\n",
    "        beta[:,t-1] = np.dot(self_A, (self_B[:,observations[t]] * beta[:,t]))\n",
    "        beta[:,t-1] = c[t-1] * beta[:,t-1]\n",
    "\n",
    "    norm = np.sum(alpha[:,T-1])\n",
    "    seq = ''\n",
    "    for t in range(T):\n",
    "        g,state = max(((beta[i,t]*alpha[i,t])/norm,i) for i in states)\n",
    "        seq +=str(state)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate(time):\n",
    "    def drawFromNormal(probs):\n",
    "        return np.where(np.random.multinomial(1,probs) == 1)[0][0]\n",
    "    observations = np.zeros(time)\n",
    "    states = np.zeros(time)\n",
    "    states[0] = drawFromNormal(self_pi)\n",
    "    observations[0] = drawFromNormal(self_B[states[0],:])\n",
    "    for t in range(1,time):\n",
    "        states[t] = drawFromNormal(self_A[states[t-1],:])\n",
    "        observations[t] = drawFromNormal(self_B[states[t],:])\n",
    "    return observations,states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04 바움웰치 알고리즘\n",
    "우도를 최대화 하는 파라미터를 찾는 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(observations,criterion):\n",
    "    N = self_A.shape[0]\n",
    "    T = len(observations)\n",
    "    M = self_B.shape[1]\n",
    "    A = self_A\n",
    "    B = self_B\n",
    "    pi = copy(self_pi)\n",
    "    convergence = False\n",
    "    while not convergence:\n",
    "        # alpha_t(i) = P(O_1 O_2 ... O_t, q_t = S_i | hmm)\n",
    "        # Initialize alpha\n",
    "        alpha = np.zeros((N,T))\n",
    "        c = np.zeros(T) #scale factors\n",
    "        alpha[:,0] = pi.T * self_B[:,observations[0]]\n",
    "        c[0] = 1.0/np.sum(alpha[:,0])\n",
    "        alpha[:,0] = c[0] * alpha[:,0]\n",
    "        # Update alpha for each observation step\n",
    "        for t in range(1,T):\n",
    "            alpha[:,t] = np.dot(alpha[:,t-1].T, self_A).T * self_B[:,observations[t]]\n",
    "            c[t] = 1.0/np.sum(alpha[:,t])\n",
    "            alpha[:,t] = c[t] * alpha[:,t]\n",
    "        #P(O=O_0,O_1,...,O_T-1 | hmm)\n",
    "        P_O = np.sum(alpha[:,T-1])\n",
    "        # beta_t(i) = P(O_t+1 O_t+2 ... O_T | q_t = S_i , hmm)\n",
    "        # Initialize beta\n",
    "        beta = np.zeros((N,T))\n",
    "        beta[:,T-1] = 1\n",
    "        beta[:,T-1] = c[T-1] * beta[:,T-1]\n",
    "        # Update beta backwards froT end of sequence\n",
    "        for t in range(len(observations)-1,0,-1):\n",
    "            beta[:,t-1] = np.dot(self_A, (self_B[:,observations[t]] * beta[:,t]))\n",
    "            beta[:,t-1] = c[t-1] * beta[:,t-1]\n",
    "        gi = np.zeros((N,N,T-1));\n",
    "        for t in range(T-1):\n",
    "            for i in range(N):\n",
    "                gamma_num = alpha[i,t] * self_A[i,:] * self_B[:,observations[t+1]].T * \\\n",
    "                        beta[:,t+1].T\n",
    "                gi[i,:,t] = gamma_num / P_O\n",
    "        # gamma_t(i) = P(q_t = S_i | O, hmm)\n",
    "        gamma = np.squeeze(np.sum(gi,axis=1))\n",
    "        # Need final gamma element for new B\n",
    "        prod =  (alpha[:,T-1] * beta[:,T-1]).reshape((-1,1))\n",
    "        gamma_T = prod/P_O\n",
    "        gamma = np.hstack((gamma,  gamma_T)) #append one Tore to gamma!!!\n",
    "        newpi = gamma[:,0]\n",
    "        newA = np.sum(gi,2) / np.sum(gamma[:,:-1],axis=1).reshape((-1,1))\n",
    "        newB = copy(B)\n",
    "        sumgamma = np.sum(gamma,axis=1)\n",
    "        for ob_k in range(M):\n",
    "            list_k = observations == ob_k\n",
    "            newB[:,ob_k] = np.sum(gamma[:,list_k],axis=1) / sumgamma\n",
    "        if np.max(abs(pi - newpi)) < criterion and \\\n",
    "               np.max(abs(A - newA)) < criterion and \\\n",
    "               np.max(abs(B - newB)) < criterion:\n",
    "            convergence = True;\n",
    "        A[:],B[:],pi[:] = newA,newB,newpi\n",
    "    self_A[:] = newA\n",
    "    self_B[:] = newB\n",
    "    self_pi[:] = newpi\n",
    "    self_gamma = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pi = np.array([0.6, 0.4])\n",
    "A = np.array([[0.7, 0.3],      [0.6, 0.4]])\n",
    "B = np.array([[0.7, 0.1, 0.2], [0.1, 0.6, 0.3]])\n",
    "self_pi = pi; self_A = A; self_B = B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi sequence: (0.0044452799999999994, ' sequence: ', [0, 1, 0, 0])\n",
      "max prob sequence: 0100\n"
     ]
    }
   ],
   "source": [
    "print ('Viterbi sequence:',ViterbiSequence(np.array([0,1,0,2])))\n",
    "print ('max prob sequence:',maxProbSequence(np.array([0,1,0,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#obs,states = hmmguess.simulate(4)\n",
    "train(np.array([0,1,0,2]),0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated initial probabilities\n",
      " [ 1.  0.]\n",
      "Estimated state transition probabililities\n",
      " [[ 0.  1.]\n",
      " [ 1.  0.]]\n",
      "Estimated observation probabililities\n",
      " [[ 1.          0.          0.        ]\n",
      " [ 0.          0.38196618  0.61803382]]\n"
     ]
    }
   ],
   "source": [
    "print ('Estimated initial probabilities\\n',pi)\n",
    "print ('Estimated state transition probabililities\\n',A)\n",
    "print ('Estimated observation probabililities\\n',B)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
