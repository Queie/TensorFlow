{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 - Statistical Language mModeling\n",
    "통계 언어 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# << context >>\n",
    "# 1 단어 빈도계산 (1gram, 2gram, 3gram)\n",
    "# 2 주어진 텍스트 MLE 개발 (Maximum Likelihood Estimation : 최대우도 측정모델)\n",
    "# 3 MLE 모델의 스무딩 적용 \n",
    "# 4 MLE의 back-off 매커니즘 개발\n",
    "# 5 mix and match 를 억디위한 데이터 보간법 적용\n",
    "# 6 혼잡도(perplexity)를 활용한 언어모델의 평가\n",
    "# 7 Metropolis-Hastings (기각표본추출 알고리즘- 몬테카를로 시뮬레이션에 적용) 를 언어모델링에 적용\n",
    "# 8 Gibbs sampling을 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 1 단어빈도 측정\n",
    "1-gram, 2-gram, 3-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 n-Gram token으로 생성하기\n",
    "1-gram, 2-gram, 3-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('De', 'verzekeringsmaatschappijen', 'verhelen', 'niet')\n",
      "('op', 'het', 'Amerikaanse', 'ingrijpen')\n",
      "('Oogziekenhuis', ',', 'waar', 'de')\n",
      "('de', 'kopers', 'van', 'de')\n",
      "('we', 'hier', 'Hollandse', 'lijnrechters')\n",
      "('defensie', '.', 'Ik', 'heb')\n",
      "('blank', 'deel', 'uiteenvalt', '.')\n",
      "('treedt', ',', 'ben', 'ik')\n",
      "('hij', 'vindt', 'er', 'een')\n",
      "('bij', 'nadere', 'uitwerking', 'niet')\n"
     ]
    }
   ],
   "source": [
    "# 1-gram, 2-gram, 3-gram 으로 token 만들기\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import alpino\n",
    "\n",
    "unigrams = ngrams(alpino.words(),1)\n",
    "bigrams = ngrams(alpino.words(),2)\n",
    "quadgrams = ngrams(alpino.words(),4)\n",
    "\n",
    "for i, word in enumerate(quadgrams):\n",
    "    if i % 15000 == 0: print(word) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 우도를 활용한 상위빈도 text 추출하기\n",
    "Maximum Likelihood Estimation : 최대우도 측정모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# token 생성\n",
    "from nltk.corpus import webtext\n",
    "tokens=[t.lower()    for t in webtext.words('grail.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi-gram   : [(\"'\", 's'), ('arthur', ':'), ('#', '1'), (\"'\", 't'), ('villager', '#'), ('#', '2'), (']', '['), ('1', ':'), ('oh', ','), ('black', 'knight')]\n"
     ]
    }
   ],
   "source": [
    "# bi-gram 중, 우도비율 상위 10개 목록 출력\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "words = BigramCollocationFinder.from_words(tokens)\n",
    "\n",
    "print('bi-gram   :' ,words.nbest(BigramAssocMeasures.likelihood_ratio, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 2 텍스트의 MLE 개발\n",
    "Multinomial logistic regression : 다항 기호 논리학 회귀\n",
    "\n",
    "주어진 발생에 대한 확률분포를 포함하는 freqdist를 생성한다\n",
    "\n",
    "http://www.nltk.org/howto/probability.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 token 을  freqdist 객체로 변환하기\n",
    "nltk.FreqDist() : 단어의 빈도 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'!': 1,\n",
       "          'a': 1,\n",
       "          'anywhere': 1,\n",
       "          'fish': 1,\n",
       "          'goes': 1,\n",
       "          'good': 1,\n",
       "          'no': 1,\n",
       "          'porpoise': 1,\n",
       "          'without': 1})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# freqdist 생성하기\n",
    "text1 = ['no', 'good', 'fish', 'goes', 'anywhere', 'without', 'a', 'porpoise', '!']\n",
    "text2 = ['no', 'good', 'porpoise', 'likes', 'to', 'fish', 'fish', 'anywhere', '.']\n",
    "\n",
    "import nltk\n",
    "fd1 = nltk.FreqDist(text1); fd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text1 과 text2 의 도수빈도를 결합\n",
    "both = nltk.FreqDist(text1 + text2); both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# freqdist 를 pickle로 저장\n",
    "import pickle\n",
    "pickled = pickle.dumps(fd1)\n",
    "fd1 == pickle.loads(pickled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 3 은닉 마르코프 모델 추정\n",
    "Hidden Markov Model estimation\n",
    "\n",
    "$x_0, x_1..$ : 은닉상태 , $y_0, y_1 ..$ : 관찰 가능한 상태\n",
    "\n",
    "<img src=\"http://iacs-courses.seas.harvard.edu/courses/am207/blog/hmm.png\" align='left' width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 Hidden Markov Model estimation\n",
    "HMM추정을 사용해 테스트를 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HMM추정을 사용해 테스트를 수행\n",
    "# brown 말뭉치의 내용 확인 (단어, 품사)의 tuple로 구성\n",
    "import nltk\n",
    "cor = nltk.corpus.brown.tagged_sents(categories='adventure')[:500]\n",
    "cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltk의 unique_list 내부 살펴보기\n",
    "# tag 살펴보기\n",
    "from nltk.util import unique_list\n",
    "tag_set = unique_list(tag     for sent in cor     for (word,tag) in sent)\n",
    "print(len(tag_set))\n",
    "tag_set[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentence 살펴보기 \n",
    "symbols = unique_list(word    for sent in cor     for (word,tag) in sent)\n",
    "print(len(symbols)); symbols[::250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 히든 마르코프 훈련모듈 활성화 (92개 tag,  1464개 문장)\n",
    "trainer = nltk.tag.HiddenMarkovModelTrainer(tag_set, symbols)\n",
    "train_corpus = []\n",
    "test_corpus = []\n",
    "\n",
    "# train 90% , test 10% 데이터 생성\n",
    "for i in range(len(cor)):\n",
    "    if i % 10:  train_corpus+=[cor[i]]\n",
    "    else: test_corpus+=[cor[i]]         \n",
    "print(len(train_corpus))\n",
    "print(len(test_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 은닉 마르코프 추정법칙 테스트\n",
    "def train_and_test(est):\n",
    "    hmm = trainer.train_supervised(train_corpus, estimator=est)\n",
    "    print('%.2f%%' % (100 * hmm.evaluate(test_corpus)))\n",
    "\n",
    "train_and_test(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 최대 우도추정 모델\n",
    "Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import MLEProbDist\n",
    "\n",
    "mle = lambda fd, bins: MLEProbDist(fd)\n",
    "train_and_test(mle)\n",
    "\n",
    "# Laplace (= Lidstone with gamma==1)\n",
    "from nltk import MLEProbDist, LaplaceProbDist\n",
    "train_and_test(LaplaceProbDist) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Expected Likelihood Estimation (= Lidstone with gamma==0.5)\n",
    "from nltk import ELEProbDist\n",
    "train_and_test(ELEProbDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lidstone Estimation, for gamma==0.1, 0.5 and 1 (the later two should be exactly equal to MLE and ELE above)\n",
    "from nltk import LidstoneProbDist\n",
    "def lidstone(gamma):\n",
    "    return lambda fd, bins: LidstoneProbDist(fd, gamma, bins)\n",
    "\n",
    "print(train_and_test(lidstone(0.1)))\n",
    "print(train_and_test(lidstone(0.5)))\n",
    "print(train_and_test(lidstone(1.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 텍스트의 MLE 개발\n",
    "다중회귀모델 개발"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from__future__import print_function,unicode_literals\n",
    "# __docformat__='epytext en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os, numpy\n",
    "from collections import defaultdict\n",
    "from nltk import compat\n",
    "from nltk.data import gzip_open_unicode\n",
    "from nltk.util import OrderedDict\n",
    "from nltk.probability import DictionaryProbDist\n",
    "from nltk.classify.api import ClassifierI\n",
    "from nltk.classify.util import CutoffChecker,accuracy,log_likelihood\n",
    "from nltk.classify.megam import call_megam, write_megam_file,parse_megam_weights\n",
    "from nltk.classify.tadm import call_tadm,write_tadm_file,parse_tadm_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 최대우도 추정을 위한 사용자 함수 (도수분포에 기초하여 모든 확률을 계산)\n",
    "# ProbDistI : 텍스트 개별 발생확률 분포를 결정시 사용\n",
    "class MLEProbDist: # (ProbDistI): \n",
    "    def __init__(self, freqdist, bins=None):\n",
    "        self._freqdist = freqdist\n",
    "    # 주어진 사건에 대한 확률분포를 포함하는 freqdist를 생성\n",
    "    def freqdist(self):\n",
    "        return self._freqdist\n",
    "    def prob(self, sample):\n",
    "        return self._freqdist.freq(sample)\n",
    "    def max(self):\n",
    "        return self._freqdist.max()\n",
    "    def samples(self):\n",
    "        return self._freqdist.keys()\n",
    "    def __repr__(self):  # ProbDist 의 문자열을 반환\n",
    "        return '<MLEProbDist based on %d samples>' % self._freqdist.N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 단어의 도수분포(0~1) 를 얻기 위한 함수\n",
    "class LidstoneProbDist: #(ProbDistI)\n",
    "    SUM_TO_ONE = False\n",
    "\n",
    "    def __init__(self, freqdist, gamma, bins=None): # 초기함수\n",
    "        if (bins == 0) or (bins is None and freqdist.N() == 0):\n",
    "            name = self.__class__.__name__[:-8]\n",
    "            raise ValueError('A %s probability distribution ' % name +\n",
    "                             'must have at least one bin.') # 오류발생시 출력\n",
    "        \n",
    "        if (bins is not None) and (bins < freqdist.B()): # 유효한 값을 갖는 경우\n",
    "            name = self.__class__.__name__[:-8]\n",
    "            raise ValueError('\\nThe number of bins in a %s distribution ' % name +          \n",
    "                             '(%d) must be greater than or equal to\\n' % bins +\n",
    "                             'the number of bins in the FreqDist used ' +\n",
    "                             'to create it (%d).' % freqdist.B()) # 오류발생시 출력\n",
    "            self._freqdist = freqdist\n",
    "            self._gamma = float(gamma)\n",
    "            self._N = self._freqdist.N()\n",
    "\n",
    "            if bins is None:\n",
    "                bins = freqdist.B()\n",
    "            self._bins = bins\n",
    "            self._divisor = self._N + bins * gamma\n",
    "\n",
    "            if self._divisor == 0.0: # 극단치를 갖는경우, 확률을 0으로 강제, 카운트 X\n",
    "                self._gamma = 0\n",
    "                self._divisor = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def freqdist(self): # 확률분포에 기초한 도수분포값을 출력\n",
    "        return self._freqdist\n",
    "\n",
    "    def prob(self, sample): \n",
    "        c = self._freqdist[sample]\n",
    "        return (c + self._gamma) / self._divisor\n",
    "\n",
    "    # 가능성 높은 샘플생성 : 발생빈도가 높은 샘플을 선택\n",
    "    def max(self):\n",
    "        return self._freqdist.max()\n",
    "\n",
    "    def samples(self):\n",
    "        return self._freqdist.keys()\n",
    "\n",
    "    def discount(self):\n",
    "        gb = self._gamma * self._bins\n",
    "        return gb / (self._N + gb)\n",
    "    \n",
    "    # ProbDIst의 문자열을 출력\n",
    "    def __repr__(self):\n",
    "        return '<LidstoneProbDist based on %d samples>' % self._freqdist.N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# freqdist를 발생하는 확률분포 계산\n",
    "class LaplaceProbDist(LidstoneProbDist):\n",
    "\n",
    "    # 샘플의 확률 : (c+1) / (N+B)\n",
    "    def __init__(self, freqdist, bins=None):\n",
    "        LidstoneProbDist.__init__(self, freqdist, 1, bins)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<LaplaceProbDist based on %d samples>' % self._freqdist.N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 도수 분포를 계산\n",
    "class ELEProbDist(LidstoneProbDist):\n",
    "\n",
    "    # 샘플의 확률 : (c+0.5) / (N+B/2)\n",
    "    def __init__(self, freqdist, bins=None):\n",
    "        LidstoneProbDist.__init__(self, freqdist, 0.5, bins)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<ELEProbDist based on %d samples>' % self._freqdist.N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 기존의 샘플 빈도에 기초하여, '균일확률질량'(uniform probability mass)을 추출\n",
    "# T : 관찰된 샘플의 수\n",
    "# N : 관찰된 이벤트 수\n",
    "# T / (N + T) : count가 0일떄 샘플의 확률질량\n",
    "# c / (N + T) : 기타의 경우 샘플의 확률질량\n",
    "# Z : 정규화 계수\n",
    "class WittenBellProbDist: #(ProbDistI):\n",
    "    def __init__(self, freqdist, bins=None):\n",
    "        assert bins is None or bins >= freqdist.B(),\\\n",
    "        'bins parameter must not be less than %d=freqdist.B()' % freqdist.B()\n",
    "        if bins is None:\n",
    "            bins = freqdist.B()\n",
    "            self._freqdist = freqdist\n",
    "            self._T = self._freqdist.B()\n",
    "            self._Z = bins - self._freqdist.B()\n",
    "            self._N = self._freqdist.N()\n",
    "            # self._P0 is P(0), precalculated for efficiency:\n",
    "        if self._N == 0:\n",
    "            # if freqdist is empty, we approximate P(0) by a UniformProbDist:\n",
    "            self._P0 = 1.0 / self._Z\n",
    "        else:\n",
    "            self._P0 = self._T / float(self._Z * (self._N + self._T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # inherit docs from ProbDistI\n",
    "    def prob(self, sample):\n",
    "        c = self._freqdist[sample]\n",
    "        return (c / float(self._N + self._T) if c != 0 else self._P0)\n",
    "    def max(self):\n",
    "        return self._freqdist.max()\n",
    "    def samples(self):\n",
    "        return self._freqdist.keys()\n",
    "    def freqdist(self):\n",
    "        return self._freqdist\n",
    "    def discount(self):\n",
    "        raise NotImplementedError()    \n",
    "    # String representation of ProbDist is obtained\n",
    "    def __repr__(self):\n",
    "        return '<WittenBellProbDist based on %d samples>' % self._freqdist.N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 위의 CLASS를 기초로 '최대우도값'을 추청"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.probability import *\n",
    "# train_and_test(mle)\n",
    "# train_and_test(LaplaceProbDist)\n",
    "# train_and_test(ELEProbDist)\n",
    "def lidstone(gamma):\n",
    "    return lambda fd, bins: LidstoneProbDist(fd, gamma, bins)\n",
    "# train_and_test(lidstone(0.1))\n",
    "# train_and_test(lidstone(0.5))\n",
    "# train_and_test(lidstone(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 4 MLE 모델의 Smoothing 적용\n",
    "알려지지 않은 단어는 확률이 0이므로, 이를 해결시 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 Add-one smoothing\n",
    "가짜 수 (Pseudo Count) 1을 활용하여, 확률이 0이 아닌 값을 덧붙인다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "corpus = u\"<s> hello how are you doing ? Hope you find the book interesting. </s>\".split()\n",
    "sentence = u\"<s>how are you doing</s>\".split()\n",
    "vocabulary = set(corpus)\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장의 bi-gram 말뭉치 수를 count\n",
    "cfd = nltk.ConditionalFreqDist(nltk.bigrams(corpus))\n",
    "[cfd[a][b] for (a,b) in nltk.bigrams(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장의 각 단어 count\n",
    "[cfd[a].N() for (a,b) in nltk.bigrams(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1.0, 0.0]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLE 확률에 대한 FreqDist\n",
    "[cfd[a].freq(b) for (a,b) in nltk.bigrams(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 bi-gram 에 대한 Laplace smoothing \n",
    "[1 + cfd[a][b] for (a,b) in nltk.bigrams(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 14, 15]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어의 수를 정규화\n",
    "[len(vocabulary) + cfd[a].N() for (a,b) in nltk.bigrams(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07692307692307693, 0.14285714285714285, 0.06666666666666667]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 bi-gram의 Laplace smoothing 확률\n",
    "[1.0 * (1+cfd[a][b]) / (len(vocabulary)+cfd[a].N()) for (a,b) in nltk.bigrams(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1.0, 0.0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLEProbDist는 smoothing 하지 않은 확률분포\n",
    "cpd_mle = nltk.ConditionalProbDist(cfd, nltk.MLEProbDist, bins=len(vocabulary))\n",
    "[cpd_mle[a].prob(b) for (a,b) in nltk.bigrams(sentence)] # .prob() 메소트로 MLE 확률값 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07692307692307693, 0.14285714285714285, 0.06666666666666667]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LaplaceProbDist : smoothing 된 PorbDist 추가 기능\n",
    "cpd_laplace = nltk.ConditionalProbDist(cfd, nltk.LaplaceProbDist, bins=len(vocabulary))\n",
    "[cpd_laplace[a].prob(b) for (a,b) in nltk.bigrams(sentence)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 Good Turing\n",
    "보이지 않는 객체의 확룰을 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markbaum/Python/python36/lib/python3.6/site-packages/nltk/probability.py:1364: UserWarning: SimpleGoodTuring did not find a proper best fit line for smoothing probabilities of occurrences. The probability estimates are likely to be unreliable.\n",
      "  warnings.warn('SimpleGoodTuring did not find a proper best fit '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.93%\n"
     ]
    }
   ],
   "source": [
    "# nltk 의 simple Good Turing \n",
    "gt = lambda fd, bins: SimpleGoodTuringProbDist(fd, bins=1e5)\n",
    "train_and_test(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bill Gale and Geoffrey Sampson have presented Simple Good Turing:\n",
    "\n",
    "# 한쌍의 (pi, qi)이 주어진경우, pi 는 빈도를, qi는 빈도의 빈도를 의미한다\n",
    "# 오차제곰을 최소로 하는 결과를 위해. E(p) 와 E(q) 즉 pi와 qi의 평균을 사용한다\n",
    "# - slope, b = sigma ((pi-E(p)(qi-E(q))) / sigma ((pi-E(p))(pi-E(p)))\n",
    "# - intercept: a = E(q) - b.E(p)\n",
    "class SimpleGoodTuringProbDist(ProbDistI):\n",
    "    SUM_TO_ONE = False\n",
    "    \n",
    "    # param freqdist는 확률분포가 추정되는 빈도의 수\n",
    "    # Param bins은 가능한 샘플의 수를 추정시 사용\n",
    "    def __init__(self, freqdist, bins=None):\n",
    "        assert bins is None or bins > freqdist.B(),\\\n",
    "        'bins parameter must not be less than %d=freqdist.B()+1' %(freqdist.B()+1)\n",
    "        if bins is None:\n",
    "            bins = freqdist.B() + 1\n",
    "            self._freqdist = freqdist\n",
    "            self._bins = bins\n",
    "            r, nr = self._r_Nr()\n",
    "            self.find_best_fit(r, nr)\n",
    "            self._switch(r, nr)\n",
    "            self._renormalize(r, nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def _r_Nr_non_zero(self):\n",
    "        r_Nr = self._freqdist.r_Nr()\n",
    "        del r_Nr[0]\n",
    "        return r_Nr\n",
    "\n",
    "    # Nr(r)>0 인 2개목록 (r,Nr)에서 도수분포를 나눈다\n",
    "    def _r_Nr(self): \n",
    "        nonzero = self._r_Nr_non_zero()\n",
    "        if not nonzero: \n",
    "            return [], []\n",
    "        return zip(*sorted(nonzero.items()))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # self._slope, self._intercept 매개변수 조정을 위한, 선형회귀 분석을 시행한다\n",
    "    # (log 공간에서 작업을 함으로써, 부동소숫점 underflow가 발생하지 않도록 한다)\n",
    "    # 높은 샘플빈고를 갖는 경우, Nr = 1을 따라 수평이 된다\n",
    "    # Log 공간에서의 연선을 위해, 주변 0값으로 양의 Nr값을 평균화 한다 (Church and Gale, 1991)\n",
    "    def find_best_fit(self, r, nr):\n",
    "        if not r or not nr: #  r 또는 nr이 비었는지 확인\n",
    "            return\n",
    "        \n",
    "        zr = []\n",
    "        for j in range(len(r)):\n",
    "            i = (r[j-1] if j > 0 else 0)\n",
    "            k = (2 * r[j] - i if j == len(r) - 1 else r[j+1])\n",
    "            zr_ = 2.0 * nr[j] / (k - i)\n",
    "            zr.append(zr_)\n",
    "            log_r = [math.log(i) for i in r]\n",
    "            log_zr = [math.log(i) for i in zr]\n",
    "            xy_cov = x_var = 0.0\n",
    "            x_mean = 1.0 * sum(log_r) / len(log_r)\n",
    "            y_mean = 1.0 * sum(log_zr) / len(log_zr)\n",
    "            for (x, y) in zip(log_r, log_zr):\n",
    "                xy_cov += (x - x_mean) * (y - y_mean)\n",
    "                x_var += (x - x_mean)**2\n",
    "                \n",
    "        self._slope = (xy_cov / x_var if x_var != 0 else 0.0)\n",
    "        if self._slope >= -1:\n",
    "            warnings.warn('SimpleGoodTuring did not find a proper best fit '\n",
    "                          'line for smoothing probabilities of occurrences. '\n",
    "                          'The probability estimates are likely to be '\n",
    "                          'unreliable.')\n",
    "        \n",
    "        self._intercept = y_mean - self._slope * x_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        # E[Nr]을 추정시 Nr에서 Sr로 전환해야 하는 r frontier를 계산한다\n",
    "        def _switch(self, r, nr):\n",
    "            for i, r_ in enumerate(r):\n",
    "                # r이 끝에 있거나, 간격이 있는 경우\n",
    "                if len(r) == i + 1 or r[i+1] != r_ + 1:\n",
    "                    self._switch_at = r_\n",
    "                    break\n",
    "                Sr = self.smoothedNr\n",
    "                smooth_r_star = (r_ + 1) * Sr(r_+1) / Sr(r_)\n",
    "                unsmooth_r_star = 1.0 * (r_ + 1) * nr[i+1] / nr[i]\n",
    "\n",
    "                std = math.sqrt(self._variance(r_, nr[i], nr[i+1]))\n",
    "                if abs(unsmooth_r_star-smooth_r_star) <= 1.96 * std:\n",
    "                    self._switch_at = r_\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        def _variance(self, r, nr, nr_1):\n",
    "            r = float(r)\n",
    "            nr = float(nr)\n",
    "            nr_1 = float(nr_1)\n",
    "            return (r + 1.0)**2 * (nr_1 / nr**2) * (1.0 + nr_1 / nr)\n",
    "\n",
    "        def _renormalize(self, r, nr):\n",
    "            prob_cov = 0.0\n",
    "            for r_, nr_ in zip(r, nr):\n",
    "                prob_cov += nr_ * self._prob_measure(r_)\n",
    "            if prob_cov:\n",
    "                self._renormal = (1 - self._prob_measure(0)) / prob_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        # count r 로 샘플의 수를 반환한다\n",
    "        # Nr = a*r^b (적절한 쌍곡선형 상관을 제공하기 위해 b < -1) \n",
    "        # 방정식의 대수 형테애 대한 간단한 회귀 기법으로, a와 b를 추정한다\n",
    "        # log Nr = a + b*log(r)\n",
    "        def smoothedNr(self, r):\n",
    "            return math.exp(self._intercept + self._slope * math.log(r))\n",
    "\n",
    "        def prob(self, sample): # 샘플의 확률을 반환한다\n",
    "            count = self._freqdist[sample]\n",
    "            p = self._prob_measure(count)\n",
    "            if count == 0:\n",
    "                if self._bins == self._freqdist.B(): p = 0.0\n",
    "                else: p = p / (1.0 * self._bins - self._freqdist.B())\n",
    "            else: p = p * self._renormal\n",
    "            return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        def _prob_measure(self, count):\n",
    "            if count == 0 and self._freqdist.N() == 0 : \n",
    "                return 1.0\n",
    "            elif count == 0 and self._freqdist.N() != 0:\n",
    "                return 1.0 * self._freqdist.Nr(1) / self._freqdist.N()\n",
    "            if self._switch_at > count:\n",
    "                Er_1 = 1.0 * self._freqdist.Nr(count+1)\n",
    "                Er = 1.0 * self._freqdist.Nr(count)\n",
    "            else:\n",
    "                Er_1 = self.smoothedNr(count+1)\n",
    "                Er = self.smoothedNr(count)\n",
    "            r_star = (count + 1) * Er_1 / Er\n",
    "            return r_star / self._freqdist.N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        # 확률의 총합이 1인지를 통해서 (prob_sum != 1.0) 확인\n",
    "        def check(self):\n",
    "            prob_sum = 0.0\n",
    "            for i in range(0, len(self._Nr)):\n",
    "                prob_sum += self._Nr[i] * self._prob_measure(i) / self._renormal\n",
    "            print(\"Probability Sum:\", prob_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        # 보이는 이벤트에서 보이지 않는 이벤트로 확률을 전송\n",
    "        def discount(self):\n",
    "            return 1.0 * self.smoothedNr(1) / self._freqdist.N()\n",
    "        def max(self):\n",
    "            return self._freqdist.max()\n",
    "        def samples(self):\n",
    "            return self._freqdist.keys()\n",
    "        def freqdist(self):\n",
    "            return self._freqdist\n",
    "        def __repr__(self): # ProbDist 의 문자열 표현을 얻는다\n",
    "            return '<SimpleGoodTuringProbDist based on %d samples>'\\\n",
    "            % self._freqdist.N()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 크네저 네이 추정\n",
    "Kneser Ney estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = \" Hello , please read the book thoroughly .\\\n",
    " If you have any queries , then don't hesitate to ask.\\\n",
    " There is no shortcut to success .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "for x,y,z in nltk.trigrams(sent):\n",
    "    pass\n",
    "    #print(x[0],y[0],z[0])\n",
    "    #print(x[1],y[1],z[1])  # 여기서 오류가 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# corpus = [[((x[0],y[0],z[0]),(x[1],y[1],z[1])) for x, y, z in nltk.trigrams(sent)] \n",
    "#           for sent in corpus[:100]]\n",
    "# tag_set = unique_list(tag for sent in corpus for (word,tag) in sent)\n",
    "# print(len(tag_set))\n",
    "# symbols = unique_list(word for sent in corpus for (word,tag) in sent)\n",
    "# print(len(symbols))\n",
    "# trainer = nltk.tag.HiddenMarkovModelTrainer(tag_set, symbols)\n",
    "# train_corpus = []\n",
    "# test_corpus = []\n",
    "# for i in range(len(corpus)):\n",
    "#     if i % 10: train_corpus += [corpus[i]]\n",
    "#     else:      test_corpus += [corpus[i]]\n",
    "# kn = lambda fd, bins: KneserNeyProbDist(fd)\n",
    "# train_and_test(kn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04 위튼 벨 추정\n",
    "Witten Bell estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.12%\n"
     ]
    }
   ],
   "source": [
    "train_and_test(WittenBellProbDist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 5 MLE의 back-off 매커니즘 개발\n",
    "Develop a back-off mechanism for MLE\n",
    "\n",
    "http://blog.naver.com/PostView.nhn?blogId=hot1455&logNo=60128729107"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train시 n-gram이 n번이상 보일경우, 이전정보가 주어지면 token의 조건부 확률은 n-gram의 MLE에 비례한다.\n",
    "# 그렇지 않으면 조건부 확률은 (n-1)gram의 back-off 조건부 확률(이전 정보가 주어진 token의 조건부 확률)과 동일하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# param word: 확률계산할 단어\n",
    "# type word: str\n",
    "# param context: 단어를 포함한 문장\n",
    "# type context: list(str)\n",
    "def prob(self, word, context): # Katz back-off 모델\n",
    "    context = tuple(context)\n",
    "    if(context+(word,) in self._ngrams) or (self._n == 1):\n",
    "        return self[context].prob(word)\n",
    "    else:\n",
    "        return self._alpha(context) * self._backoff.prob(word,context[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 6  Mix and Match를 얻기위한 데이터 보간법\n",
    "uni-gram을 수행후 bi-gram을 수행함으로써 둘의 확률을 결합하는 보간모델의 개발이 가능하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 7 혼잡도를 통한 언어 모델의 평가\n",
    "perplexity(text) : 텍스트의 혼잡도를 평가한다 (2**cross entropy로 측정)\n",
    "\n",
    "텍스트를 예측하는데 유용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculates the perplexity of the given text.\n",
    "This is simply 2 ** cross-entropy for the text.\n",
    ":param text: words to calculate perplexity of\n",
    ":type text: list(str)\n",
    "\"\"\"\n",
    "def perplexity(self, text):\n",
    "    return pow(2.0, self.entropy(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 8 모델링 언어에서 메트로폴리스 헤어스팅스 적용\n",
    "마르코프 모델에서 사후분포처리를 수행하기 위해 Metropolis-Hastings sampler를 사용한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 9 Gibbs sampling 을 언어처리시 적용\n",
    "Gibbs sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N번 시행과 Z번 성공시 Bernoulli 가능성(likelihood)\n",
    "def bern(theta,z,N):\n",
    "    return np.clip(theta**z*(1-theta)**(N-z),0,1)\n",
    "# N번 시행과 Z번 성공시 Bernoulli 가능성(likelihood)\n",
    "def bern2(theta1,theta2,z1,z2,N1,N2):\n",
    "    return bern(theta1,z1,N1)*bern(theta2,z2,N2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_thetas(xmin,xmax,n):\n",
    "    xs=np.linspace(xmin,xmax,n)\n",
    "    widths=(xs[1:]-xs[:-1])/2.0\n",
    "    thetas=xs[:-1]+widths\n",
    "    return hetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_plots(X,Y,prior,likelihood,posterior,projection=None):\n",
    "    fig,ax=plt.subplots(1,3,subplot_kw=dict(projection=projection,aspect='equal'),figsize=(12,3))\n",
    "    if projection=='3d':\n",
    "        ax[0].plot_surface(X,Y,prior,alpha=0.3,cmap=plt.cm.jet)\n",
    "        ax[1].plot_surface(X,Y,likelihood,alpha=0.3,cmap=plt.cm.jet)\n",
    "        ax[2].plot_surface(X,Y,posterior,alpha=0.3,cmap=plt.cm.jet)\n",
    "    else:\n",
    "        ax[0].contour(X,Y,prior)\n",
    "        ax[1].contour(X,Y,likelihood)\n",
    "        ax[2].contour(X,Y,posterior)\n",
    "        ax[0].set_title('Prior')\n",
    "        ax[1].set_title('Likelihood')\n",
    "        ax[2].set_title('Posteior')\n",
    "    plt.tight_layout()\n",
    "    thetas1=make_thetas(0,1,101)\n",
    "    thetas2=make_thetas(0,1,101)\n",
    "    X,Y=np.meshgrid(thetas1,thetas2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Metropolice 시행\n",
    "# import numpy as np\n",
    "# a = 2\n",
    "# b = 3\n",
    "# z1 = 11\n",
    "# N1 = 14\n",
    "# z2 = 7\n",
    "# N2 = 14\n",
    "# prior = lambda theta1,theta2:stats.beta(a,b).pdf(theta1)*stats.beta(a,b).pdf(theta2)\n",
    "# lik = partial(bern2,z1=z1,z2=z2,N1=N1,N2=N2)\n",
    "# target = lambda theta1,theta2:prior(theta1,theta2)*lik(theta1,theta2)\n",
    "# theta = np.array([0.5,0.5])\n",
    "# niters = 10000\n",
    "# burnin = 500\n",
    "# sigma = np.diag([0.2,0.2])\n",
    "# thetas = np.zeros((niters-burnin,2),np.float)\n",
    "# for i in range(niters):\n",
    "#     new_theta = stats.multivariate_normal(theta,sigma).rvs()\n",
    "#     p = min(target(*new_theta)/target(*theta),1)\n",
    "#     if np.random.rand() < p:\n",
    "#         theta = new_theta\n",
    "#     if i >= burnin:\n",
    "#         thetas[i-burnin] = theta\n",
    "#         kde = stats.gaussian_kde(thetas.T)\n",
    "# XY = np.vstack([X.ravel(),Y.ravel()])\n",
    "# posterior_metroplis = kde(XY).reshape(X.shape)\n",
    "# make_plots(X,Y,prior(X,Y),lik(X,Y),posterior_metroplis)\n",
    "# make_plots(X,Y,prior(X,Y),lik(X,Y),posterior_metroplis,projection='3d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gibbs 수행\n",
    "# a = 2\n",
    "# b = 3\n",
    "# z1 = 11\n",
    "# N1 = 14\n",
    "# z2 = 7\n",
    "# N2 = 14\n",
    "# prior = lambda theta1,theta2:stats.beta(a,b).pdf(theta1)*stats.\n",
    "# beta(a,b).pdf(theta2)\n",
    "# lik = partial(bern2,z1=z1,z2=z2,N1=N1,N2=N2)\n",
    "# target = lambdatheta1,theta2:prior(theta1,theta2)*lik(theta1,theta2)\n",
    "# theta = np.array([0.5,0.5])\n",
    "# niters = 10000\n",
    "# burnin = 500\n",
    "# sigma = np.diag([0.2,0.2])\n",
    "# thetas = np.zeros((niters-burnin,2),np.float)\n",
    "# for i in range(niters):\n",
    "#     theta = [stats.beta(a+z1,b+N1-z1).rvs(),theta[1]]\n",
    "#     theta = [theta[0],stats.beta(a+z2,b+N2-z2).rvs()]\n",
    "# if i>= burnin:\n",
    "#     thetas[i-burnin] = theta\n",
    "#     kde = stats.gaussian_kde(thetas.T)\n",
    "#     XY = np.vstack([X.ravel(),Y.ravel()])\n",
    "#     posterior_gibbs = kde(XY).reshape(X.shape)\n",
    "# make_plots(X,Y,prior(X,Y),lik(X,Y),posterior_gibbs)\n",
    "# make_plots(X,Y,prior(X,Y),lik(X,Y),posterior_gibbs,projection='3d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
