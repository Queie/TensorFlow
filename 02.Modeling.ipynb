{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 - Statistical Language mModeling\n",
    "통계 언어 모델링 http://untitledtblog.tistory.com/31 (알고리즘해설)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# << context >>\n",
    "# 1 단어 빈도계산 (1gram, 2gram, 3gram)\n",
    "# 2 주어진 텍스트 MLE 개발 (Maximum Likelihood Estimation : 최대우도 측정모델)\n",
    "# 3 MLE 모델의 스무딩 적용 \n",
    "# 4 MLE의 back-off 매커니즘 개발\n",
    "# 5 mix and match 를 억디위한 데이터 보간법 적용\n",
    "# 6 혼잡도(perplexity)를 활용한 언어모델의 평가\n",
    "# 7 Metropolis-Hastings (기각표본추출 알고리즘- 몬테카를로 시뮬레이션에 적용) 를 언어모델링에 적용\n",
    "# 8 Gibbs sampling을 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 1 단어빈도 측정\n",
    "n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 n-Gram token으로 생성하기\n",
    "1-gram, 2-gram, 3-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['De', 'verzekeringsmaatschappijen', 'verhelen', ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import alpino\n",
    "alpino.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('De', 'verzekeringsmaatschappijen', 'verhelen', 'niet')\n",
      "('op', 'het', 'Amerikaanse', 'ingrijpen')\n",
      "('Oogziekenhuis', ',', 'waar', 'de')\n",
      "('de', 'kopers', 'van', 'de')\n",
      "('we', 'hier', 'Hollandse', 'lijnrechters')\n",
      "('defensie', '.', 'Ik', 'heb')\n",
      "('blank', 'deel', 'uiteenvalt', '.')\n",
      "('treedt', ',', 'ben', 'ik')\n",
      "('hij', 'vindt', 'er', 'een')\n",
      "('bij', 'nadere', 'uitwerking', 'niet')\n"
     ]
    }
   ],
   "source": [
    "# 1-gram, 2-gram, 3-gram 으로 token 만들기\n",
    "from nltk.util import ngrams\n",
    "\n",
    "unigrams = ngrams(alpino.words(),1)  # ngrams( tokens, n-num )\n",
    "bigrams = ngrams(alpino.words(),2)\n",
    "quadgrams = ngrams(alpino.words(),4)\n",
    "\n",
    "for i, word in enumerate(quadgrams):\n",
    "    if i % 15000 == 0: print(word) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 우도를 활용한 상위빈도 text 추출하기\n",
    "Maximum Likelihood Estimation : 최대우도 측정모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16967 ['scene', '1', ':', '[', 'wind', ']', '[', 'clop', 'clop', 'clop', ']', 'king', 'arthur', ':', 'whoa', 'there', '!', '[', 'clop', 'clop', 'clop', ']', 'soldier', '#', '1', ':', 'halt', '!', 'who', 'goes', 'there', '?', 'arthur', ':', 'it', 'is', 'i', ',', 'arthur', ',', 'son', 'of', 'uther', 'pendragon', ',']\n"
     ]
    }
   ],
   "source": [
    "# token 생성\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "tokens=[t.lower()    for t in webtext.words('grail.txt')]\n",
    "print(len(tokens), tokens[:45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi-gram   : [(\"'\", 's'), ('arthur', ':'), ('#', '1'), (\"'\", 't'), ('villager', '#'), ('#', '2'), (']', '['), ('1', ':'), ('oh', ','), ('black', 'knight')]\n"
     ]
    }
   ],
   "source": [
    "# bi-gram 중, 우도비율 상위 10개 목록 출력\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "words = BigramCollocationFinder.from_words(tokens)\n",
    "print('bi-gram   :' ,words.nbest(BigramAssocMeasures.likelihood_ratio, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tri-gram   : [('[', 'boom', ']'), ('[', 'singing', ']'), ('[', 'music', ']'), ('[', 'clang', ']'), ('.', 'arthur', ':'), ('[', 'chanting', ']'), ('[', 'pause', ']'), ('[', 'squeak', ']'), ('[', 'thud', ']'), ('[', 'bonk', ']')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "\n",
    "words = TrigramCollocationFinder.from_words(tokens)\n",
    "print('Tri-gram   :' ,words.nbest(TrigramAssocMeasures.likelihood_ratio, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 2 텍스트의 MLE 개발 (Multinomial logistic regression : 다항 기호 논리학 회귀식)\n",
    "주어진 발생에 대한 확률분포를 포함하는 freqdist를 생성한다\n",
    "\n",
    "http://www.nltk.org/howto/probability.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 token 을  freqdist 객체로 변환하기\n",
    "nltk.FreqDist() : 단어의 빈도 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'!': 1,\n",
       "          'a': 1,\n",
       "          'anywhere': 1,\n",
       "          'fish': 1,\n",
       "          'goes': 1,\n",
       "          'good': 1,\n",
       "          'no': 1,\n",
       "          'porpoise': 1,\n",
       "          'without': 1})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# freqdist 생성하기\n",
    "text1 = ['no', 'good', 'fish', 'goes', 'anywhere', 'without', 'a', 'porpoise', '!']\n",
    "text2 = ['no', 'good', 'porpoise', 'likes', 'to', 'fish', 'fish', 'anywhere', '.']\n",
    "\n",
    "import nltk\n",
    "fd1 = nltk.FreqDist(text1); fd1    # text1's token 의  Freqency distance 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'!': 1,\n",
       "          '.': 1,\n",
       "          'a': 1,\n",
       "          'anywhere': 2,\n",
       "          'fish': 3,\n",
       "          'goes': 1,\n",
       "          'good': 2,\n",
       "          'likes': 1,\n",
       "          'no': 2,\n",
       "          'porpoise': 2,\n",
       "          'to': 1,\n",
       "          'without': 1})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text1 과 text2 의 도수빈도를 결합\n",
    "both = nltk.FreqDist(text1 + text2); both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# freqdist 를 pickle로 저장\n",
    "import pickle\n",
    "pickled = pickle.dumps(fd1)\n",
    "fd1 == pickle.loads(pickled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 3 은닉 마르코프 모델 추정\n",
    "Hidden Markov Model estimation\n",
    "\n",
    "$x_0, x_1..$ : 은닉상태 , $y_0, y_1 ..$ : 관찰 가능한 상태\n",
    "\n",
    "<img src=\"http://iacs-courses.seas.harvard.edu/courses/am207/blog/hmm.png\" align='left' width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 Hidden Markov Model estimation\n",
    "HMM추정을 사용해 테스트를 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Dan', 'NP'), ('Morgan', 'NP'), ('told', 'VBD'), ('himself', 'PPL'), ('he', 'PPS'), ('would', 'MD'), ('forget', 'VB'), ('Ann', 'NP'), ('Turner', 'NP'), ('.', '.')], [('He', 'PPS'), ('was', 'BEDZ'), ('well', 'RB'), ('rid', 'JJ'), ('of', 'IN'), ('her', 'PPO'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HMM추정을 사용해 테스트를 수행\n",
    "# brown 말뭉치의 내용 확인 (단어, 품사)의 tuple로 구성\n",
    "import nltk\n",
    "cor = nltk.corpus.brown.tagged_sents(categories='adventure')[:500]\n",
    "cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['NP', 'IN', 'PPS+MD', 'NNS', 'JJR', 'BED', 'BER', 'HVD*', 'OD', 'HVG']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk의 unique_list 내부 살펴보기\n",
    "# tag 살펴보기\n",
    "from nltk.util import unique_list\n",
    "tag_set = unique_list(tag     for sent in cor     for (word,tag) in sent)\n",
    "print(len(tag_set))\n",
    "tag_set[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Dan', 'them', 'possibility', 'wonder', 'cheek', 'careful']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence 살펴보기 \n",
    "symbols = unique_list(word    for sent in cor     for (word,tag) in sent)\n",
    "print(len(symbols)); symbols[::250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.75%\n"
     ]
    }
   ],
   "source": [
    "# 히든 마르코프 훈련모듈 활성화 (92개 tag,  1464개 문장) \n",
    "def train_and_test(est):\n",
    "    import nltk  # nltk.tag.HiddenMarkovModelTrainer()\n",
    "    trainer = nltk.tag.HiddenMarkovModelTrainer(tag_set, symbols)\n",
    "    train_corpus, test_corpus = [], []  \n",
    "    for i in range(len(cor)) :\n",
    "        if i % 10 :  train_corpus += [cor[i]]   # train 90% , test 10% 데이터 생성\n",
    "        else :       test_corpus += [cor[i]]\n",
    "    hmm = trainer.train_supervised(train_corpus, estimator=est)  # .train_supervised()\n",
    "    print('%.2f%%' % (100 * hmm.evaluate(test_corpus)))\n",
    "\n",
    "train_and_test(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 최대 우도추정 모델\n",
    "Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function <lambda> at 0x7fd8e8381400>\n",
      "22.75%\n"
     ]
    }
   ],
   "source": [
    "from nltk import MLEProbDist\n",
    "\n",
    "mle = lambda fd, bins: MLEProbDist(fd)\n",
    "print(mle)\n",
    "train_and_test(mle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.probability.LaplaceProbDist'>\n",
      "66.04%\n"
     ]
    }
   ],
   "source": [
    "# Laplace (= Lidstone with gamma==1)\n",
    "from nltk import MLEProbDist, LaplaceProbDist\n",
    "print(LaplaceProbDist)\n",
    "train_and_test(LaplaceProbDist) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.probability.ELEProbDist'>\n",
      "73.01%\n"
     ]
    }
   ],
   "source": [
    "# Expected Likelihood Estimation (= Lidstone with gamma==0.5)\n",
    "from nltk import ELEProbDist\n",
    "print(ELEProbDist)\n",
    "train_and_test(ELEProbDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.51%\n",
      "None\n",
      "73.01%\n",
      "None\n",
      "66.04%\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Lidstone Estimation, for gamma==0.1, 0.5 and 1 \n",
    "# (the later two should be exactly equal to MLE and ELE above)\n",
    "from nltk import LidstoneProbDist\n",
    "def lidstone(gamma):\n",
    "    return lambda fd, bins: LidstoneProbDist(fd, gamma, bins)\n",
    "\n",
    "print(train_and_test(lidstone(0.1)))\n",
    "print(train_and_test(lidstone(0.5)))\n",
    "print(train_and_test(lidstone(1.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 텍스트의 MLE 개발\n",
    "Lidstone-ProbDist 확률계산공식 01 : $ (c + \\gamma ) / (N(결과)+ B(전체글자수) * \\gamma) $ \n",
    "\n",
    "Lidstone-ProbDist 확률계산공식 02 : $ (c+1)/(N+B) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.51%\n",
      "73.01%\n",
      "82.51%\n"
     ]
    }
   ],
   "source": [
    "# 위의 CLASS를 기초로 '최대우도값'을 추청\n",
    "\n",
    "def lidstone(gamma):\n",
    "    return lambda fd, bins: LidstoneProbDist(fd, gamma, bins)\n",
    "\n",
    "train_and_test(lidstone(0.1))\n",
    "train_and_test(lidstone(0.5))\n",
    "train_and_test(lidstone(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 4 MLE 모델의 Smoothing 적용\n",
    "DB에 포함되지 않은단어 분석시, 0%확률을 보완하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 Add-one smoothing\n",
    "모든 데이터에 count 1을 추가하여 %를 보완한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 {'you', 'find', 'interesting.', 'the', 'are', 'Hope', 'hello', '</s>', 'doing?', 'book', 'how', '<s>'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "corpus = u\"<s> hello how are you doing? Hope you find the book interesting. </s>\".split()\n",
    "vocabulary = set(corpus)\n",
    "print(len(vocabulary), vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConditionalFreqDist(nltk.probability.FreqDist,\n",
       "                    {'<s>': FreqDist({'hello': 1}),\n",
       "                     'Hope': FreqDist({'you': 1}),\n",
       "                     'are': FreqDist({'you': 1}),\n",
       "                     'book': FreqDist({'interesting.': 1}),\n",
       "                     'doing?': FreqDist({'Hope': 1}),\n",
       "                     'find': FreqDist({'the': 1}),\n",
       "                     'hello': FreqDist({'how': 1}),\n",
       "                     'how': FreqDist({'are': 1}),\n",
       "                     'interesting.': FreqDist({'</s>': 1}),\n",
       "                     'the': FreqDist({'book': 1}),\n",
       "                     'you': FreqDist({'doing?': 1, 'find': 1})})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 문장의 bi-gram 말뭉치 수를 count\n",
    "cfd = nltk.ConditionalFreqDist(nltk.bigrams(corpus))\n",
    "cfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token :  <s>how are\n",
      "token :  are you\n",
      "token :  you doing</s>\n"
     ]
    }
   ],
   "source": [
    "# token의 생성\n",
    "sentence = u\"<s>how are you doing</s>\".split()\n",
    "for (a,b) in nltk.bigrams(sentence):\n",
    "    print('token : ',a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[cfd[a][b] for (a,b) in nltk.bigrams(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장의 각 단어 count\n",
    "[cfd[a].N() for (a,b) in nltk.bigrams(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1.0, 0.0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLE 확률에 대한 FreqDist\n",
    "[cfd[a].freq(b) for (a,b) in nltk.bigrams(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 bi-gram 에 대한 Laplace smoothing \n",
    "[1 + cfd[a][b] for (a,b) in nltk.bigrams(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 13, 14]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어의 수를 정규화\n",
    "[len(vocabulary) + cfd[a].N() for (a,b) in nltk.bigrams(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08333333333333333, 0.15384615384615385, 0.07142857142857142]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 bi-gram의 Laplace smoothing 확률\n",
    "[1.0 * (1+cfd[a][b]) / (len(vocabulary)+cfd[a].N()) for (a,b) in nltk.bigrams(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1.0, 0.0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLEProbDist는 smoothing 하지 않은 확률분포\n",
    "cpd_mle = nltk.ConditionalProbDist(cfd, nltk.MLEProbDist, bins=len(vocabulary))\n",
    "[cpd_mle[a].prob(b) for (a,b) in nltk.bigrams(sentence)] # .prob() 메소트로 MLE 확률값 계산`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08333333333333333, 0.15384615384615385, 0.07142857142857142]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LaplaceProbDist : smoothing 된 PorbDist 추가 기능\n",
    "cpd_laplace = nltk.ConditionalProbDist(cfd, nltk.LaplaceProbDist, bins=len(vocabulary))\n",
    "[cpd_laplace[a].prob(b) for (a,b) in nltk.bigrams(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/markbaum/Python/python36/lib/python3.6/site-packages/nltk/probability.py'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.getfile(nltk.SimpleGoodTuringProbDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'</s>': 1,\n",
       "          '<s>': 1,\n",
       "          'Hope': 1,\n",
       "          'are': 1,\n",
       "          'book': 1,\n",
       "          'doing?': 1,\n",
       "          'find': 1,\n",
       "          'hello': 1,\n",
       "          'how': 1,\n",
       "          'interesting.': 1,\n",
       "          'the': 1,\n",
       "          'you': 2})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어의 빈도 구하기\n",
    "freq_corpus = nltk.FreqDist(corpus)\n",
    "freq_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8461538461538463"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SGT 모형생성\n",
    "sgt_pd=nltk.SimpleGoodTuringProbDist(freq_corpus)\n",
    "sgt_pd.discount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8461538461538461,\n",
       " 0.010216928778282503,\n",
       " 0.04145993728504631,\n",
       " 0.8461538461538461]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sgt_pd.prob(v) for v in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11.000000000000002,\n",
       " 1.0,\n",
       " 0.24593726024224907,\n",
       " 0.0909090909090909,\n",
       " 0.04201010351659397,\n",
       " 0.022357932749295366,\n",
       " 0.013116978439320593,\n",
       " 0.008264462809917354,\n",
       " 0.0054986487250421575,\n",
       " 0.003819100319690357]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SGT 결과\n",
    "[sgt_pd.smoothedNr(v+1) for v in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.245937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.042010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.022358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.013117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.008264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.005499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.003819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "0  11.000000\n",
       "1   1.000000\n",
       "2   0.245937\n",
       "3   0.090909\n",
       "4   0.042010\n",
       "5   0.022358\n",
       "6   0.013117\n",
       "7   0.008264\n",
       "8   0.005499\n",
       "9   0.003819"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "plot_sgt = pd.DataFrame([sgt_pd.smoothedNr(v+1) for v in range(10)])\n",
    "plot_sgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd8e256c7b8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd8e3f71400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plot_sgt.plot(kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 Good Turing\n",
    "선형회귀식을 활용하여 로그공간에 빈도의 근사치로 보완\n",
    "\n",
    "$ c' = (c+1)N(c+1)/N(c)for c>=1 $   cf)c == 0일떄 0의 빈도를 갖는 샘플 : N(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markbaum/Python/python36/lib/python3.6/site-packages/nltk/probability.py:1364: UserWarning: SimpleGoodTuring did not find a proper best fit line for smoothing probabilities of occurrences. The probability estimates are likely to be unreliable.\n",
      "  warnings.warn('SimpleGoodTuring did not find a proper best fit '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.93%\n"
     ]
    }
   ],
   "source": [
    "# nltk 의 simple Good Turing \n",
    "from nltk import  SimpleGoodTuringProbDist\n",
    "\n",
    "gt = lambda fd, bins: SimpleGoodTuringProbDist(fd, bins=1e5)\n",
    "train_and_test(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import ProbDistI\n",
    "# Bill Gale and Geoffrey Sampson have presented Simple Good Turing:\n",
    "\n",
    "# 한쌍의 (pi, qi)이 주어진경우, pi 는 빈도를, qi는 빈도의 빈도를 의미한다\n",
    "# 오차제곰을 최소로 하는 결과를 위해. E(p) 와 E(q) 즉 pi와 qi의 평균을 사용한다\n",
    "# - slope, b = sigma ((pi-E(p)(qi-E(q))) / sigma ((pi-E(p))(pi-E(p)))\n",
    "# - intercept: a = E(q) - b.E(p)\n",
    "class SimpleGoodTuringProbDist(ProbDistI):\n",
    "    SUM_TO_ONE = False\n",
    "    \n",
    "    # param freqdist는 확률분포가 추정되는 빈도의 수\n",
    "    # Param bins은 가능한 샘플의 수를 추정시 사용\n",
    "    def __init__(self, freqdist, bins=None):\n",
    "        assert bins is None or bins > freqdist.B(),\\\n",
    "        'bins parameter must not be less than %d=freqdist.B()+1' %(freqdist.B()+1)\n",
    "        if bins is None:\n",
    "            bins = freqdist.B() + 1\n",
    "            self._freqdist = freqdist\n",
    "            self._bins = bins\n",
    "            r, nr = self._r_Nr()\n",
    "            self.find_best_fit(r, nr)\n",
    "            self._switch(r, nr)\n",
    "            self._renormalize(r, nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def _r_Nr_non_zero(self):\n",
    "        r_Nr = self._freqdist.r_Nr()\n",
    "        del r_Nr[0]\n",
    "        return r_Nr\n",
    "\n",
    "    # Nr(r)>0 인 2개목록 (r,Nr)에서 도수분포를 나눈다\n",
    "    def _r_Nr(self): \n",
    "        nonzero = self._r_Nr_non_zero()\n",
    "        if not nonzero: \n",
    "            return [], []\n",
    "        return zip(*sorted(nonzero.items()))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # self._slope, self._intercept 매개변수 조정을 위한, 선형회귀 분석을 시행한다\n",
    "    # (log 공간에서 작업을 함으로써, 부동소숫점 underflow가 발생하지 않도록 한다)\n",
    "    # 높은 샘플빈고를 갖는 경우, Nr = 1을 따라 수평이 된다\n",
    "    # Log 공간에서의 연선을 위해, 주변 0값으로 양의 Nr값을 평균화 한다 (Church and Gale, 1991)\n",
    "    def find_best_fit(self, r, nr):\n",
    "        if not r or not nr: #  r 또는 nr이 비었는지 확인\n",
    "            return\n",
    "        \n",
    "        zr = []\n",
    "        for j in range(len(r)):\n",
    "            i = (r[j-1] if j > 0 else 0)\n",
    "            k = (2 * r[j] - i if j == len(r) - 1 else r[j+1])\n",
    "            zr_ = 2.0 * nr[j] / (k - i)\n",
    "            zr.append(zr_)\n",
    "            log_r = [math.log(i) for i in r]\n",
    "            log_zr = [math.log(i) for i in zr]\n",
    "            xy_cov = x_var = 0.0\n",
    "            x_mean = 1.0 * sum(log_r) / len(log_r)\n",
    "            y_mean = 1.0 * sum(log_zr) / len(log_zr)\n",
    "            for (x, y) in zip(log_r, log_zr):\n",
    "                xy_cov += (x - x_mean) * (y - y_mean)\n",
    "                x_var += (x - x_mean)**2\n",
    "                \n",
    "        self._slope = (xy_cov / x_var if x_var != 0 else 0.0)\n",
    "        if self._slope >= -1:\n",
    "            warnings.warn('SimpleGoodTuring did not find a proper best fit '\n",
    "                          'line for smoothing probabilities of occurrences. '\n",
    "                          'The probability estimates are likely to be '\n",
    "                          'unreliable.')\n",
    "        \n",
    "        self._intercept = y_mean - self._slope * x_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        # E[Nr]을 추정시 Nr에서 Sr로 전환해야 하는 r frontier를 계산한다\n",
    "        def _switch(self, r, nr):\n",
    "            for i, r_ in enumerate(r):\n",
    "                # r이 끝에 있거나, 간격이 있는 경우\n",
    "                if len(r) == i + 1 or r[i+1] != r_ + 1:\n",
    "                    self._switch_at = r_\n",
    "                    break\n",
    "                Sr = self.smoothedNr\n",
    "                smooth_r_star = (r_ + 1) * Sr(r_+1) / Sr(r_)\n",
    "                unsmooth_r_star = 1.0 * (r_ + 1) * nr[i+1] / nr[i]\n",
    "\n",
    "                std = math.sqrt(self._variance(r_, nr[i], nr[i+1]))\n",
    "                if abs(unsmooth_r_star-smooth_r_star) <= 1.96 * std:\n",
    "                    self._switch_at = r_\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        def _variance(self, r, nr, nr_1):\n",
    "            r = float(r)\n",
    "            nr = float(nr)\n",
    "            nr_1 = float(nr_1)\n",
    "            return (r + 1.0)**2 * (nr_1 / nr**2) * (1.0 + nr_1 / nr)\n",
    "\n",
    "        def _renormalize(self, r, nr):\n",
    "            prob_cov = 0.0\n",
    "            for r_, nr_ in zip(r, nr):\n",
    "                prob_cov += nr_ * self._prob_measure(r_)\n",
    "            if prob_cov:\n",
    "                self._renormal = (1 - self._prob_measure(0)) / prob_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        # count r 로 샘플의 수를 반환한다\n",
    "        # Nr = a*r^b (적절한 쌍곡선형 상관을 제공하기 위해 b < -1) \n",
    "        # 방정식의 대수 형테애 대한 간단한 회귀 기법으로, a와 b를 추정한다\n",
    "        # log Nr = a + b*log(r)\n",
    "        def smoothedNr(self, r):\n",
    "            return math.exp(self._intercept + self._slope * math.log(r))\n",
    "\n",
    "        def prob(self, sample): # 샘플의 확률을 반환한다\n",
    "            count = self._freqdist[sample]\n",
    "            p = self._prob_measure(count)\n",
    "            if count == 0:\n",
    "                if self._bins == self._freqdist.B(): p = 0.0\n",
    "                else: p = p / (1.0 * self._bins - self._freqdist.B())\n",
    "            else: p = p * self._renormal\n",
    "            return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        def _prob_measure(self, count):\n",
    "            if count == 0 and self._freqdist.N() == 0 : \n",
    "                return 1.0\n",
    "            elif count == 0 and self._freqdist.N() != 0:\n",
    "                return 1.0 * self._freqdist.Nr(1) / self._freqdist.N()\n",
    "            if self._switch_at > count:\n",
    "                Er_1 = 1.0 * self._freqdist.Nr(count+1)\n",
    "                Er = 1.0 * self._freqdist.Nr(count)\n",
    "            else:\n",
    "                Er_1 = self.smoothedNr(count+1)\n",
    "                Er = self.smoothedNr(count)\n",
    "            r_star = (count + 1) * Er_1 / Er\n",
    "            return r_star / self._freqdist.N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        # 확률의 총합이 1인지를 통해서 (prob_sum != 1.0) 확인\n",
    "        def check(self):\n",
    "            prob_sum = 0.0\n",
    "            for i in range(0, len(self._Nr)):\n",
    "                prob_sum += self._Nr[i] * self._prob_measure(i) / self._renormal\n",
    "            print(\"Probability Sum:\", prob_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        # 보이는 이벤트에서 보이지 않는 이벤트로 확률을 전송\n",
    "        def discount(self):\n",
    "            return 1.0 * self.smoothedNr(1) / self._freqdist.N()\n",
    "        def max(self):\n",
    "            return self._freqdist.max()\n",
    "        def samples(self):\n",
    "            return self._freqdist.keys()\n",
    "        def freqdist(self):\n",
    "            return self._freqdist\n",
    "        def __repr__(self): # ProbDist 의 문자열 표현을 얻는다\n",
    "            return '<SimpleGoodTuringProbDist based on %d samples>'\\\n",
    "            % self._freqdist.N()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 크네저 네이 추정\n",
    "Tri-gram에서 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = \" Hello , please read the book thoroughly .\\\n",
    " If you have any queries , then don't hesitate to ask.\\\n",
    " There is no shortcut to success .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "for x,y,z in nltk.trigrams(sent):\n",
    "    pass\n",
    "    #print(x[0],y[0],z[0])\n",
    "    #print(x[1],y[1],z[1])  # 여기서 오류가 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# corpus = [[((x[0],y[0],z[0]),(x[1],y[1],z[1])) for x, y, z in nltk.trigrams(sent)] \n",
    "#           for sent in corpus[:100]]\n",
    "# tag_set = unique_list(tag for sent in corpus for (word,tag) in sent)\n",
    "# print(len(tag_set))\n",
    "# symbols = unique_list(word for sent in corpus for (word,tag) in sent)\n",
    "# print(len(symbols))\n",
    "# trainer = nltk.tag.HiddenMarkovModelTrainer(tag_set, symbols)\n",
    "# train_corpus = []\n",
    "# test_corpus = []\n",
    "# for i in range(len(corpus)):\n",
    "#     if i % 10: train_corpus += [corpus[i]]\n",
    "#     else:      test_corpus += [corpus[i]]\n",
    "# kn = lambda fd, bins: KneserNeyProbDist(fd)\n",
    "# train_and_test(kn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04 위튼 벨 추정\n",
    "Witten Bell estimation을 활용하여 0의 값을 보완한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.12%\n"
     ]
    }
   ],
   "source": [
    "from nltk import WittenBellProbDist\n",
    "train_and_test(WittenBellProbDist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 5 MLE의 back-off 매커니즘 개발\n",
    "Develop a back-off mechanism for MLE\n",
    "\n",
    "http://blog.naver.com/PostView.nhn?blogId=hot1455&logNo=60128729107"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train시 n-gram이 n번이상 보일경우, 이전정보가 주어지면 token의 조건부 확률은 n-gram의 MLE에 비례한다.\n",
    "# 그렇지 않으면 조건부 확률은 (n-1)gram의 back-off 조건부 확률(이전 정보가 주어진 token의 조건부 확률)과 동일하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# param word: 확률계산할 단어\n",
    "# type word: str\n",
    "# param context: 단어를 포함한 문장\n",
    "# type context: list(str)\n",
    "def prob(self, word, context): # Katz back-off 모델\n",
    "    context = tuple(context)\n",
    "    if(context+(word,) in self._ngrams) or (self._n == 1):\n",
    "        return self[context].prob(word)\n",
    "    else:\n",
    "        return self._alpha(context) * self._backoff.prob(word,context[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 6  Mix and Match를 얻기위한 데이터 보간법\n",
    "uni-gram을 수행후 bi-gram을 수행함으로써 둘의 확률을 결합하는 보간모델의 개발이 가능하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 7 혼잡도를 통한 언어 모델의 평가\n",
    "perplexity(text) : 텍스트의 혼잡도를 평가한다 (2**cross entropy로 측정)\n",
    "\n",
    "텍스트를 예측하는데 유용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculates the perplexity of the given text.\n",
    "This is simply 2 ** cross-entropy for the text.\n",
    ":param text: words to calculate perplexity of\n",
    ":type text: list(str)\n",
    "\"\"\"\n",
    "def perplexity(self, text):\n",
    "    return pow(2.0, self.entropy(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 8 모델링 언어에서 메트로폴리스 헤어스팅스 적용\n",
    "마르코프 모델에서 사후분포처리를 수행하기 위해 Metropolis-Hastings sampler를 사용한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 9 Gibbs sampling 을 언어처리시 적용\n",
    "Gibbs sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N번 시행과 Z번 성공시 Bernoulli 가능성(likelihood)\n",
    "def bern(theta,z,N):\n",
    "    return np.clip(theta**z*(1-theta)**(N-z),0,1)\n",
    "# N번 시행과 Z번 성공시 Bernoulli 가능성(likelihood)\n",
    "def bern2(theta1,theta2,z1,z2,N1,N2):\n",
    "    return bern(theta1,z1,N1)*bern(theta2,z2,N2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_thetas(xmin,xmax,n):\n",
    "    xs=np.linspace(xmin,xmax,n)\n",
    "    widths=(xs[1:]-xs[:-1])/2.0\n",
    "    thetas=xs[:-1]+widths\n",
    "    return hetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_plots(X,Y,prior,likelihood,posterior,projection=None):\n",
    "    fig,ax=plt.subplots(1,3,subplot_kw=dict(projection=projection,aspect='equal'),figsize=(12,3))\n",
    "    if projection=='3d':\n",
    "        ax[0].plot_surface(X,Y,prior,alpha=0.3,cmap=plt.cm.jet)\n",
    "        ax[1].plot_surface(X,Y,likelihood,alpha=0.3,cmap=plt.cm.jet)\n",
    "        ax[2].plot_surface(X,Y,posterior,alpha=0.3,cmap=plt.cm.jet)\n",
    "    else:\n",
    "        ax[0].contour(X,Y,prior)\n",
    "        ax[1].contour(X,Y,likelihood)\n",
    "        ax[2].contour(X,Y,posterior)\n",
    "        ax[0].set_title('Prior')\n",
    "        ax[1].set_title('Likelihood')\n",
    "        ax[2].set_title('Posteior')\n",
    "    plt.tight_layout()\n",
    "    thetas1=make_thetas(0,1,101)\n",
    "    thetas2=make_thetas(0,1,101)\n",
    "    X,Y=np.meshgrid(thetas1,thetas2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Metropolice 시행\n",
    "# import numpy as np\n",
    "# a = 2\n",
    "# b = 3\n",
    "# z1 = 11\n",
    "# N1 = 14\n",
    "# z2 = 7\n",
    "# N2 = 14\n",
    "# prior = lambda theta1,theta2:stats.beta(a,b).pdf(theta1)*stats.beta(a,b).pdf(theta2)\n",
    "# lik = partial(bern2,z1=z1,z2=z2,N1=N1,N2=N2)\n",
    "# target = lambda theta1,theta2:prior(theta1,theta2)*lik(theta1,theta2)\n",
    "# theta = np.array([0.5,0.5])\n",
    "# niters = 10000\n",
    "# burnin = 500\n",
    "# sigma = np.diag([0.2,0.2])\n",
    "# thetas = np.zeros((niters-burnin,2),np.float)\n",
    "# for i in range(niters):\n",
    "#     new_theta = stats.multivariate_normal(theta,sigma).rvs()\n",
    "#     p = min(target(*new_theta)/target(*theta),1)\n",
    "#     if np.random.rand() < p:\n",
    "#         theta = new_theta\n",
    "#     if i >= burnin:\n",
    "#         thetas[i-burnin] = theta\n",
    "#         kde = stats.gaussian_kde(thetas.T)\n",
    "# XY = np.vstack([X.ravel(),Y.ravel()])\n",
    "# posterior_metroplis = kde(XY).reshape(X.shape)\n",
    "# make_plots(X,Y,prior(X,Y),lik(X,Y),posterior_metroplis)\n",
    "# make_plots(X,Y,prior(X,Y),lik(X,Y),posterior_metroplis,projection='3d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gibbs 수행\n",
    "# a = 2\n",
    "# b = 3\n",
    "# z1 = 11\n",
    "# N1 = 14\n",
    "# z2 = 7\n",
    "# N2 = 14\n",
    "# prior = lambda theta1,theta2:stats.beta(a,b).pdf(theta1)*stats.\n",
    "# beta(a,b).pdf(theta2)\n",
    "# lik = partial(bern2,z1=z1,z2=z2,N1=N1,N2=N2)\n",
    "# target = lambdatheta1,theta2:prior(theta1,theta2)*lik(theta1,theta2)\n",
    "# theta = np.array([0.5,0.5])\n",
    "# niters = 10000\n",
    "# burnin = 500\n",
    "# sigma = np.diag([0.2,0.2])\n",
    "# thetas = np.zeros((niters-burnin,2),np.float)\n",
    "# for i in range(niters):\n",
    "#     theta = [stats.beta(a+z1,b+N1-z1).rvs(),theta[1]]\n",
    "#     theta = [theta[0],stats.beta(a+z2,b+N2-z2).rvs()]\n",
    "# if i>= burnin:\n",
    "#     thetas[i-burnin] = theta\n",
    "#     kde = stats.gaussian_kde(thetas.T)\n",
    "#     XY = np.vstack([X.ravel(),Y.ravel()])\n",
    "#     posterior_gibbs = kde(XY).reshape(X.shape)\n",
    "# make_plots(X,Y,prior(X,Y),lik(X,Y),posterior_gibbs)\n",
    "# make_plots(X,Y,prior(X,Y),lik(X,Y),posterior_gibbs,projection='3d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
