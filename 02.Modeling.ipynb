{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 - Statistical Language mModeling\n",
    "통계 언어 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# << context >>\n",
    "# 1 단어 빈도계산 (1gram, 2gram, 3gram)\n",
    "# 2 주어진 텍스트 MLE 개발 (Maximum Likelihood Estimation : 최대우도 측정모델)\n",
    "# 3 MLE 모델의 스무딩 적용 \n",
    "# 4 MLE의 back-off 매커니즘 개발\n",
    "# 5 mix and match 를 억디위한 데이터 보간법 적용\n",
    "# 6 혼잡도(perplexity)를 활용한 언어모델의 평가\n",
    "# 7 Metropolis-Hastings (기각표본추출 알고리즘- 몬테카를로 시뮬레이션에 적용) 를 언어모델링에 적용\n",
    "# 8 Gibbs sampling을 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 1 단어빈도 측정\n",
    "1-gram, 2-gram, 3-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 n-Gram token으로 생성하기\n",
    "1-gram, 2-gram, 3-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('De', 'verzekeringsmaatschappijen', 'verhelen', 'niet')\n",
      "('op', 'het', 'Amerikaanse', 'ingrijpen')\n",
      "('Oogziekenhuis', ',', 'waar', 'de')\n",
      "('de', 'kopers', 'van', 'de')\n",
      "('we', 'hier', 'Hollandse', 'lijnrechters')\n",
      "('defensie', '.', 'Ik', 'heb')\n",
      "('blank', 'deel', 'uiteenvalt', '.')\n",
      "('treedt', ',', 'ben', 'ik')\n",
      "('hij', 'vindt', 'er', 'een')\n",
      "('bij', 'nadere', 'uitwerking', 'niet')\n"
     ]
    }
   ],
   "source": [
    "# 1-gram, 2-gram, 3-gram 으로 token 만들기\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import alpino\n",
    "\n",
    "unigrams = ngrams(alpino.words(),1)\n",
    "bigrams = ngrams(alpino.words(),2)\n",
    "quadgrams = ngrams(alpino.words(),4)\n",
    "\n",
    "for i, word in enumerate(quadgrams):\n",
    "    if i % 15000 == 0: print(word) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 우도를 활용한 상위빈도 text 추출하기\n",
    "Maximum Likelihood Estimation : 최대우도 측정모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# token 생성\n",
    "from nltk.corpus import webtext\n",
    "tokens=[t.lower()    for t in webtext.words('grail.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi-gram   : [(\"'\", 's'), ('arthur', ':'), ('#', '1'), (\"'\", 't'), ('villager', '#'), ('#', '2'), (']', '['), ('1', ':'), ('oh', ','), ('black', 'knight')]\n"
     ]
    }
   ],
   "source": [
    "# bi-gram 중, 우도비율 상위 10개 목록 출력\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "words = BigramCollocationFinder.from_words(tokens)\n",
    "\n",
    "print('bi-gram   :' ,words.nbest(BigramAssocMeasures.likelihood_ratio, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 2 텍스트의 MLE 개발\n",
    "Multinomial logistic regression : 다항 기호 논리학 회귀\n",
    "\n",
    "주어진 발생에 대한 확률분포를 포함하는 freqdist를 생성한다\n",
    "\n",
    "http://www.nltk.org/howto/probability.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 token 을  freqdist 객체로 변환하기\n",
    "nltk.FreqDist() : 단어의 빈도 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'!': 1,\n",
       "          'a': 1,\n",
       "          'anywhere': 1,\n",
       "          'fish': 1,\n",
       "          'goes': 1,\n",
       "          'good': 1,\n",
       "          'no': 1,\n",
       "          'porpoise': 1,\n",
       "          'without': 1})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# freqdist 생성하기\n",
    "text1 = ['no', 'good', 'fish', 'goes', 'anywhere', 'without', 'a', 'porpoise', '!']\n",
    "text2 = ['no', 'good', 'porpoise', 'likes', 'to', 'fish', 'fish', 'anywhere', '.']\n",
    "\n",
    "import nltk\n",
    "fd1 = nltk.FreqDist(text1); fd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'!': 1,\n",
       "          '.': 1,\n",
       "          'a': 1,\n",
       "          'anywhere': 2,\n",
       "          'fish': 3,\n",
       "          'goes': 1,\n",
       "          'good': 2,\n",
       "          'likes': 1,\n",
       "          'no': 2,\n",
       "          'porpoise': 2,\n",
       "          'to': 1,\n",
       "          'without': 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text1 과 text2 의 도수빈도를 결합\n",
    "both = nltk.FreqDist(text1 + text2); both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# freqdist 를 pickle로 저장\n",
    "import pickle\n",
    "pickled = pickle.dumps(fd1)\n",
    "fd1 == pickle.loads(pickled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 3 은닉 마르코프 모델 추정\n",
    "Hidden Markov Model estimation\n",
    "\n",
    "$x_0, x_1..$ : 은닉상태 , $y_0, y_1 ..$ : 관찰 가능한 상태\n",
    "\n",
    "<img src=\"http://iacs-courses.seas.harvard.edu/courses/am207/blog/hmm.png\" align='left' width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 Hidden Markov Model estimation\n",
    "HMM추정을 사용해 테스트를 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Dan', 'NP'), ('Morgan', 'NP'), ('told', 'VBD'), ('himself', 'PPL'), ('he', 'PPS'), ('would', 'MD'), ('forget', 'VB'), ('Ann', 'NP'), ('Turner', 'NP'), ('.', '.')], [('He', 'PPS'), ('was', 'BEDZ'), ('well', 'RB'), ('rid', 'JJ'), ('of', 'IN'), ('her', 'PPO'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HMM추정을 사용해 테스트를 수행\n",
    "# brown 말뭉치의 내용 확인 (단어, 품사)의 tuple로 구성\n",
    "import nltk\n",
    "cor = nltk.corpus.brown.tagged_sents(categories='adventure')[:500]\n",
    "cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['NP', 'IN', 'PPS+MD', 'NNS', 'JJR', 'BED', 'BER', 'HVD*', 'OD', 'HVG']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk의 unique_list 내부 살펴보기\n",
    "# tag 살펴보기\n",
    "from nltk.util import unique_list\n",
    "tag_set = unique_list(tag     for sent in cor     for (word,tag) in sent)\n",
    "print(len(tag_set))\n",
    "tag_set[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Dan', 'them', 'possibility', 'wonder', 'cheek', 'careful']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence 살펴보기 \n",
    "symbols = unique_list(word    for sent in cor     for (word,tag) in sent)\n",
    "print(len(symbols)); symbols[::250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# 히든 마르코프 훈련모듈 활성화 (92개 tag,  1464개 문장)\n",
    "trainer = nltk.tag.HiddenMarkovModelTrainer(tag_set, symbols)\n",
    "train_corpus = []\n",
    "test_corpus = []\n",
    "\n",
    "# train 90% , test 10% 데이터 생성\n",
    "for i in range(len(cor)):\n",
    "    if i % 10:  train_corpus+=[cor[i]]\n",
    "    else: test_corpus+=[cor[i]]         \n",
    "print(len(train_corpus))\n",
    "print(len(test_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.75%\n"
     ]
    }
   ],
   "source": [
    "# 은닉 마르코프 추정법칙 테스트\n",
    "def train_and_test(est):\n",
    "    hmm = trainer.train_supervised(train_corpus, estimator=est)\n",
    "    print('%.2f%%' % (100 * hmm.evaluate(test_corpus)))\n",
    "\n",
    "train_and_test(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 최대 우도추정 모델\n",
    "Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.75%\n",
      "66.04%\n"
     ]
    }
   ],
   "source": [
    "from nltk import MLEProbDist\n",
    "\n",
    "mle = lambda fd, bins: MLEProbDist(fd)\n",
    "train_and_test(mle)\n",
    "\n",
    "# Laplace (= Lidstone with gamma==1)\n",
    "from nltk import MLEProbDist, LaplaceProbDist\n",
    "train_and_test(LaplaceProbDist) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.01%\n"
     ]
    }
   ],
   "source": [
    "# Expected Likelihood Estimation (= Lidstone with gamma==0.5)\n",
    "from nltk import ELEProbDist\n",
    "train_and_test(ELEProbDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.51%\n",
      "None\n",
      "73.01%\n",
      "None\n",
      "66.04%\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Lidstone Estimation, for gamma==0.1, 0.5 and 1 (the later two should be exactly equal to MLE and ELE above)\n",
    "from nltk import LidstoneProbDist\n",
    "def lidstone(gamma):\n",
    "    return lambda fd, bins: LidstoneProbDist(fd, gamma, bins)\n",
    "\n",
    "print(train_and_test(lidstone(0.1)))\n",
    "print(train_and_test(lidstone(0.5)))\n",
    "print(train_and_test(lidstone(1.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 텍스트의 MLE 개발\n",
    "다중회귀모델 개발"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 위의 CLASS를 기초로 '최대우도값'을 추청\n",
    "import nltk\n",
    "from nltk.probability import *\n",
    "# train_and_test(mle)\n",
    "# train_and_test(LaplaceProbDist)\n",
    "# train_and_test(ELEProbDist)\n",
    "def lidstone(gamma):\n",
    "    return lambda fd, bins: LidstoneProbDist(fd, gamma, bins)\n",
    "# train_and_test(lidstone(0.1))\n",
    "# train_and_test(lidstone(0.5))\n",
    "# train_and_test(lidstone(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 4 MLE 모델의 Smoothing 적용\n",
    "알려지지 않은 단어는 확률이 0이므로, 이를 해결시 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 Add-one smoothing\n",
    "가짜 수 (Pseudo Count) 1을 활용하여, 확률이 0이 아닌 값을 덧붙인다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "corpus = u\"<s> hello how are you doing ? Hope you find the book interesting. </s>\".split()\n",
    "vocabulary = set(corpus)\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 문장의 bi-gram 말뭉치 수를 count\n",
    "sentence = u\"<s>how are you doing</s>\".split()\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(nltk.bigrams(corpus))\n",
    "[cfd[a][b] for (a,b) in nltk.bigrams(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장의 각 단어 count\n",
    "[cfd[a].N() for (a,b) in nltk.bigrams(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1.0, 0.0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLE 확률에 대한 FreqDist\n",
    "[cfd[a].freq(b) for (a,b) in nltk.bigrams(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 bi-gram 에 대한 Laplace smoothing \n",
    "[1 + cfd[a][b] for (a,b) in nltk.bigrams(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 14, 15]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어의 수를 정규화\n",
    "[len(vocabulary) + cfd[a].N() for (a,b) in nltk.bigrams(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07692307692307693, 0.14285714285714285, 0.06666666666666667]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 bi-gram의 Laplace smoothing 확률\n",
    "[1.0 * (1+cfd[a][b]) / (len(vocabulary)+cfd[a].N()) for (a,b) in nltk.bigrams(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1.0, 0.0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLEProbDist는 smoothing 하지 않은 확률분포\n",
    "cpd_mle = nltk.ConditionalProbDist(cfd, nltk.MLEProbDist, bins=len(vocabulary))\n",
    "[cpd_mle[a].prob(b) for (a,b) in nltk.bigrams(sentence)] # .prob() 메소트로 MLE 확률값 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07692307692307693, 0.14285714285714285, 0.06666666666666667]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LaplaceProbDist : smoothing 된 PorbDist 추가 기능\n",
    "cpd_laplace = nltk.ConditionalProbDist(cfd, nltk.LaplaceProbDist, bins=len(vocabulary))\n",
    "[cpd_laplace[a].prob(b) for (a,b) in nltk.bigrams(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/markbaum/Python/python36/lib/python3.6/site-packages/nltk/probability.py'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.getfile(nltk.SimpleGoodTuringProbDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'</s>': 1,\n",
       "          '<s>': 1,\n",
       "          '?': 1,\n",
       "          'Hope': 1,\n",
       "          'are': 1,\n",
       "          'book': 1,\n",
       "          'doing': 1,\n",
       "          'find': 1,\n",
       "          'hello': 1,\n",
       "          'how': 1,\n",
       "          'interesting.': 1,\n",
       "          'the': 1,\n",
       "          'you': 2})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어의 빈도 구하기\n",
    "freq_corpus = nltk.FreqDist(corpus)\n",
    "freq_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SGT 모형생성\n",
    "sgt_pd=nltk.SimpleGoodTuringProbDist(freq_corpus)\n",
    "sgt_pd.discount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8571428571428571,\n",
       " 0.008814431021059511,\n",
       " 0.03708397060442877,\n",
       " 0.8571428571428571]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sgt_pd.prob(v) for v in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12.0,\n",
       " 1.0,\n",
       " 0.23373268044331344,\n",
       " 0.08333333333333333,\n",
       " 0.03744553081485653,\n",
       " 0.019477723370276128,\n",
       " 0.011208215258425342,\n",
       " 0.00694444444444445,\n",
       " 0.004552580492268005,\n",
       " 0.0031204609012380384]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SGT 결과\n",
    "[sgt_pd.smoothedNr(v+1) for v in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.233733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.037446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.019478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.011208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.006944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.004553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.003120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "0  12.000000\n",
       "1   1.000000\n",
       "2   0.233733\n",
       "3   0.083333\n",
       "4   0.037446\n",
       "5   0.019478\n",
       "6   0.011208\n",
       "7   0.006944\n",
       "8   0.004553\n",
       "9   0.003120"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "plot_sgt = pd.DataFrame([sgt_pd.smoothedNr(v+1) for v in range(10)])\n",
    "plot_sgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fbc6b4756a0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTOROULIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbc6b400fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plot_sgt.plot(kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 Good Turing\n",
    "보이지 않는 객체의 확룰을 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltk 의 simple Good Turing \n",
    "gt = lambda fd, bins: SimpleGoodTuringProbDist(fd, bins=1e5)\n",
    "train_and_test(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bill Gale and Geoffrey Sampson have presented Simple Good Turing:\n",
    "\n",
    "# 한쌍의 (pi, qi)이 주어진경우, pi 는 빈도를, qi는 빈도의 빈도를 의미한다\n",
    "# 오차제곰을 최소로 하는 결과를 위해. E(p) 와 E(q) 즉 pi와 qi의 평균을 사용한다\n",
    "# - slope, b = sigma ((pi-E(p)(qi-E(q))) / sigma ((pi-E(p))(pi-E(p)))\n",
    "# - intercept: a = E(q) - b.E(p)\n",
    "class SimpleGoodTuringProbDist(ProbDistI):\n",
    "    SUM_TO_ONE = False\n",
    "    \n",
    "    # param freqdist는 확률분포가 추정되는 빈도의 수\n",
    "    # Param bins은 가능한 샘플의 수를 추정시 사용\n",
    "    def __init__(self, freqdist, bins=None):\n",
    "        assert bins is None or bins > freqdist.B(),\\\n",
    "        'bins parameter must not be less than %d=freqdist.B()+1' %(freqdist.B()+1)\n",
    "        if bins is None:\n",
    "            bins = freqdist.B() + 1\n",
    "            self._freqdist = freqdist\n",
    "            self._bins = bins\n",
    "            r, nr = self._r_Nr()\n",
    "            self.find_best_fit(r, nr)\n",
    "            self._switch(r, nr)\n",
    "            self._renormalize(r, nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def _r_Nr_non_zero(self):\n",
    "        r_Nr = self._freqdist.r_Nr()\n",
    "        del r_Nr[0]\n",
    "        return r_Nr\n",
    "\n",
    "    # Nr(r)>0 인 2개목록 (r,Nr)에서 도수분포를 나눈다\n",
    "    def _r_Nr(self): \n",
    "        nonzero = self._r_Nr_non_zero()\n",
    "        if not nonzero: \n",
    "            return [], []\n",
    "        return zip(*sorted(nonzero.items()))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # self._slope, self._intercept 매개변수 조정을 위한, 선형회귀 분석을 시행한다\n",
    "    # (log 공간에서 작업을 함으로써, 부동소숫점 underflow가 발생하지 않도록 한다)\n",
    "    # 높은 샘플빈고를 갖는 경우, Nr = 1을 따라 수평이 된다\n",
    "    # Log 공간에서의 연선을 위해, 주변 0값으로 양의 Nr값을 평균화 한다 (Church and Gale, 1991)\n",
    "    def find_best_fit(self, r, nr):\n",
    "        if not r or not nr: #  r 또는 nr이 비었는지 확인\n",
    "            return\n",
    "        \n",
    "        zr = []\n",
    "        for j in range(len(r)):\n",
    "            i = (r[j-1] if j > 0 else 0)\n",
    "            k = (2 * r[j] - i if j == len(r) - 1 else r[j+1])\n",
    "            zr_ = 2.0 * nr[j] / (k - i)\n",
    "            zr.append(zr_)\n",
    "            log_r = [math.log(i) for i in r]\n",
    "            log_zr = [math.log(i) for i in zr]\n",
    "            xy_cov = x_var = 0.0\n",
    "            x_mean = 1.0 * sum(log_r) / len(log_r)\n",
    "            y_mean = 1.0 * sum(log_zr) / len(log_zr)\n",
    "            for (x, y) in zip(log_r, log_zr):\n",
    "                xy_cov += (x - x_mean) * (y - y_mean)\n",
    "                x_var += (x - x_mean)**2\n",
    "                \n",
    "        self._slope = (xy_cov / x_var if x_var != 0 else 0.0)\n",
    "        if self._slope >= -1:\n",
    "            warnings.warn('SimpleGoodTuring did not find a proper best fit '\n",
    "                          'line for smoothing probabilities of occurrences. '\n",
    "                          'The probability estimates are likely to be '\n",
    "                          'unreliable.')\n",
    "        \n",
    "        self._intercept = y_mean - self._slope * x_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        # E[Nr]을 추정시 Nr에서 Sr로 전환해야 하는 r frontier를 계산한다\n",
    "        def _switch(self, r, nr):\n",
    "            for i, r_ in enumerate(r):\n",
    "                # r이 끝에 있거나, 간격이 있는 경우\n",
    "                if len(r) == i + 1 or r[i+1] != r_ + 1:\n",
    "                    self._switch_at = r_\n",
    "                    break\n",
    "                Sr = self.smoothedNr\n",
    "                smooth_r_star = (r_ + 1) * Sr(r_+1) / Sr(r_)\n",
    "                unsmooth_r_star = 1.0 * (r_ + 1) * nr[i+1] / nr[i]\n",
    "\n",
    "                std = math.sqrt(self._variance(r_, nr[i], nr[i+1]))\n",
    "                if abs(unsmooth_r_star-smooth_r_star) <= 1.96 * std:\n",
    "                    self._switch_at = r_\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        def _variance(self, r, nr, nr_1):\n",
    "            r = float(r)\n",
    "            nr = float(nr)\n",
    "            nr_1 = float(nr_1)\n",
    "            return (r + 1.0)**2 * (nr_1 / nr**2) * (1.0 + nr_1 / nr)\n",
    "\n",
    "        def _renormalize(self, r, nr):\n",
    "            prob_cov = 0.0\n",
    "            for r_, nr_ in zip(r, nr):\n",
    "                prob_cov += nr_ * self._prob_measure(r_)\n",
    "            if prob_cov:\n",
    "                self._renormal = (1 - self._prob_measure(0)) / prob_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        # count r 로 샘플의 수를 반환한다\n",
    "        # Nr = a*r^b (적절한 쌍곡선형 상관을 제공하기 위해 b < -1) \n",
    "        # 방정식의 대수 형테애 대한 간단한 회귀 기법으로, a와 b를 추정한다\n",
    "        # log Nr = a + b*log(r)\n",
    "        def smoothedNr(self, r):\n",
    "            return math.exp(self._intercept + self._slope * math.log(r))\n",
    "\n",
    "        def prob(self, sample): # 샘플의 확률을 반환한다\n",
    "            count = self._freqdist[sample]\n",
    "            p = self._prob_measure(count)\n",
    "            if count == 0:\n",
    "                if self._bins == self._freqdist.B(): p = 0.0\n",
    "                else: p = p / (1.0 * self._bins - self._freqdist.B())\n",
    "            else: p = p * self._renormal\n",
    "            return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        def _prob_measure(self, count):\n",
    "            if count == 0 and self._freqdist.N() == 0 : \n",
    "                return 1.0\n",
    "            elif count == 0 and self._freqdist.N() != 0:\n",
    "                return 1.0 * self._freqdist.Nr(1) / self._freqdist.N()\n",
    "            if self._switch_at > count:\n",
    "                Er_1 = 1.0 * self._freqdist.Nr(count+1)\n",
    "                Er = 1.0 * self._freqdist.Nr(count)\n",
    "            else:\n",
    "                Er_1 = self.smoothedNr(count+1)\n",
    "                Er = self.smoothedNr(count)\n",
    "            r_star = (count + 1) * Er_1 / Er\n",
    "            return r_star / self._freqdist.N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        # 확률의 총합이 1인지를 통해서 (prob_sum != 1.0) 확인\n",
    "        def check(self):\n",
    "            prob_sum = 0.0\n",
    "            for i in range(0, len(self._Nr)):\n",
    "                prob_sum += self._Nr[i] * self._prob_measure(i) / self._renormal\n",
    "            print(\"Probability Sum:\", prob_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        # 보이는 이벤트에서 보이지 않는 이벤트로 확률을 전송\n",
    "        def discount(self):\n",
    "            return 1.0 * self.smoothedNr(1) / self._freqdist.N()\n",
    "        def max(self):\n",
    "            return self._freqdist.max()\n",
    "        def samples(self):\n",
    "            return self._freqdist.keys()\n",
    "        def freqdist(self):\n",
    "            return self._freqdist\n",
    "        def __repr__(self): # ProbDist 의 문자열 표현을 얻는다\n",
    "            return '<SimpleGoodTuringProbDist based on %d samples>'\\\n",
    "            % self._freqdist.N()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 크네저 네이 추정\n",
    "Kneser Ney estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = \" Hello , please read the book thoroughly .\\\n",
    " If you have any queries , then don't hesitate to ask.\\\n",
    " There is no shortcut to success .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "for x,y,z in nltk.trigrams(sent):\n",
    "    pass\n",
    "    #print(x[0],y[0],z[0])\n",
    "    #print(x[1],y[1],z[1])  # 여기서 오류가 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# corpus = [[((x[0],y[0],z[0]),(x[1],y[1],z[1])) for x, y, z in nltk.trigrams(sent)] \n",
    "#           for sent in corpus[:100]]\n",
    "# tag_set = unique_list(tag for sent in corpus for (word,tag) in sent)\n",
    "# print(len(tag_set))\n",
    "# symbols = unique_list(word for sent in corpus for (word,tag) in sent)\n",
    "# print(len(symbols))\n",
    "# trainer = nltk.tag.HiddenMarkovModelTrainer(tag_set, symbols)\n",
    "# train_corpus = []\n",
    "# test_corpus = []\n",
    "# for i in range(len(corpus)):\n",
    "#     if i % 10: train_corpus += [corpus[i]]\n",
    "#     else:      test_corpus += [corpus[i]]\n",
    "# kn = lambda fd, bins: KneserNeyProbDist(fd)\n",
    "# train_and_test(kn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04 위튼 벨 추정\n",
    "Witten Bell estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_and_test(WittenBellProbDist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 5 MLE의 back-off 매커니즘 개발\n",
    "Develop a back-off mechanism for MLE\n",
    "\n",
    "http://blog.naver.com/PostView.nhn?blogId=hot1455&logNo=60128729107"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train시 n-gram이 n번이상 보일경우, 이전정보가 주어지면 token의 조건부 확률은 n-gram의 MLE에 비례한다.\n",
    "# 그렇지 않으면 조건부 확률은 (n-1)gram의 back-off 조건부 확률(이전 정보가 주어진 token의 조건부 확률)과 동일하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# param word: 확률계산할 단어\n",
    "# type word: str\n",
    "# param context: 단어를 포함한 문장\n",
    "# type context: list(str)\n",
    "def prob(self, word, context): # Katz back-off 모델\n",
    "    context = tuple(context)\n",
    "    if(context+(word,) in self._ngrams) or (self._n == 1):\n",
    "        return self[context].prob(word)\n",
    "    else:\n",
    "        return self._alpha(context) * self._backoff.prob(word,context[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 6  Mix and Match를 얻기위한 데이터 보간법\n",
    "uni-gram을 수행후 bi-gram을 수행함으로써 둘의 확률을 결합하는 보간모델의 개발이 가능하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 7 혼잡도를 통한 언어 모델의 평가\n",
    "perplexity(text) : 텍스트의 혼잡도를 평가한다 (2**cross entropy로 측정)\n",
    "\n",
    "텍스트를 예측하는데 유용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculates the perplexity of the given text.\n",
    "This is simply 2 ** cross-entropy for the text.\n",
    ":param text: words to calculate perplexity of\n",
    ":type text: list(str)\n",
    "\"\"\"\n",
    "def perplexity(self, text):\n",
    "    return pow(2.0, self.entropy(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 8 모델링 언어에서 메트로폴리스 헤어스팅스 적용\n",
    "마르코프 모델에서 사후분포처리를 수행하기 위해 Metropolis-Hastings sampler를 사용한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 9 Gibbs sampling 을 언어처리시 적용\n",
    "Gibbs sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N번 시행과 Z번 성공시 Bernoulli 가능성(likelihood)\n",
    "def bern(theta,z,N):\n",
    "    return np.clip(theta**z*(1-theta)**(N-z),0,1)\n",
    "# N번 시행과 Z번 성공시 Bernoulli 가능성(likelihood)\n",
    "def bern2(theta1,theta2,z1,z2,N1,N2):\n",
    "    return bern(theta1,z1,N1)*bern(theta2,z2,N2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_thetas(xmin,xmax,n):\n",
    "    xs=np.linspace(xmin,xmax,n)\n",
    "    widths=(xs[1:]-xs[:-1])/2.0\n",
    "    thetas=xs[:-1]+widths\n",
    "    return hetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_plots(X,Y,prior,likelihood,posterior,projection=None):\n",
    "    fig,ax=plt.subplots(1,3,subplot_kw=dict(projection=projection,aspect='equal'),figsize=(12,3))\n",
    "    if projection=='3d':\n",
    "        ax[0].plot_surface(X,Y,prior,alpha=0.3,cmap=plt.cm.jet)\n",
    "        ax[1].plot_surface(X,Y,likelihood,alpha=0.3,cmap=plt.cm.jet)\n",
    "        ax[2].plot_surface(X,Y,posterior,alpha=0.3,cmap=plt.cm.jet)\n",
    "    else:\n",
    "        ax[0].contour(X,Y,prior)\n",
    "        ax[1].contour(X,Y,likelihood)\n",
    "        ax[2].contour(X,Y,posterior)\n",
    "        ax[0].set_title('Prior')\n",
    "        ax[1].set_title('Likelihood')\n",
    "        ax[2].set_title('Posteior')\n",
    "    plt.tight_layout()\n",
    "    thetas1=make_thetas(0,1,101)\n",
    "    thetas2=make_thetas(0,1,101)\n",
    "    X,Y=np.meshgrid(thetas1,thetas2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Metropolice 시행\n",
    "# import numpy as np\n",
    "# a = 2\n",
    "# b = 3\n",
    "# z1 = 11\n",
    "# N1 = 14\n",
    "# z2 = 7\n",
    "# N2 = 14\n",
    "# prior = lambda theta1,theta2:stats.beta(a,b).pdf(theta1)*stats.beta(a,b).pdf(theta2)\n",
    "# lik = partial(bern2,z1=z1,z2=z2,N1=N1,N2=N2)\n",
    "# target = lambda theta1,theta2:prior(theta1,theta2)*lik(theta1,theta2)\n",
    "# theta = np.array([0.5,0.5])\n",
    "# niters = 10000\n",
    "# burnin = 500\n",
    "# sigma = np.diag([0.2,0.2])\n",
    "# thetas = np.zeros((niters-burnin,2),np.float)\n",
    "# for i in range(niters):\n",
    "#     new_theta = stats.multivariate_normal(theta,sigma).rvs()\n",
    "#     p = min(target(*new_theta)/target(*theta),1)\n",
    "#     if np.random.rand() < p:\n",
    "#         theta = new_theta\n",
    "#     if i >= burnin:\n",
    "#         thetas[i-burnin] = theta\n",
    "#         kde = stats.gaussian_kde(thetas.T)\n",
    "# XY = np.vstack([X.ravel(),Y.ravel()])\n",
    "# posterior_metroplis = kde(XY).reshape(X.shape)\n",
    "# make_plots(X,Y,prior(X,Y),lik(X,Y),posterior_metroplis)\n",
    "# make_plots(X,Y,prior(X,Y),lik(X,Y),posterior_metroplis,projection='3d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gibbs 수행\n",
    "# a = 2\n",
    "# b = 3\n",
    "# z1 = 11\n",
    "# N1 = 14\n",
    "# z2 = 7\n",
    "# N2 = 14\n",
    "# prior = lambda theta1,theta2:stats.beta(a,b).pdf(theta1)*stats.\n",
    "# beta(a,b).pdf(theta2)\n",
    "# lik = partial(bern2,z1=z1,z2=z2,N1=N1,N2=N2)\n",
    "# target = lambdatheta1,theta2:prior(theta1,theta2)*lik(theta1,theta2)\n",
    "# theta = np.array([0.5,0.5])\n",
    "# niters = 10000\n",
    "# burnin = 500\n",
    "# sigma = np.diag([0.2,0.2])\n",
    "# thetas = np.zeros((niters-burnin,2),np.float)\n",
    "# for i in range(niters):\n",
    "#     theta = [stats.beta(a+z1,b+N1-z1).rvs(),theta[1]]\n",
    "#     theta = [theta[0],stats.beta(a+z2,b+N2-z2).rvs()]\n",
    "# if i>= burnin:\n",
    "#     thetas[i-burnin] = theta\n",
    "#     kde = stats.gaussian_kde(thetas.T)\n",
    "#     XY = np.vstack([X.ravel(),Y.ravel()])\n",
    "#     posterior_gibbs = kde(XY).reshape(X.shape)\n",
    "# make_plots(X,Y,prior(X,Y),lik(X,Y),posterior_gibbs)\n",
    "# make_plots(X,Y,prior(X,Y),lik(X,Y),posterior_gibbs,projection='3d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
