{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch9. Nural Network  For  XOR\n",
    "- http://pythonkim.tistory.com/33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. XOR 문제의 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p> 단일 Logistic Regression 으로는 해결할수 없음이 수학적으로 증명</p></br>\n",
    "<p> Logistic을 3개를 연결 후, forward, backward 로 해결하는 방안을 발표 </p></br>\n",
    "<img src=\"./img/lab09-1.jpg\" align=\"left\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p> Nural Network 구성도 </p></br></br>\n",
    "<img src=\"./img/lec_09_07.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p> 숫자 대입을 통한 증명 </p></br></br>\n",
    "<img src=\"./img/lec_09_05.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p> TensorFlow 를 통한 Nural Network  </p></br></br>\n",
    "<img src=\"./img/lec_09_09.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Basic Derivative (미분의 기초)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01) Derivative (미분)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p> < $ \\frac{d}{dx}f(x) $ = 함수 결과값($f(x)$) 과 $wX$ 변수간의 변화율 > </p></br>\n",
    "<p> $ f(x) = 3 (상수) $ : $ \\frac{d}{dx}f(x) = 0$ </p></br>\n",
    "<p> $ f(x) = x $ : $ \\frac{d}{dx}f(x) = 1$ </p></br>\n",
    "<p> $ f(x) = 2x $ : $ \\frac{d}{dx}f(x) = 2$ </p></br>\n",
    "\n",
    "<img src=\"./img/lec_09_10.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02) Partial derivative (편미분)\n",
    "- $ \\frac{d}{dx}$ 에서 $x$ 를 제외한 나머지 <strong>변수</strong>는 상수로 취금"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> $ f(x,y) = 2x = (x + x) $ = $ (\\frac{df}{dx} +\\frac{df}{dx}) $ = ( 1 + 1 ) = 2 </p></br>\n",
    "<p> $ f(x,y) = xy $ : $ \\frac{df}{dx}f(x) = y$ , $ \\frac{df}{dy}f(x) = x$ ,  </p></br>\n",
    "<p> $ f(x,y) = x + y $ : $ \\frac{df}{dy}f(x) = x + 0 = 1$ , $ \\frac{df}{dx}f(x) = 0 + y = 1$ ,  </p></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyphothesis & Cost/ Loss function 적용\n",
    "- http://pythonkim.tistory.com/36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p> $ H(x) = Sigmoid(z) $<t>&nbsp;&nbsp;&nbsp;</t> $ z = wX + b $ </p></br> \n",
    "<p> $ \\frac{df}{dx} = \\frac{df}{dg} * \\frac{dg}{dx} $ (각 각의 미분 곱) </p></br>\n",
    "<p> 하지만 연결갯수가 많으면, 연산시 곤란한 경우가 많다 </p>\n",
    "<img src=\"./img/lec_09_15.png\" align=\"left\">\n",
    "<img src=\"./img/lec_09_16.png\" align=\"left\" width=518>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chain Rule \n",
    "- Back Propagation 의 원할한 적용을 위해, 수식이 아닌 변수별 미분을 계산 후 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"./img/lec_09_18.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/lec_09_21.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sigmoid 함수에 적용\n",
    "g(z)를 풀어보면 위의 그림이 나오고, 말로 하면 아래처럼 된다.\n",
    "\n",
    "  1. z에 -1을 곱해서 음수로 만든다.\n",
    "  2. exp(지수)를 계산한다.\n",
    "  3. 1을 더한다.\n",
    "  4. 앞의 결과를 분모로 취한다. (1/x)\n",
    "\n",
    "\n",
    "출처: http://pythonkim.tistory.com/36 [파이쿵]\n",
    "<p></p></br></br></br>\n",
    "<img src=\"./img/lec_09_22.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Back Propagation in TensorFlow\n",
    "<p></p></br></br></br>\n",
    "<img src=\"./img/lec_09_24.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"./img/lec_09-02.jpg\" align=\"left\" width=450>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. TensorFlow Coding\n",
    "- http://pythonkim.tistory.com/37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01) Logistic regression 을 활용한  XOR ( 해결 불가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1.,  1.],\n",
       "       [ 0.,  1.,  0.,  1.],\n",
       "       [ 0.,  1.,  1.,  0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy = np.loadtxt('./data/07train.txt', unpack=True)\n",
    "x_data = xy[0:-1]  # column을 기준으로 Slicing\n",
    "y_data = xy[-1]\n",
    "xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# W 값은 x1, x2... 의 갯수에 맞워서 설정,   범위지정 ( -1.0 , 1.0 )\n",
    "W = tf.Variable(tf.random_uniform([1, len(x_data)], -1.0, 1.0)) \n",
    "h = tf.matmul(W, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hypothesis / Cost,Loss Function (Cross Entropy)\n",
    "hypothesis = tf.div(1., 1. + tf.exp(-h))\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "rate = tf.Variable(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(rate)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#init = tf.initialize_all_variables()\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# With 문 밖에서 실행하면 오류가 발생한다.. Why????\n",
    "# print('Accuracy :', accuracy.eval({X:x_data, Y:y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step : 0 Cost : 0.723344 W : [[ 0.06424129 -0.7320224 ]]\n",
      "Step : 200 Cost : 0.693983 W : [[ 0.10701776 -0.1226778 ]]\n",
      "Step : 400 Cost : 0.693214 W : [[ 0.03261442 -0.03297038]]\n",
      "Step : 600 Cost : 0.693153 W : [[ 0.00935474 -0.00936282]]\n",
      "Step : 800 Cost : 0.693148 W : [[ 0.00267076 -0.00267094]]\n",
      "--------------------------------------------------\n",
      "[[ 0.5         0.49980825  0.50019175  0.5       ]]\n",
      "[[ 1.  0.  1.  1.]]\n",
      "[[False False  True False]]\n",
      "0.25\n",
      "Accuracy : 0.25\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(1000):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(\"Step :\",step,\"Cost :\",sess.run(cost, feed_dict={X: x_data, Y: y_data}),\"W :\",sess.run(W))\n",
    "    print('-'*50)\n",
    "    # Test model --( \"hypothesis 값이 0.5 보다 크면 1을 출력 | 작으면 0을 출력 )\n",
    "    correct_prediction = tf.equal(tf.floor(hypothesis+0.5), Y)\n",
    "    # Calculate Accuraty ----------------------------------------\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "    #            [ 가설결과값   ,  가설결과를  1|0 로 변환  ,   실측값과 비교       ,    정확도  ]\n",
    "    # sess.run([hypothesis, tf.floor(hypothesis+0.5), correct_prediction, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    param = [hypothesis, tf.floor(hypothesis+0.5), correct_prediction, accuracy]\n",
    "    result = sess.run(param, feed_dict={X:x_data, Y:y_data})\n",
    "    for i in result:\n",
    "        print(i)\n",
    "    print('Accuracy :', accuracy.eval({X:x_data, Y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02) Nural Network 의 Layer를 활용한  XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"./img/lab_09_05.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hypothesis 01 ----------------------------------\n",
    "W1 = tf.Variable(tf.random_normal([2, 2]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([2]), name='bias1')\n",
    "# Hypothesis 02 ----------------------------------\n",
    "W2 = tf.Variable(tf.random_normal([2, 1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hypothesis 01 / 02 연결\n",
    "# layer1 : W1과 W2 연결링크\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Accuracy computation\n",
    "# (0, 1의 값으로 치환) True if hypothesis > 0.5 else False  (0.5를 기준 True/False)  cf) sigmoid 는 (0이 기준) \n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "# accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate Accuraty ----------------------------------------\n",
    "correct_prediction = tf.equal(tf.floor(hypothesis+0.5), Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inputing Tensor -----------------------------------------------------------\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Cost: 1.67573 \n",
      " Run (W1): \n",
      " [[ 0.45435488  1.90757763]\n",
      " [ 2.49049306 -0.31131512]] \n",
      " Run (W2): \n",
      " [[-0.9618476]\n",
      " [-0.3904123]]\n",
      "Step: 2500 Cost: 0.455303 \n",
      " Run (W1): \n",
      " [[ 3.19586134  2.77109623]\n",
      " [ 4.13764858 -3.76263928]] \n",
      " Run (W2): \n",
      " [[ 3.00797892]\n",
      " [ 3.04310775]]\n",
      "Step: 5000 Cost: 0.374748 \n",
      " Run (W1): \n",
      " [[ 4.09495497  3.72429633]\n",
      " [ 6.05614042 -5.93587446]] \n",
      " Run (W2): \n",
      " [[ 4.40109062]\n",
      " [ 4.8988018 ]]\n",
      "Step: 7500 Cost: 0.361454 \n",
      " Run (W1): \n",
      " [[ 4.41272259  4.07868767]\n",
      " [ 6.94131327 -6.8301568 ]] \n",
      " Run (W2): \n",
      " [[ 5.02457237]\n",
      " [ 5.64771986]]\n",
      "Step: 10000 Cost: 0.356487 \n",
      " Run (W1): \n",
      " [[ 4.59705591  4.2829361 ]\n",
      " [ 7.49655294 -7.37908649]] \n",
      " Run (W2): \n",
      " [[ 5.41629553]\n",
      " [ 6.10405207]]\n"
     ]
    }
   ],
   "source": [
    "# Launch graph --------------------------------------------------------------\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(10001):\n",
    "    sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 2500 == 0:\n",
    "        print(\"Step:\", step, \"Cost:\", sess.run(cost, feed_dict={\n",
    "              X: x_data, Y: y_data}), \"\\n Run (W1):\" ,\"\\n\" , sess.run(W1),\"\\n Run (W2):\" ,\"\\n\", sess.run(W2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hypothesis:  [[ 0.01188578]\n",
      " [ 0.98649675]\n",
      " [ 0.98649931]\n",
      " [ 0.02103897]] \n",
      "Correct:  [[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]] \n",
      "Check:  [[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Accuracy report\n",
    "h, c, cp, a = sess.run([hypothesis, predicted, correct_prediction, accuracy],\n",
    "                   feed_dict={X: x_data, Y: y_data})    \n",
    "print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c,\"\\nCheck: \", cp,\"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03) Nural Network 의 Layer를 활용한  XOR 2   Widely!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer 를 연결시 주의할 점\n",
    "# W1 의 [ alpha'n', Beta1 ]  W2 의 [ alpha2, Beta2] ..... 일때\n",
    "# Beta1 == alpha2 조건만 맞추면서 연결을 해야 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"./img/lec-0920.png\" align=\"left\" width=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hypothesis 01\n",
    "W1 = tf.Variable(tf.random_normal([2, 10]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([10]), name='bias1')\n",
    "# Hypothesis 02\n",
    "W2 = tf.Variable(tf.random_normal([10, 10]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name='bias2')\n",
    "# Hypothesis 03\n",
    "W3 = tf.Variable(tf.random_normal([10, 10]), name='weight3')\n",
    "b3 = tf.Variable(tf.random_normal([10]), name='bias3')\n",
    "# Hypothesis 04\n",
    "W4 = tf.Variable(tf.random_normal([10, 1]), name='weight4')\n",
    "b4 = tf.Variable(tf.random_normal([1]), name='bias4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hypothesis가 4개 연결시, 연결 Layer는 3개\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3) \n",
    "hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate Accuraty ----------------------------------------\n",
    "# accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "correct_prediction = tf.equal(tf.floor(hypothesis+0.5), Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Launch graph ----------------------------------------------------------------\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inputing Tensor -------------------------\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Cost: 0.699468 \n",
      "Run(W1): [[ 0.8255766   0.11741684 -0.37857988  0.03720431 -0.16897374  0.06273444\n",
      "  -0.45793277 -0.64736688  0.1191975   0.56422961]\n",
      " [-0.6582796  -1.44163287 -1.65199482 -0.62886155 -0.6099205  -0.79839098\n",
      "   1.81451118  1.15398788  0.79301912 -0.32405707]] \n",
      "Run(W2): [[ 0.26021865  1.08286452 -0.34417772  0.68993306  2.07101345  1.31702328\n",
      "   0.53171605  0.36783499 -1.76340747  0.49645901]\n",
      " [-1.28458571  1.67629504  0.07772854  0.69253516  0.89586341  0.84895229\n",
      "  -0.98565137 -0.71070445 -1.09179044  0.05667585]\n",
      " [ 0.55054748 -1.08216381 -1.07649922 -0.42474654 -1.28795838 -0.37219256\n",
      "  -0.96133071  0.8322953   1.07704258 -0.37373456]\n",
      " [ 0.37147674  0.4206098  -0.11292756 -0.2099696   0.06636294 -1.01685238\n",
      "  -1.59421051  0.65487713 -0.03577053  1.49644411]\n",
      " [ 0.16753438 -0.07063635  0.88377374  0.00506692 -1.29968488 -2.50376225\n",
      "   0.61387986 -0.26480421  0.14375317 -0.19888981]\n",
      " [ 0.93069553  0.17624846  0.21177645  0.25446764  1.01522756 -0.05360224\n",
      "   0.01630942  0.58889443 -1.14143276 -1.4611702 ]\n",
      " [-1.36316204  0.02127578  1.6091212   0.71934646 -1.2007314   0.16852701\n",
      "  -0.14677776 -0.56698465 -1.86039782 -0.30960527]\n",
      " [ 0.1884193  -1.71122122 -1.57751131  0.41715562  0.21650919 -0.54442954\n",
      "   0.69474912  1.24389076  1.17499495 -0.87317103]\n",
      " [ 1.03703868  1.69481254  0.62260693  0.26959035  0.90438032 -0.39223266\n",
      "   1.43511701  0.24726436  0.86593848  0.23989022]\n",
      " [-0.01405895 -0.02698975 -0.74754089 -0.09712845  1.65624452 -0.79438025\n",
      "  -0.94430971 -0.85633004  0.37167615 -1.14161015]]\n",
      "Step: 2500 Cost: 0.018359 \n",
      "Run(W1): [[ 2.04447746  1.52075052 -1.79359531  0.90512449 -0.61072445  0.63535386\n",
      "  -1.39952099 -2.9038744   0.91926587  1.60476196]\n",
      " [-2.42437148 -2.73947501 -1.19627571 -1.24754167 -0.46448848 -1.39179754\n",
      "   2.17163038  2.25647187  0.17498276 -1.39275742]] \n",
      "Run(W2): [[ 0.0244513   2.34321761 -0.34352291  1.07296658  3.02676702  0.80528158\n",
      "   0.44625011  0.24712004 -1.95026338  1.01001489]\n",
      " [-1.36340189  3.16434169  0.1001013   0.72461075  1.5434587   0.48827973\n",
      "  -1.09169376 -1.06540513 -1.28451669  0.55649143]\n",
      " [ 0.71987206 -0.720451   -1.02064919 -0.89342642 -1.79177332 -0.20168689\n",
      "  -0.97259003  0.60753417  1.04736495 -0.34471464]\n",
      " [ 0.33041528  1.22759235 -0.05642442 -0.2195086   0.05385558 -1.17355275\n",
      "  -1.7481252   0.52673554 -0.04439032  1.74921238]\n",
      " [-0.0388931  -0.06131117  1.01164603  0.17217202 -1.96450925 -2.65258694\n",
      "   0.54730207 -0.07836892  0.28909773 -0.07844616]\n",
      " [ 0.81190652  0.8689791   0.24685901  0.38390148  1.27822292 -0.31293178\n",
      "  -0.03003616  0.48249125 -1.21326613 -1.1684742 ]\n",
      " [-1.169698   -0.42254251  1.77076459  0.21685788 -2.94890189  0.62863314\n",
      "  -0.41002563 -0.35689008 -1.51397455 -0.59977704]\n",
      " [-0.16141099 -3.56834006 -1.41745043  0.88577843 -0.91523802 -0.59452051\n",
      "   0.93189865  1.9801358   1.4691329  -1.09801662]\n",
      " [ 1.11190498  1.82584703  0.64434671  0.16802569  0.70752871 -0.31325668\n",
      "   1.31728184  0.24751033  0.9242658   0.22140437]\n",
      " [-0.18118216  0.77701133 -0.74438894  0.19539367  2.24260092 -1.1342175\n",
      "  -1.03035176 -0.91605633  0.2819334  -0.81919777]]\n",
      "Step: 5000 Cost: 0.00505537 \n",
      "Run(W1): [[ 2.14793968  1.6522218  -1.88981104  0.99398178 -0.67544669  0.70804727\n",
      "  -1.53932214 -3.12079     0.98397142  1.70580053]\n",
      " [-2.58839464 -2.80159688 -1.21722746 -1.34865856 -0.45358863 -1.45878351\n",
      "   2.28228879  2.4196806   0.12865883 -1.49493003]] \n",
      "Run(W2): [[ 0.01297429  2.43656015 -0.33691722  1.16507268  3.16884565  0.74041206\n",
      "   0.44205323  0.23783147 -1.9772557   1.08218205]\n",
      " [-1.33629715  3.31662345  0.10297277  0.70433629  1.61370134  0.49472171\n",
      "  -1.13707864 -1.14126813 -1.30311501  0.59095049]\n",
      " [ 0.73015422 -0.71599507 -1.02016401 -0.94852734 -1.85829341 -0.16494063\n",
      "  -0.96866703  0.57269484  1.05364633 -0.37138307]\n",
      " [ 0.34567469  1.33216345 -0.04873089 -0.23348482  0.0716578  -1.15537572\n",
      "  -1.77831531  0.47818217 -0.05210823  1.76039827]\n",
      " [-0.05916772 -0.05800422  1.03196907  0.19228698 -2.0472331  -2.62687111\n",
      "   0.57376158 -0.07785569  0.29823112 -0.10367946]\n",
      " [ 0.81571329  0.94579506  0.25235692  0.40109935  1.32460082 -0.3226271\n",
      "  -0.04208849  0.45201218 -1.22526336 -1.14242256]\n",
      " [-1.17265415 -0.47174305  1.78866518  0.14334367 -3.1678679   0.72881258\n",
      "  -0.39727494 -0.35760295 -1.47885334 -0.69540375]\n",
      " [-0.25066334 -3.77439809 -1.39515662  1.03499699 -1.00628233 -0.64812702\n",
      "   1.06196034  2.10563493  1.48557997 -1.11808026]\n",
      " [ 1.11796343  1.85899615  0.64813626  0.16045736  0.70272541 -0.29953521\n",
      "   1.30113649  0.2373354   0.92501426  0.21821983]\n",
      " [-0.18010674  0.86948335 -0.73828775  0.24044921  2.33104873 -1.1620636\n",
      "  -1.04983652 -0.93610293  0.26474306 -0.7755577 ]]\n",
      "Step: 7500 Cost: 0.0027778 \n",
      "Run(W1): [[ 2.18904018  1.70600808 -1.92779279  1.03079331 -0.70558041  0.73595172\n",
      "  -1.59115148 -3.2080636   1.00563431  1.74354219]\n",
      " [-2.65521121 -2.82408333 -1.23089564 -1.38939857 -0.45217341 -1.48447299\n",
      "   2.33027363  2.48823166  0.11003156 -1.53491485]] \n",
      "Run(W2): [[ 0.00661857  2.47035646 -0.33403605  1.20700181  3.22517633  0.71318561\n",
      "   0.44021189  0.23507138 -1.99101424  1.1143111 ]\n",
      " [-1.32683206  3.37619472  0.1043555   0.69128782  1.63937795  0.504354\n",
      "  -1.16080511 -1.17441845 -1.31291902  0.60402209]\n",
      " [ 0.73392206 -0.71400082 -1.01995516 -0.97320008 -1.88394165 -0.14726947\n",
      "  -0.96737111  0.55715996  1.05634665 -0.38387766]\n",
      " [ 0.35090315  1.37333608 -0.04565678 -0.24060825  0.08031135 -1.1441021\n",
      "  -1.79396737  0.45678332 -0.0574354   1.76513863]\n",
      " [-0.0664373  -0.05723391  1.03997481  0.20136683 -2.07882643 -2.61320543\n",
      "   0.58503318 -0.07959332  0.30097842 -0.11605354]\n",
      " [ 0.81644237  0.97577804  0.25465795  0.40792629  1.34344816 -0.32413054\n",
      "  -0.04900307  0.43854749 -1.23197544 -1.13130248]\n",
      " [-1.17205119 -0.49075952  1.79552531  0.11118358 -3.25317192  0.77386916\n",
      "  -0.39158607 -0.35990426 -1.46343017 -0.73900688]\n",
      " [-0.28313303 -3.85482836 -1.38657141  1.10548902 -1.03904676 -0.67661178\n",
      "   1.12290728  2.15708995  1.49314296 -1.12598073]\n",
      " [ 1.12020791  1.87219584  0.6497035   0.15737987  0.70156312 -0.29290295\n",
      "   1.29335713  0.2329492   0.92465824  0.21699151]\n",
      " [-0.18057817  0.90513462 -0.73569125  0.25966999  2.36601567 -1.17179823\n",
      "  -1.05989552 -0.94473487  0.25552884 -0.75645572]]\n",
      "Step: 10000 Cost: 0.00187938 \n",
      "Run(W1): [[ 2.21410012  1.73917198 -1.95086455  1.05370665 -0.72514862  0.75257564\n",
      "  -1.62192774 -3.26138759  1.01756871  1.76586139]\n",
      " [-2.69637966 -2.83714294 -1.24087334 -1.41415632 -0.45228198 -1.49976969\n",
      "   2.36049771  2.53106689  0.098621   -1.55909693]] \n",
      "Run(W2): [[  2.22997577e-03   2.48993492e+00  -3.32228988e-01   1.23423040e+00\n",
      "    3.25929928e+00   6.95826530e-01   4.39011425e-01   2.33728498e-01\n",
      "   -2.00017953e+00   1.13499892e+00]\n",
      " [ -1.32154894e+00   3.41205215e+00   1.05269559e-01   6.81694031e-01\n",
      "    1.65431452e+00   5.12317359e-01  -1.17716181e+00  -1.19529998e+00\n",
      "   -1.31944859e+00   6.11773610e-01]\n",
      " [  7.36121356e-01  -7.12665796e-01  -1.01995516e+00  -9.89163876e-01\n",
      "   -1.89924717e+00  -1.35443822e-01  -9.66782272e-01   5.47278821e-01\n",
      "    1.05808580e+00  -3.92085969e-01]\n",
      " [  3.53742540e-01   1.39812911e+00  -4.37762626e-02  -2.45383188e-01\n",
      "    8.60198140e-02  -1.13621211e+00  -1.80472934e+00   4.43373948e-01\n",
      "   -6.12392798e-02   1.76819324e+00]\n",
      " [ -7.06344172e-02  -5.69420718e-02   1.04481852e+00   2.07261130e-01\n",
      "   -2.09753704e+00  -2.60407877e+00   5.91897726e-01  -8.09754133e-02\n",
      "    3.02511811e-01  -1.24244064e-01]\n",
      " [  8.16599071e-01   9.93766785e-01   2.56088972e-01   4.12085772e-01\n",
      "    1.35499537e+00  -3.24479133e-01  -5.39574847e-02   4.30094033e-01\n",
      "   -1.23655093e+00  -1.12426591e+00]\n",
      " [ -1.17115712e+00  -5.02051234e-01   1.79963553e+00   9.06667709e-02\n",
      "   -3.30432558e+00   8.02882135e-01  -3.88197422e-01  -3.61703575e-01\n",
      "   -1.45354223e+00  -7.67246664e-01]\n",
      " [ -3.01841438e-01  -3.90311551e+00  -1.38145268e+00   1.15180266e+00\n",
      "   -1.05796349e+00  -6.96231306e-01   1.16260302e+00   2.18880415e+00\n",
      "    1.49801791e+00  -1.13076639e+00]\n",
      " [  1.12150300e+00   1.88014507e+00   6.50676787e-01   1.55527011e-01\n",
      "    7.01070607e-01  -2.88615435e-01   1.28819311e+00   2.30260760e-01\n",
      "    9.24266934e-01   2.16288984e-01]\n",
      " [ -1.81174219e-01   9.26380932e-01  -7.34072387e-01   2.71817565e-01\n",
      "    2.38717318e+00  -1.17756188e+00  -1.06678104e+00  -9.50070560e-01\n",
      "    2.49300003e-01  -7.44233191e-01]]\n"
     ]
    }
   ],
   "source": [
    "for step in range(10001):\n",
    "    sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 2500 == 0:\n",
    "        print(\"Step:\",step, \"Cost:\", sess.run(cost, feed_dict={\n",
    "              X: x_data, Y: y_data}), \"\\nRun(W1):\",sess.run(W1),\"\\nRun(W2):\",sess.run(W2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hypothesis:  [[ 0.0018599 ]\n",
      " [ 0.99830532]\n",
      " [ 0.99800247]\n",
      " [ 0.00195833]] \n",
      "Correct:  [[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]] \n",
      "Check:  [[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Accuracy report\n",
    "h, c, cp, a = sess.run([hypothesis, predicted, correct_prediction, accuracy],\n",
    "                   feed_dict={X: x_data, Y: y_data})\n",
    "print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nCheck: \", cp, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
