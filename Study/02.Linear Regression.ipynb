{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch2. Linear Regression\n",
    "- https://github.com/hunkim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.reduce_mean() : 누적합의 평균\n",
    "t = [1., 2., 3., 4.]\n",
    "reduceMean = tf.reduce_mean(t) \n",
    "tf.Session().run(reduceMean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Build Graph Using TF operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X and Y data\n",
    "x_train = [1,2,3]\n",
    "y_train = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variable : TensorFlow 에서 연산 중 변경가능한 자료\n",
    "W = tf.Variable(tf.random_normal([1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x W + b 의 가설성립\n",
    "hypothesis = x_train * W + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cost/ Loss function (검증)\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증결과값 중에서 최솟값 찾기 (Graph 구현)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Graph를 실행\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9.51162 [ 2.05558777] [ 0.85003674]\n",
      "500 0.00134975 [ 0.95733005] [ 0.09699877]\n",
      "1000 0.000121613 [ 0.98719192] [ 0.02911587]\n",
      "1500 1.09573e-05 [ 0.99615544] [ 0.00873964]\n",
      "2000 9.87339e-07 [ 0.99884599] [ 0.00262345]\n"
     ]
    }
   ],
   "source": [
    "for step in range(2001):\n",
    "    sess.run(train)         # 1 부터 2000 까지 반복적 학습을 한다\n",
    "    if step % 500 == 0:\n",
    "        print(step, sess.run(cost), sess.run(W), sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. PlaceHolder 를 통한 기계학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01) 예제 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x_data, y_data 대신에 X, Y 를 사용\n",
    "# http://stackoverflow.com/question/36693740/\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9.87339e-07 [ 0.99884874] [ 0.00261714]\n",
      "400 1.44105e-07 [ 0.99956018] [ 0.00099975]\n",
      "800 2.1096e-08 [ 0.99983168] [ 0.00038249]\n",
      "1200 3.10091e-09 [ 0.99993557] [ 0.00014662]\n",
      "1600 4.64259e-10 [ 0.99997485] [  5.68197283e-05]\n",
      "2000 6.65494e-11 [ 0.99999052] [  2.14709853e-05]\n"
     ]
    }
   ],
   "source": [
    "# Fit the Line\n",
    "for step in range(2001):\n",
    "    cost_val, W_val, b_val, _ = \\\n",
    "        sess.run([cost, W, b, train],\n",
    "                feed_dict={X:[1,2,3], Y:[1,2,3]})\n",
    "    if step % 400 == 0:\n",
    "        print(step, cost_val, W_val, b_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02) 다차원 입력을 활용할 때 의 full Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal([1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32 ,shape=[None])  # shap=[None] : 어떤 자료형이든지 모두 입력가능\n",
    "Y = tf.placeholder(tf.float32, shape=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 가설세우기\n",
    "hypothesis = X * W + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cost/ Loss Function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimize (최솟값 찾기)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TensorFlow Graph 그리기\n",
    "sess = tf.Session()\n",
    "# Graph 실행\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### '_' 언더 스코어의 개념과 용례\n",
    "- https://mingrammer.com/underscore-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9.47347 [ 0.77342826] [-0.58431047]\n",
      "400 0.0276729 [ 1.10763538] [ 0.71140236]\n",
      "800 0.00184245 [ 1.02777314] [ 0.99972999]\n",
      "1200 0.000122672 [ 1.00716627] [ 1.07412708]\n",
      "1600 8.16749e-06 [ 1.00184917] [ 1.09332371]\n",
      "2000 5.44337e-07 [ 1.00047743] [ 1.09827626]\n"
     ]
    }
   ],
   "source": [
    "# Graph 에 맞는 데이터를 학습한다\n",
    "for step in range(2001):\n",
    "    cost_val ,W_val, b_val, _ = sess.run([cost, W, b, train],\n",
    "        feed_dict = { X : [1,2,3,4,5],\n",
    "                     Y : [2.1, 3.1, 4.1, 5.1, 6.1]})\n",
    "    if step % 400 == 0:\n",
    "        print(step, cost_val, W_val, b_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Cost 가 최소가 되는 Liner Regression 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01) Liner Regression 이 최소가 되는 값 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bias 를 제외한 간결한 형태로 실험을 시작\n",
    "W = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = [1,2,3]\n",
    "Y = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 가설\n",
    "hypothesis = X * W   # H(x) = W x\n",
    "# 검증\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Graph 를 그리고 실행\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variable for plotting cost function\n",
    "W_val = []\n",
    "cost_val = []\n",
    "for i in range(-30,50):  # -3 ~ 5 까지 0.1 단위로 값을 대입해본다.\n",
    "    feed_W = i * 0.1\n",
    "    curr_cost, curr_W = sess.run([cost,W], feed_dict={W: feed_W})\n",
    "    W_val.append(curr_W)\n",
    "    cost_val.append(curr_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "# show the Cost function\n",
    "plt.plot(W_val ,cost_val)\n",
    "#plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02) Gradient Descent\n",
    "- 입력값 위치에 따른 최솟값을 찾는 전략"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x_data = [1,2,3]\n",
    "y_data = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal([1]), name = 'weight')\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 가설 세우기\n",
    "hypothesis = X * W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cost/ Loss function\n",
    "cost = tf.reduce_sum(tf.square(hypothesis - Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimize : Gradient Descentg using derivative \n",
    "# (비용을 최소화 하는 공식을 직접구현)\n",
    "# : W -= Learning_rate * derivative\n",
    "learning_rate = 0.1\n",
    "gradient = tf.reduce_mean((W * X-Y) *X)\n",
    "descent = W - learning_rate * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 출처 : https://codeonweb.com/entry/5f15bf8e-d704-49e0-909a-db4450433b74\n",
    "# .assign() : W 값을 업데이트 ( 새롭게 갱신 )\n",
    "#            표현 그래프(expression graph)의 일종\n",
    "#            run()으로 그 표현을 실행하기 전에는 실제 대입 연산을 수행하지 않습니다\n",
    "update = W.assign(descent)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Launch the Graph in a Session.\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0  ,  Cost: 12.2579  ,  W: [ 0.06428468]\n",
      "Step: 4  ,  Cost: 0.0802426  ,  W: [ 0.92429256]\n",
      "Step: 8  ,  Cost: 0.000525284  ,  W: [ 0.99387461]\n",
      "Step: 12  ,  Cost: 3.43867e-06  ,  W: [ 0.99950439]\n",
      "Step: 16  ,  Cost: 2.24609e-08  ,  W: [ 0.99995995]\n",
      "Step: 20  ,  Cost: 1.49289e-10  ,  W: [ 0.99999672]\n"
     ]
    }
   ],
   "source": [
    "# Initialize Global Variables in the Graph (변수 중 최적화한 값을 추출)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(21):\n",
    "    sess.run(update, feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 4 == 0:\n",
    "        print (\"Step:\", step ,\" , \",\n",
    "               \"Cost:\", sess.run(cost, feed_dict={X:x_data, Y:y_data}),\" , \",\n",
    "               \"W:\", sess.run(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03) Minimize Gradient Descent\n",
    "- 미분 없이 최솟값 찾는 전략\n",
    "- 초기값으로 전혀 무관한 값을 입력시 실행경과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 01> 초기값을 5 에서 시작시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "X = [1,2,3]\n",
    "Y = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 초기값으로 전혀 상관없는 값을 입력\n",
    "W = tf.Variable(5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 가설성립 : 선형모델\n",
    "hypothesis = X * W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 비용함수\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimize : Gradient Descentg using derivative \n",
    "#learning_rate = 0.1\n",
    "#gradient = tf.reduce_mean((W * X-Y) *X)\n",
    "#descent = W - learning_rate * gradient\n",
    "#update = W.assign(descent)  # .assign() : W 값을 업데이트 한다( 새롭게 갱신 )6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf.train.GradientDescentOptimizer() : 최적화 함수 간략\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.0\n",
      "10 1.26667\n",
      "20 1.01778\n",
      "30 1.00119\n",
      "40 1.00008\n",
      "50 1.00001\n",
      "60 1.0\n",
      "70 1.0\n",
      "80 1.0\n",
      "90 1.0\n"
     ]
    }
   ],
   "source": [
    "# 그래프를 실행\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(100):\n",
    "    if step % 10 == 0:\n",
    "        print(step ,sess.run(W))\n",
    "        sess.run(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02> 초기값을 -3 에서 시작시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 초기값으로 전혀 상관없는 값을 입력\n",
    "W = tf.Variable(-3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TensorFlow 실행 후, Graph는 메모리에서 삭제 (iterable)\n",
    "# 가설성립 : 선형모델\n",
    "hypothesis = X * W\n",
    "# 비용함수\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 최적화 함수 (그래프를 그린다) : Gradient Descent Magic\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프를 실행\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(100):\n",
    "    if step % 10 == 0:\n",
    "        print(step ,sess.run(W))\n",
    "        sess.run(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Gradient 의 값을 임으로 조정하고 싶을 때 (고급옵션)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "# tf Graph Input\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "W = tf.Variable(5.) # 초기값 지정\n",
    "hypothesis = X * W  # 가설성립"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient 01 (기울기를 미분으로 계산)\n",
    "gradient = tf.reduce_mean((W * X - Y) * X) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(hypothesis - Y)) # 비용함수\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01) # Minimize: Gradient Descent Magic\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient 02 ( optimizer.compute_gradients( Cost ) : Cost 최적 Gradient(기울기)을 연산 )\n",
    "gvs = optimizer.compute_gradients(cost)\n",
    "# Apply gradients\n",
    "apply_gradients = optimizer.apply_gradients(gvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [37.333332, 5.0, [(37.333336, 5.0)]]\n",
      "1 [33.848888, 4.6266665, [(33.848888, 4.6266665)]]\n",
      "2 [30.689657, 4.2881775, [(30.689657, 4.2881775)]]\n",
      "3 [27.825287, 3.9812808, [(27.825287, 3.9812808)]]\n",
      "4 [25.228262, 3.703028, [(25.228264, 3.703028)]]\n",
      "5 [22.873621, 3.4507453, [(22.873623, 3.4507453)]]\n",
      "6 [20.738752, 3.2220092, [(20.738752, 3.2220092)]]\n",
      "7 [18.803137, 3.0146217, [(18.803137, 3.0146217)]]\n",
      "8 [17.048176, 2.8265903, [(17.048176, 2.8265903)]]\n",
      "9 [15.457013, 2.6561086, [(15.457014, 2.6561086)]]\n",
      "10 [14.014359, 2.5015385, [(14.01436, 2.5015385)]]\n",
      "11 [12.706352, 2.3613949, [(12.706352, 2.3613949)]]\n",
      "12 [11.520427, 2.2343314, [(11.520427, 2.2343314)]]\n",
      "13 [10.445186, 2.119127, [(10.445186, 2.119127)]]\n",
      "14 [9.4703016, 2.0146751, [(9.4703016, 2.0146751)]]\n",
      "15 [8.5864067, 1.9199722, [(8.5864067, 1.9199722)]]\n",
      "16 [7.7850089, 1.8341081, [(7.7850089, 1.8341081)]]\n",
      "17 [7.0584083, 1.756258, [(7.0584083, 1.756258)]]\n",
      "18 [6.3996239, 1.685674, [(6.3996239, 1.685674)]]\n",
      "19 [5.8023257, 1.6216778, [(5.8023257, 1.6216778)]]\n",
      "20 [5.260776, 1.5636545, [(5.260776, 1.5636545)]]\n",
      "21 [4.7697697, 1.5110468, [(4.7697697, 1.5110468)]]\n",
      "22 [4.3245912, 1.4633491, [(4.3245912, 1.4633491)]]\n",
      "23 [3.9209633, 1.4201032, [(3.9209635, 1.4201032)]]\n",
      "24 [3.5550067, 1.3808936, [(3.5550067, 1.3808936)]]\n",
      "25 [3.2232056, 1.3453435, [(3.2232056, 1.3453435)]]\n",
      "26 [2.9223735, 1.3131114, [(2.9223738, 1.3131114)]]\n",
      "27 [2.6496189, 1.2838877, [(2.6496186, 1.2838877)]]\n",
      "28 [2.4023216, 1.2573916, [(2.4023218, 1.2573916)]]\n",
      "29 [2.1781051, 1.2333684, [(2.1781051, 1.2333684)]]\n",
      "30 [1.9748148, 1.2115873, [(1.9748147, 1.2115873)]]\n",
      "31 [1.7904993, 1.1918392, [(1.7904994, 1.1918392)]]\n",
      "32 [1.623386, 1.1739342, [(1.6233861, 1.1739342)]]\n",
      "33 [1.4718695, 1.1577003, [(1.4718695, 1.1577003)]]\n",
      "34 [1.3344955, 1.1429816, [(1.3344957, 1.1429816)]]\n",
      "35 [1.2099417, 1.1296366, [(1.2099419, 1.1296366)]]\n",
      "36 [1.0970144, 1.1175373, [(1.0970144, 1.1175373)]]\n",
      "37 [0.9946267, 1.1065671, [(0.9946267, 1.1065671)]]\n",
      "38 [0.90179497, 1.0966209, [(0.90179503, 1.0966209)]]\n",
      "39 [0.81762749, 1.087603, [(0.81762755, 1.087603)]]\n",
      "40 [0.74131513, 1.0794266, [(0.74131513, 1.0794266)]]\n",
      "41 [0.67212623, 1.0720135, [(0.67212629, 1.0720135)]]\n",
      "42 [0.60939401, 1.0652922, [(0.60939401, 1.0652922)]]\n",
      "43 [0.55251688, 1.0591983, [(0.55251688, 1.0591983)]]\n",
      "44 [0.50094914, 1.0536731, [(0.50094914, 1.0536731)]]\n",
      "45 [0.45419374, 1.0486636, [(0.45419377, 1.0486636)]]\n",
      "46 [0.41180158, 1.0441216, [(0.41180158, 1.0441216)]]\n",
      "47 [0.37336722, 1.0400037, [(0.37336725, 1.0400037)]]\n",
      "48 [0.33851996, 1.03627, [(0.33851999, 1.03627)]]\n",
      "49 [0.30692515, 1.0328848, [(0.30692515, 1.0328848)]]\n",
      "50 [0.27827826, 1.0298156, [(0.27827829, 1.0298156)]]\n",
      "51 [0.25230527, 1.0270327, [(0.25230527, 1.0270327)]]\n",
      "52 [0.2287569, 1.0245097, [(0.2287569, 1.0245097)]]\n",
      "53 [0.20740573, 1.022222, [(0.20740573, 1.022222)]]\n",
      "54 [0.18804836, 1.020148, [(0.18804836, 1.020148)]]\n",
      "55 [0.17049654, 1.0182675, [(0.17049655, 1.0182675)]]\n",
      "56 [0.15458433, 1.0165626, [(0.15458435, 1.0165626)]]\n",
      "57 [0.14015675, 1.0150168, [(0.14015675, 1.0150168)]]\n",
      "58 [0.12707591, 1.0136153, [(0.12707591, 1.0136153)]]\n",
      "59 [0.11521538, 1.0123445, [(0.11521538, 1.0123445)]]\n",
      "60 [0.10446167, 1.0111923, [(0.10446167, 1.0111923)]]\n",
      "61 [0.094712019, 1.0101477, [(0.094712019, 1.0101477)]]\n",
      "62 [0.085872017, 1.0092006, [(0.085872017, 1.0092006)]]\n",
      "63 [0.077858053, 1.0083419, [(0.077858053, 1.0083419)]]\n",
      "64 [0.070591293, 1.0075634, [(0.070591293, 1.0075634)]]\n",
      "65 [0.064002357, 1.0068574, [(0.064002357, 1.0068574)]]\n",
      "66 [0.05802846, 1.0062174, [(0.05802846, 1.0062174)]]\n",
      "67 [0.052612226, 1.005637, [(0.052612226, 1.005637)]]\n",
      "68 [0.047702473, 1.005111, [(0.047702473, 1.005111)]]\n",
      "69 [0.043249767, 1.0046339, [(0.043249767, 1.0046339)]]\n",
      "70 [0.039213181, 1.0042014, [(0.039213181, 1.0042014)]]\n",
      "71 [0.035553534, 1.0038093, [(0.035553537, 1.0038093)]]\n",
      "72 [0.032236177, 1.0034539, [(0.032236181, 1.0034539)]]\n",
      "73 [0.029227654, 1.0031315, [(0.029227655, 1.0031315)]]\n",
      "74 [0.02649951, 1.0028392, [(0.02649951, 1.0028392)]]\n",
      "75 [0.024025917, 1.0025742, [(0.024025917, 1.0025742)]]\n",
      "76 [0.021783749, 1.002334, [(0.021783751, 1.002334)]]\n",
      "77 [0.01975123, 1.0021162, [(0.019751232, 1.0021162)]]\n",
      "78 [0.017907381, 1.0019187, [(0.017907381, 1.0019187)]]\n",
      "79 [0.016236702, 1.0017396, [(0.016236704, 1.0017396)]]\n",
      "80 [0.014720838, 1.0015773, [(0.014720838, 1.0015773)]]\n",
      "81 [0.01334699, 1.00143, [(0.013346991, 1.00143)]]\n",
      "82 [0.012100856, 1.0012965, [(0.012100856, 1.0012965)]]\n",
      "83 [0.010971785, 1.0011755, [(0.010971785, 1.0011755)]]\n",
      "84 [0.0099481745, 1.0010659, [(0.0099481754, 1.0010659)]]\n",
      "85 [0.009018898, 1.0009663, [(0.009018898, 1.0009663)]]\n",
      "86 [0.0081768828, 1.0008761, [(0.0081768837, 1.0008761)]]\n",
      "87 [0.0074131489, 1.0007943, [(0.0074131489, 1.0007943)]]\n",
      "88 [0.0067215762, 1.0007201, [(0.0067215762, 1.0007201)]]\n",
      "89 [0.0060940585, 1.0006529, [(0.0060940585, 1.0006529)]]\n",
      "90 [0.0055252709, 1.000592, [(0.0055252714, 1.000592)]]\n",
      "91 [0.0050098896, 1.0005368, [(0.0050098896, 1.0005368)]]\n",
      "92 [0.0045425892, 1.0004867, [(0.0045425892, 1.0004867)]]\n",
      "93 [0.0041189194, 1.0004413, [(0.0041189194, 1.0004413)]]\n",
      "94 [0.0037339528, 1.0004001, [(0.003733953, 1.0004001)]]\n",
      "95 [0.0033854644, 1.0003628, [(0.0033854644, 1.0003628)]]\n",
      "96 [0.0030694802, 1.0003289, [(0.0030694804, 1.0003289)]]\n",
      "97 [0.0027837753, 1.0002983, [(0.0027837753, 1.0002983)]]\n",
      "98 [0.0025234222, 1.0002704, [(0.0025234222, 1.0002704)]]\n",
      "99 [0.0022875469, 1.0002451, [(0.0022875469, 1.0002451)]]\n"
     ]
    }
   ],
   "source": [
    "# Optional: modify gradient if necessary\n",
    "# gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "sess = tf.Session()   # Launch the graph in a session.\n",
    "sess.run(tf.global_variables_initializer())  # Initializes global variables in the graph.\n",
    "for step in range(100):\n",
    "    print(step, sess.run([gradient, W, gvs]))\n",
    "    sess.run(apply_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
