{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 - Parsing\n",
    "파싱-훈련 데이터 분석하기\n",
    "\n",
    "http://www.nltk.org/howto/parse.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 구문분석 : 'formal grammer'를 기준으로 문법검사\n",
    "# 1 Treebak의 문맥 자유문법 규칙 추출\n",
    "# 2 CFG에서 확률적 문맥 자유 문법을 생성 (CFG(문맥 자유 문법) : Context Free Grammer)\n",
    "# 3 CYK 차트 파싱 알고리즘\n",
    "# 4 Earley 차트 파싱 알고리즘  ('차트 파싱'은 동적 프로그래밍 접근방식을 따른다)\n",
    "# 5 '구문구조', '담화구조', '형태론적 계도'등 기준을 갖고서 파싱한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 1 Parsing\n",
    "형식문법(formal Grammer)으로 정의된 규칙에 부합여부를 찾는 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 Tree bank corpus\n",
    "트리뱅크 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BracketParseCorpusReader in '.../corpora/treebank/combined' (not loaded yet)>\n",
      "['wsj_0001.mrg', 'wsj_0051.mrg', 'wsj_0101.mrg', 'wsj_0151.mrg']\n"
     ]
    }
   ],
   "source": [
    "# corpus가 저장된 파일의 목록 살펴보기\n",
    "import nltk,  nltk.corpus\n",
    "print(str(nltk.corpus.treebank).replace('\\\\\\\\','/'))\n",
    "print(nltk.corpus.treebank.fileids()[::50])   # .fileids() : 파일에 대한 식별자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['McDermott', 'International', 'Inc.', 'said', '0', ...]\n",
      "[('McDermott', 'NNP'), ('International', 'NNP'), ...]\n"
     ]
    }
   ],
   "source": [
    "# 개별 파일의 corpus 살펴보기\n",
    "from nltk.corpus import treebank\n",
    "print(treebank.words('wsj_0007.mrg'))        # 단어 확인\n",
    "print(treebank.tagged_words('wsj_0007.mrg')) # 단어와 Tagging 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP-SBJ\n",
      "    (NP (NNP Bailey) (NNP Controls))\n",
      "    (, ,)\n",
      "    (VP\n",
      "      (VBN based)\n",
      "      (NP (-NONE- *))\n",
      "      (PP-LOC-CLR\n",
      "        (IN in)\n",
      "        (NP (NP (NNP Wickliffe)) (, ,) (NP (NNP Ohio)))))\n",
      "    (, ,))\n",
      "  (VP\n",
      "    (VBZ makes)\n",
      "    (NP\n",
      "      (JJ computerized)\n",
      "      (JJ industrial)\n",
      "      (NNS controls)\n",
      "      (NNS systems)))\n",
      "  (. .))\n"
     ]
    }
   ],
   "source": [
    "# Penn Treebak corpus의 코드내용 살펴보기\n",
    "# corpus 구조도 살펴보기\n",
    "# import nltk\n",
    "# from nltk.corpus import treebank\n",
    "print(treebank.parsed_sents('wsj_0007.mrg')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Mr./NNP Vinken/NNP)\n",
      "  is/VBZ\n",
      "  (NP chairman/NN)\n",
      "  of/IN\n",
      "  (NP Elsevier/NNP N.V./NNP)\n",
      "  ,/,\n",
      "  (NP the/DT Dutch/NNP publishing/VBG group/NN)\n",
      "  ./.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markbaum/Python/python36/lib/python3.6/site-packages/nltk/tokenize/regexp.py:123: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return [tok for tok in self._regexp.split(text) if tok]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9oAAABtCAIAAADRUo5AAAAACXBIWXMAAA3XAAAN1wFCKJt4AAAAHXRFWHRTb2Z0d2FyZQBHUEwgR2hvc3RzY3JpcHQgOS4xOeMCIOUAAB3KSURBVHic7d1PbBvXnQfwJ0uxQ8nremRQaVW0koZIFysdFhtKOQlrAaZSJIVuJo8OejB1yKF7iDU6LBAfyTqXBEgBcg8FfCSLxQJtU6CcFgrWuUQaA4tdErvpckRtg9gRG46TWIycxNYefvXDy8xwOCKHnD/8fg7CiBxy3rx58/ibN79Hjp2cnDAAAAAAAPDDGb8LAAAAAAAwuhCOAwAAAAD4BuE4AAAAAIBvEI4DAAAAAPgG4TgAAAAAgG8QjgMAAAAA+AbhOAAAAACAbxCOAwAAAAD4ZsLvAgAARF+5XN7b28tkMpIkybLsd3EAACBAMDoOADBY+XxeluV8Pq9pWqFQ8Ls4AAAQLBgdBwAYrFarlUwmGWPZbFZVVb+LAwAAwTJ2cnLidxkAAKJM0zRVVVutViaTobgcAACAQzgOADAkqqpqmqYoit8FAQCAAEHuOADAYOXzeVpIpVKtVsvfwgAAQNAgdxwAYLAqlQpjLJlMViqVTCbjd3EAACBYkKwCADBwhmFompZKpfwuCAAABA7CcQAAAAAA3yB3HAAAAADANwjHAQAAAAB8g3AcAAAAAMA3CMcBAAAAAHyDcBwAAAAAwDf43nEAAA9ojYZxdETLdw8O7j148OH9+0a73To6an7xxeMnTx4+enTy5Mm5Z565dP781Llz58+dO//ss89duEAveWFuLjk/b3rP1NLSMHcBAAB8gS86BABgjDH98FBvNvm/Rru9t7/P/z3++ut///BDWv78+Pibx4/vf/bZo2++GXIhz5879/xzz9Hy9PnztCBNTsrxuGnNxMyM9cHk/Lw0NTXoQgIAwKkgHAeAiDCOjrRGQ3ykUq2K/+rNptFu83+1RuOB8G+fxhgbP3PmmydP6N+zExPPz8wsff/7fzc7u/r88zTOrTUaqVu35Hj8nzc2vnnyZG9/X282/+ujj/77/n3+PhcnJ8+Oj0tTU4+fPDlhbOrs2SeM/en+fefQn4LsL778khfAvSuLi+Z3s4vvp8+fT87NmR6U43F5Zua0WwQAABHCcQAIEPXbAbR2cNB6+JD/a4qn9WZzXxjPdvDchQsnJydfPw1VeVaJlTQ5+fjJk/HxcWtoe/7cuYV4fOLMmQux2IN2++jRo/89POTPXllclONxaXJyfWnJYRCaR+TqjRviOjQ2T9cP2sGBeKlwcXIyOT9PofD60tKEEPTrzWb9aRmMdpuP7netmecuXHj2mWf4Mi188vnnF2KxiTN/nVN0IRb7v1bLZQ2LXpibs+6+NZS3je+lqSlr0g4AQLQhHAcAL4kp1MwST4shI2PMODq6e3Dg5m1/eOnSpacR3pdff312YuLZiQnG2Ceff84Ye2Z8/OGjR/c/+6zTy//hhz9kjH1+fDxx5sx3YjF61V8ePjx69Mi0Jg0VU2DNGLt0/vy9Bw/aX31VbzbFEPmFuTk5Hpfj8eWFheTc3KlGiDtF5FZqtUoBt95s6s2mWFe8AJSU4pBlLibhmDJwtKdv2PVALMTjfLz8b5599uz4OC3PXrz4vYsXaacuTk5enJz81p5a3tP9Ee+0dc4ayjPG1i31gPgeAIIP4TgAfIspnhbHX/+6wrfDqT/Uam7e1hRRUTrEg3abB7gXYrHPv/ySlv/0yScXYrGJ8XGHfBIaM6bl5Nxc4y9/YYw9aLcpIvzTJ5/Ezp6tffyx9eWmgJsCOEq60BoNvdmkHBLt4IAPDFPhk3NziZmZ5Px8/+Gd+4jcxP0ges9pJGLOT6fwnXU77uLhFlNfxBFxHiibsvb5tsQLOWsBXJbElth4ON4kRMsLC9YHMcUWALyFcBwgapynJDJLTOMyhdoawZiGJ3ng8uH9+w8fPfred77z7NmzfNPiuLhD/CSGcfz9aQCYRlXPTkzce/CA74VtVoZDwC2uxkNbo93WGg0+asvjWoodBzT9seeI3IoPomsHB6bhZ/eD6H0WgC+L+fpiS3NuZmLrMmWuiwPeLstvnUXA7C4smSX9iXgV32OKLQC4hHAcIHAGNyXRNGnPFE+bAgVT/MpDLjGscZntwJOJxQBFHHekMIs2wd/fIeCmN+wacIuoVmnMVTs4EEMuMe17mHMTPYzIre9sHB3RlYb+7TQbPtJPFxu+zMUU78CIo+Biw+7aqsXGLLZkMXz3MMw1zWpgdhe6zJKORXqbNIwptgCjA+E4wEAMaEqiaZKc6fa6KZ7umjUrxv1iCXmQ7VAwcThQLAYPhmwDgtMG3Dz+oNj9VHnA4pixuAkx7VuOx/1NLB5cRG5Cx7rTIPqVxUWqaqrnoCVjiDd8ep69Kp47YvwqXhYOc5TaGt8zy4U3sabouO8xRJhiCxBYCMcB7A1oSqIphdoUT1s/CHsIjMSS80/3fnJFxFJ1jVdo63zgkC48BhRwm7bL077FWY8882Sg2Rr9GFpEbrvpIA+i98zz2avieSpe9AaqWkxdFnEZ32OKLYC/EI5DZA1zSqK4gumDx6vxNlUIrHl4wUfZe8gVEaOKUwWptgG3bQG8DbhFYtq33myaMk8GnfbtOR8jchNTSo/pOkocRPf9xoKHhj97NRQwxRZgaBCOQ3D5PiWRDG4ArJ9cESakltrmivTzwe8+4OYhCNWhhwG3iVqt8hjR9IWDyfn54ad9ey44EbkVv/NgvfgxDaKH5fqnf0GbvRoKmGIL0AnCcRigYE5JHDTxKsI2V8T9l/fRgji65tW+9Bxw87odaJSgNRpao2FN+16IxylxImKjsyTIEblJ10F0ukSM5GHqTRhnr4YCpthCNCAchy5CMSVxCPinqW2uCHMcubGdQ9ZzrohL/KqALgn4p5G1nL4E3KaiagcHYUz79lyIInIrDKJ7KJKzV0MBU2zBFwjHoy+8UxIHTRy8t/3yPpe5IrZf3jeEXjJEAbeIqr1T2jd9Q/YoR2yhjshNqIl2HURfX1pCVNGn0Zy9GgqYYgtuIBwPgYhNSRw08WPJNiH7tLkirNuX9w2O+4Cbl5yKzS+HgnAVJH7hoO3vzIc97dtzUYrIrdRqleJFurkkNmYaJuT3Q8LS54QRZq+GF6bYRhLC8WGI/JTEIbDNFen/hx7/+qB/n/r8Q7HngDtoIQvPWLCmfVO6AvKJ3Yh2RG7iMIhOzR6D6EGA2atRhSm2QYBw3JXRnJI4BEP4oUff8cbDRy9oB62NJCwBt4h/4WCntG/ai4Aci3AZqYjcShxEF5sWwyB6SGD26ijDFNvTGqFwHFMSh2P4P/Tou2gH3CL+fRr1w0PbtG8awgzL7gTfiEfkJvzajzFmSnzil3+MMeQ+hRRmr4KDyE+xjXg4nrp1q+ttlAhMSQyITrXd/w89BtnYT39qeoSu0XlDomuJCMQHarW6/uabtCz+znxybi7suxZkFJFnL1/OZzJ+lyWI+MwE0yD6lcVF9cYNf8sGA9X/7NXK66/jA31kDXOK7dbLL3ftwCMejhd3duqHhyGdkhg6xZ0dxthAv7wvgJRSKTIBtzPj6Ki8u5ucn4/8HZ6g0Q8Po920vEVRmtFup1dW/C4LBIh19urm2hrOLPCKwxTb9aWlrhFRxMNxAAAAAIAgO+N3AQAAAAAARhfCcQAAAAAA3yAcBwAAAADwDcJxAAAAAADfIBwHAAAAAPDNhN8F8IymaYZhpFIpxpiqqoyxZDJpGIau67SCLMuyLPtZxFBRFCWfz5/qJVTtVM90OCRJkiSJHwJJkpLJJC3TCrScTCYlSfKu7INibWOXLl369NNP6dnINDDDMHK5HC13agPi4ROPuOkEDMVhDZoRaWYAw4TTCoLuJEKuXr1aqVRoeWtrixbokXq9nsvl+LPQVb1e7+ElhUKB/2s6BK1Wq1AolEol07O5XK6HbfnF2sai18ByuVyr1XKzpml/bU9A6MEoNDOAIcNpBUEWqWQVWZYrlUqnpxRF4cO04EBVVUVRNE0TH9Q0TVGUcrlsfYqTZZmPmBqGkUgkxGclScpms/wQLC8vM8Z0XZckKUTDEp3aWBgbGB3TfD6vKAo/cMVisVKp5HI5RVFoDMk9hxMQTiVKzQwgIHBaQZBFKhxnjG1ubtreXjcMI5/Phyjs81Eqlcrn86Y0g1KptL29nU6nNzc3HV7LU1PK5XI6naYHdV1XFEVRlHQ6zR+khUKhkM1mB7IbA2PbxsLYwEqlEsXi29vbPDslm80mk8l8Pp/P5+nG7ql0OgHhtCLTzACCA6cVBFZ0cscJnVHiZS7FgolEIp1O43zr2fb2drFYbLVa09PTDgF0NpulAVdKHKcHZVmml1BSMu8NKRCkx0OUZGxqYyFtYKqqrq+v07KHlW89AaE30WhmAIGC0woCK2rhOGOMBmL5vzwWhH5Qt8UsIbUtTdP4lE2RJEnT09O0rKrq8vIyBYKapvUwEOsjsY2FtIElk8lcLjeIajedgNCzCDSzIaAq6npPxuVqEHk4rWBADMNwGczYrjl+8+bNQRVtuFRVLZfLx8fHyWRybGxMVdVMJqOqaqlUunPnzv7+vm2ACFaUNKxpmlhvb7311v7+vmEYt2/fXl9fdxhImJ6eVhSFtyt+CFRVVVU1Foutrq4yxnK5nGEY9ODY2Fgojo61jU1PT4e0gcVisf39fVVVj4+PC4VCJpOZnZ1ljBWLRVVVq9WqqqqJRKLTwHmxWCyVStRIxsbGZFm2PQGHu08REaVmNgQvvfTSzZs3uw5tulwNogqnFQza22+//eqrr25sbNCH6WnXHDs5ORlwCSEi3F/5QVjgmEKo0dV+sVj0ZDUAgH50Sg1wsybCcQAACKVyuZxMJruOebtcDQDALwjHAQAAAAB8E7UvOgQAAAAACBGE4wAAAAAAvkE4DgAAAADgG4TjAAAAAAC+ieDPAHFqtcoYSy0t+V0QAIDeqdWq0W6nV1b8LghAFOiHh//x5z//q6ZNnj2bXlmR43F5ZsbvQsGoi+Y3qxR3dnLvvrvfbDLGLk5OZi9f3lxbw/kGnlNKpcTMTHZtze+CQARpjUZhZ6e8u/ug3WaMLcTj6eVldGUA7tGoXKVaZYxpBwf/c//+R62W7ZoL8bgcj8vxuDQ5mZiZoWWcazA0kQrH9cPDws5O8b33HrTbVxYXN9fW5Hi8sLPzL++9xxi7fvlyZmUFg+XgodStW9LkZPm11/wuCESHcXRU3t0t7OzcPTig0YTMiy8aR0el3V3qyq4uL2defBGD5QCccXSkNRp6s1k/PDTabb3Z1BoNuo4lz1240P7qqy+Ojxlj//ijH/39D37wTy+9pDeb7/zxj/929y5j7G+/+11pamrq3DnTCxGmw3BEJBxXq1X+WXX98uXNtbXk/Dx/1jg6yv32t+W9vf1m84W5uc21NQxngidSt24xxtQbN/wuCESBOBx+ZXExs7Ji6qmMo6Pie+8Vdnb2m00MlsNo0g8P9WZTOzhoPXyoHRwwxv5Qq4krXFlclCYn5Xj8Qbv9kWH850cf/bnVujg5mVpcXF9aSq+sSFNT4vq2p5XRbhtHR7QVvdk02m2E6TBQoQ/Hizs7pd3dP9RqdBZt/+QnpjPNdmVksIAnEI5D/6zD4V27JnEAwjZwB4gAU6qJ3mxSDioRA+LlhQVpcjK1tERnU6VaVWu1B+32QjzOo3A3m+t6D0prNBCmwyCENRzvZ8CbhqCQwQL9QzgO/egzqu4hjgcIoK6pJi/MzUlTU8m5OcbY+tKSNDUl3gBnjOmHh+W9vb39/V/t7dH6qcXFzIsvmlZzWZge7kEhTIc+hS8c1w8Pc+++238wjQwW6F/6nXeMdhvhOJyK5zknXbNcAALCfaqJm5hVazRKH3yg1mp3Dw4YYy/MzWVefDG9vOxJmOtmsLwrhOngUpjCcRoH8jzVBBks0DOlVNIODhCOg0uefMB3YjsHtIfRQQBP9JBq4vKdy7u7e/v7NJrGGLu6vGybFO6JAU3YQJgOJiEIx00nw/YrrwzirEMGC/QA4Ti4QXfShzYFUxwsp1t/A4pUAJgXqSYut1Le3d1rNKhhX5ycTK+suEwK98RAr6U5hOkjK9DhuPjFhVeXlzfX1gYdIiODBU5FKZXUWk27edPvgkBAlXd3Sx98QPmsw/+CQvHWX3plxfSVUwCn5W2qicstqrVapVqlk4iuZn287ePXtxshTI+8gIbjarVa2Nn51d4efYpsv/LKkFsVMljADaVU+vnvfnfyy1/6XRAIFhpKoAv7hXh8c20te/myX+PT4rgGBsvBpcGlmrihNRpqrVb64INBJIV7YjiD5V0hTI+MwIXj/Ac1ff8MY8hggW4QjoOJOBwetE7DNFgeqLKBX4aTauKSWq1WqlVTUnhqcTGw4WNgfwoAYXroBCUct/6gZnB+cw4ZLNAJwnEggRoOdxaiooK3hp9q4hJ9U7gpKTy1uBiiZhmQwfKuEKYHlv/huPMPagYKMljABOE4hHfIOcgD+dAnf1NN3OC/1yMmha8vLYW6EQZ2sLwrhOm+8zMcP9UPagYHMliAK+/uZn7xC4TjIygyCdkYLA+1QKWauEHfMiQmhff8ez1BFpbB8q4Qpg+ND+F4NHI/orEX0Ce1Wl1/802E4yMlql9X4u+XwEBXgU01cYN+r0dMCl9eWAjU1MxBCO9geVcI0z031HDcqx/UDBRksIwyhOOjY0S+zHvIX5EOtoKfauKSNSl8eX4+kieOs8gMlneFML1nQwrHB/SDmsGBDJbRROH43htvRGN8FKxG9qcuRyeA8FHoUk3c4Enhaq32oN2mi7rlhQW0nwgPlneFML2rwYbjw/lBzeBABsuooXC88vrruPqKHnE4/MriYmZlZQRP51EOILwV6lQTN+i+SqVapf2KalK4J3CtyyFM5wYYjmuNRurWraH9oGagiBks2htvRLLpAEM4Hl3pd96hnyGL5N28HogBxNbLL+czGb9LFA5KqcQTpkmIUk3cS926RVH4lcXF9aWlyCeFe8J0rav//Od+lyhAXIbp6o0bPhbSW4MdHVdKpVH+JKPJK/jcAgid4s4OY2wEh8OdUR6CHI9HIIIcDqVUooWwpJr0RimVEjMz0b77PThqtao3m+ht3DCF6eXXXvO7RJ7x/3vHAQAAAABG1hm/CwAAAAAAMLoQjgMAAAAA+AbhOAAAAACAbxCOAwAAAAD4BuE4AAAAAIBvJmwf1TSNMZZMJulfVVVlWZZluefNaJpmGEYqlaJ3ozfXdd36oGEYuq7Tq/rcqDPaIm2CiidJEt9lRVHy+Xyn1+q6nsvlisWiV4UJYP1EjKqqmqYpiuJyfecGMAjWNjA+Pv748WNTA5AkaZil8peu67y1M8b4GappWqlUWl5eTqfTHm7Oq4MetNPZtjzWhuTcJfawFdsGHNIejGpDfOS0XYpIbNj8WBiGQZ+8HFWdh2y324l1l63cV0Kom0e5XN7b27N2Dg4dkXNn4lxvXnVEtme0JEm8hk0nuKqqlUpleXk5lUpF7IPGfZ/s546f2KlUKlevXm21WrR85coV29VO5erVq5VKhZa3trYcHqRH6vV6Lpfjz3quXq8XCgX+L986f9b55ab1+xe0+omeU9VV1wYwCNbDbdsARop41Ew14Hnj9/CgB+10dtOQnLvE3rYSmR7Mtpz9FJ5qo16vb21t7e3t0bvRG9LfAZ3v1u124nLv3FdCqJuHw+GwLWTXzsRh17zqiDqd0bTpVqtVKBRKpRJ/ltf89evXPSlAoLjvk/1iPzrOGMtkMsViUVGUSqXCr5+KxWK9Xs9kMqVSiTF2qgs4WZYrlYrpct/2Qf6UoigejkBb398wDFo2DCORSNAyv0Dk1+U0Fp5IJGRZ3tvb297e5tdPqqqWSqVMJkO7QNfKiUTCMIxsNkuXoZ1eay1PoOon1AzDoGqv1+vT09N8EIKqq16v04GgY02r8cZsbQDMruXn8/lWqzU9PS1JUr1epwetb+i+ATC7w+3QAEaKruuyLGccf1GLj1Tt7e1lMhlxHJ2fknSA1tfX0+l0Pp+v1+ubm5vJZNL2oFtPZ+a6Dwza6eymIXXqEvvZSjR6MOrnK5UKYyyRSGSzWf6UqUuxbTMOZFnO5/OKoiSTSdONiM3NzQHsis12+UitruuFQoF20HaXXfarDhsNWvOg03l6epox1mq1qN+wrRD29GRvtVqbm5sOw/YOnYmpd7LWm/vwgwrJntbh9va2tUjOZ7QkSdlsln9OJRIJOhDUNvqpVWfWkquqau1Uqcamp6dbrRb/vLYeF9sjaLvd0/bJPrAN0ukyPZfL0dWVeNFw/fp1fjl1KltbW3S9eyJchdg+yC/dBn1xXCgU6DK0UCjQrQDOtF0q54lwubm1tdVqta5fv85fSAW2LltfayuA9RNeYovly7lcjg6W9UCUSiXTQJG1Yk0tv1Kp0L80kGC6sBbf0GUDOLE73LYNYKRQ/2N7g856kvLjS5Vve0ryarQeDvENO53OJ+76wKCdzi4bkkOX2NtWItOD2ZbT1KU4tBkr8SiYmtNA68R2u7b3oKzFOG2/an15AJuH+CFuu+98IN+6mnXlTg9aeyeHeusafvBKo/c57RlNnerW1tbVq1fpWX5nZtA6ldzaqfKnWq2Ww3E56XAErdz3yX7pODrOGEun0+l02pThJElSz/madPUmJoPaPqjruqIoiUQinU4PNHUsm81SkpabJDkqiSzLNBTKGMvlcpubm/yFmqbV63XbbDDrax02EZz6iQZ+oc/TwviB4E+1Wq2uDcDa8ukl4gs7vaHLBsDsDrdtqxgdsizTEEjXNbe3t4vFIt2yoNEs21Nyc3OzWCxms9lCoeAwCORwOrvsA4N2OrtpSKfqEl1uJcI9mKlLcWgzzvgQ5pD1s12HftVBAJsHpVPTssMdIVmW3azWibV36qHe+Jq6rovz3ByanO0ZTZ0qe3q7Q+wGaUyaMeZ8B6BnnUpu6lRVVV1fX+dPOb+nyyPIXPfJfnEKxyn938270BwUNwP+tk3H9CBvK8OhaZr76UqifD5fLpfZ0zmvsiwnEoneZvZwAayfsOsUWGiaJssydQF0HPvk1Rta24BzhzsKqMGrqurQydCnOBM+Y2xPSfpI65qMEcnT2Voe267b2iW67+FttzIiPVhvbcYwDF/CcXG71gU3L+/tgi1ozYPqgfaFh8XOFdI1eray9k69l5gxWZbL5TKdj26GKjoFOZIkUZpHMpksFoupVIqSpujbO/opYScuS55MJnO5nLXDsT0utkewEzd9sl/Gb968aX20UChomjY7Ozs7O6uqarlcPj4+TiaTiqJomlatVsVrF8bY22+//eqrr25sbMzOztpuRnyTsbExVVUzmUynB0ul0p07d/b393uLkk+FEuDESlAUhVKUeBk0Tbt9+zaVs1gsqqq6urpaLBavXbu2tLR07dq1arW6vr4uSdK9e/d+85vfHB8fl8vlWq1m+9pr166FqH5Can9/X1VVOhCUK0z1du/ePX4gfvzjH//+97+niq1Wq5qmbWxsMLsGQA+aWj6dIxsbG+VyeWlpSdO0Tz/99MMPPzS9ocsGwOzawPT0tLUBDLMafcdbu6qqqqrquk6Vb3uM3nrrrf39fcMwbt++vb6+TqNZ1lOSMTY2Nnbz5s3t7e1YLEYbsr5hp9d26gNNxQ7U6Wy7aXrK2nVbu0Tb1dxsxbYBh7cHy+VyY2NjtEeyLFu7lI2NjePjY2ubsaK6olZ0586dn/3sZ9QUKVGY2mGn1tWPTtut1Wrvv/++pmnvv//+nTt3eB9l2mU3/erq6qptmB7Y5nHnzp1arfbrX/+aCkAt3FQhx8fHmqZpmka1MTs7y3sD2w+Lrr2Truu29eYm/Lh27Rr1Trdv37579+7s7OzHH3/s0FpMZ7SpU43FYqurq7FYrFarlUolOhC6rq+urg6itm1Lbu1UY7EYb2yFQqHTcaGGansETdz3yYPYa7e8ynpxnqM9OlqtVliyISOvUql0zX91s47nG4UBsT37+jklR+F0dtl1o4d32RhC2mbq9br1Cz1s9yViXZzDFw1ZK6SffR9Qw+g6S+FUKpXK0L5VrGvJbWvMelx8z/n2ytjJyYmfVwMAAAAAQ6dpGg2+BuW7NU5DVVXDMLp+YVcAeVjyUB9BE4TjAAAAAAC+OeN3AQAAAAAARhfCcQAAAAAA3yAcBwAAAADwDcJxAAAAAADfIBwHAAAAAPANwnEAAAAAAN8gHAcAAAAA8A3CcQAAAAAA3/w/mDArSgaRVVwAAAAASUVORK5CYII=",
      "text/plain": [
       "Tree('S', [Tree('NP', [('Mr.', 'NNP'), ('Vinken', 'NNP')]), ('is', 'VBZ'), Tree('NP', [('chairman', 'NN')]), ('of', 'IN'), Tree('NP', [('Elsevier', 'NNP'), ('N.V.', 'NNP')]), (',', ','), Tree('NP', [('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN')]), ('.', '.')])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corpus 구조도 시각화 하기\n",
    "%matplotlib inline\n",
    "# import nltk\n",
    "from nltk.corpus import treebank_chunk\n",
    "\n",
    "print(treebank_chunk.chunked_sents()[1])\n",
    "treebank_chunk.chunked_sents()[1]  # .draw() # 외부 화면으로 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".leaves() \n",
      " [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')]\n",
      "\n",
      ".pos()  \n",
      " [(('Mr.', 'NNP'), 'NP'), (('Vinken', 'NNP'), 'NP'), (('is', 'VBZ'), 'S'), (('chairman', 'NN'), 'NP'), (('of', 'IN'), 'S'), (('Elsevier', 'NNP'), 'NP'), (('N.V.', 'NNP'), 'NP'), ((',', ','), 'S'), (('the', 'DT'), 'NP'), (('Dutch', 'NNP'), 'NP'), (('publishing', 'VBG'), 'NP'), (('group', 'NN'), 'NP'), (('.', '.'), 'S')]\n",
      "\n",
      ".productions() \n",
      " [S -> NP ('is', 'VBZ') NP ('of', 'IN') NP (',', ',') NP ('.', '.'), NP -> ('Mr.', 'NNP') ('Vinken', 'NNP'), NP -> ('chairman', 'NN'), NP -> ('Elsevier', 'NNP') ('N.V.', 'NNP'), NP -> ('the', 'DT') ('Dutch', 'NNP') ('publishing', 'VBG') ('group', 'NN')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markbaum/Python/python36/lib/python3.6/site-packages/nltk/tokenize/regexp.py:123: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return [tok for tok in self._regexp.split(text) if tok]\n"
     ]
    }
   ],
   "source": [
    "# Treebak에 포함된 chunk의 총 수\n",
    "# import nltk\n",
    "# from nltk.corpus import treebank_chunk\n",
    "print('.leaves() \\n',treebank_chunk.chunked_sents()[1].leaves())\n",
    "print('\\n.pos()  \\n',treebank_chunk.chunked_sents()[1].pos())\n",
    "print('\\n.productions() \\n',treebank_chunk.chunked_sents()[1].productions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question !!!!\n",
    "# 위 print(treebank_chunk.chunked_sents()[1] 뒤에 붙은 '메서드'들은\n",
    "# 무슨 이유로 개별적인 구별을 한 것인가??? 구별한 내용의 정의는??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags : 100676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 품사 주석\n",
    "print('Tags :',len(nltk.corpus.treebank.tagged_words()))\n",
    "nltk.corpus.treebank.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('I', 1), (' ', 27), ('g', 4), ('o', 14), ('t', 8), ('a', 7), ('m', 6), ('e', 15), ('s', 10), ('f', 4), ('r', 8), ('u', 5), ('P', 1), ('i', 6), ('d', 4), ('n', 4), ('U', 1), ('A', 1), ('c', 3), (',', 2), ('\\n', 1), ('Y', 2), ('h', 3), ('l', 2), ('k', 1), ('w', 2), ('b', 1), ('y', 1)])\n"
     ]
    }
   ],
   "source": [
    "# tag 의 빈도 계산하기는 아래의 코드를 활용한다\n",
    "# import nltk\n",
    "# from nltk.corpus import treebank\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "sentence = '''I got a messsage form our President of Unite state of America, \n",
    "You should know where this message came from, or You could be easy to ignored it'''\n",
    "fd = FreqDist(sentence)\n",
    "print(fd.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['一'], ['友情'], ['嘉珍', '和', '我', '住在', '同一條', '巷子'], ...]\n",
      "(S\n",
      "  (NP (NP (N‧的 (Nhaa 我) (DE 的)) (Ncb 腦海)) (Ncda 中))\n",
      "  (Dd 頓時)\n",
      "  (DM 一片)\n",
      "  (VH11 空白))\n"
     ]
    }
   ],
   "source": [
    "# sinica_treebank 를 살펴보자 : 대륙의 힘이다!!\n",
    "# import nltk\n",
    "from nltk.corpus import sinica_treebank\n",
    "print(sinica_treebank.sents())\n",
    "print(sinica_treebank.parsed_sents()[27])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 2 Treebank의 문맥 자유문법 규칙 추출\n",
    "CFG : Context Free Grammer 는 '노암촘스키'의 기준으로 정의한 문법이다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 단일 Tag 추출 및 비교하기\n",
    "from nltk import Nonterminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 문장구조 4분류 내용\n",
    "# 1 선언적 구조 : 서술문                (주어 다음 서술어)\n",
    "# 2 명령형 구조 : 명령문, 명령 또는 제안문 (동사구로 시작하고, 별도 주어문을 포함하지 않는다)\n",
    "# 3 예/ 아니오 구조 : 질의/ 응답문\n",
    "# 4 Wh- 질의구조   : 질의/ 응답문  (Wh-로 문장을 시작)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Korea"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단일 Tag의 추출 및 비교\n",
    "import nltk\n",
    "from nltk import Nonterminal\n",
    "\n",
    "# 얘는 그냥 출력만 하는건가? Tag에 대한 유효성검사도 없는듯 \n",
    "Nonterminal('Korea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NP'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonterminal1 = Nonterminal('NP')\n",
    "print(nonterminal1.symbol())\n",
    "nonterminal1.unicode_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "nonterminal2 = Nonterminal('VP')\n",
    "nonterminal3 = Nonterminal('PP')\n",
    "print(nonterminal1 == nonterminal2)\n",
    "print(nonterminal2 == nonterminal3)\n",
    "print(nonterminal1 == nonterminal3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 복수의 Tag 추출 및 비교\n",
    "from nltk import nonterminals (소문자)\n",
    "\n",
    "문장구조 : S -->  NP, VP  2단계 까지만 분석이 가능  (S -> NP, VP -> NP 는 오류!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S -> NP VP\n",
      "NP -> DT NP\n",
      "VP -> V NP NP PP\n"
     ]
    }
   ],
   "source": [
    "# 복수 tag의 추출 및 비교\n",
    "from nltk import nonterminals, Production\n",
    "nonterminals('South Korea is positioned between Japan and China')\n",
    "\n",
    "S, NP, VP, PP = nonterminals('S, NP, VP, PP')\n",
    "N, V, P, DT = nonterminals('N, V, P, DT')\n",
    "production1 = Production(S, [NP, VP])\n",
    "print(production1)\n",
    "production2 = Production(NP, [DT, NP])\n",
    "print(production2)\n",
    "production3 = Production(VP, [V, NP,NP,PP])\n",
    "print(production3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S\n",
      "(NP, VP)\n",
      "VP\n",
      "(V, NP, NP, PP)\n"
     ]
    }
   ],
   "source": [
    "print(production1.lhs()) # Left  tag 추출\n",
    "print(production1.rhs()) # Right tag 추출\n",
    "print(production3.lhs())\n",
    "print(production3.rhs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(production3 == Production(VP, [V,NP,NP,PP]))\n",
    "print(production2 == production3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 ATIS 문법을 활용\n",
    "Advanced Research Projects Agency Spoken Language Systems\n",
    "\n",
    "https://catalog.ldc.upenn.edu/LDC95S26  : 공항안내 시스템 개발용\n",
    "\n",
    "http://users.sussex.ac.uk/~johnca/cfg-resources/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Grammar with 5517 productions>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ATIS 문법에 접속\n",
    "import nltk\n",
    "gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "gram1 \n",
    "# print(gram1) 하면 자세한 내용을 알 수 있다.. \n",
    "# 근데 그 내용이 뭥미???\n",
    "# print(gram1)\n",
    "# Grammar with 5517 productions (start state = SIGMA)\n",
    "#     ABBCL_NP -> QUANP_DTI QUANP_DTI QUANP_CD AJP_JJ NOUN_NP PRPRTCL_VBG\n",
    "#     ADJ_ABL -> only\n",
    "#     ADJ_ABL -> such\n",
    "#     ADJ_AP -> pt_adj_ap\n",
    "#     ADJ_AP -> other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sent = nltk.data.load('grammars/large_grammars/atis_sentences.txt')\n",
    "sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "print(len(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "['list', 'those', 'flights', 'that', 'stop', 'over', 'in', 'salt', 'lake', 'city', '.']\n"
     ]
    }
   ],
   "source": [
    "testingsent = sent[25]\n",
    "print(testingsent[1])\n",
    "print(testingsent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['list',\n",
       " 'those',\n",
       " 'flights',\n",
       " 'that',\n",
       " 'stop',\n",
       " 'over',\n",
       " 'in',\n",
       " 'salt',\n",
       " 'lake',\n",
       " 'city',\n",
       " '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "sent = nltk.data.load('grammars/large_grammars/atis_sentences.txt')\n",
    "sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "testingsent = sent[25]\n",
    "sent = testingsent[0]\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13454\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# Bottom Up 방식의 파싱을 수행한다\n",
    "parser1 = nltk.parse.BottomUpChartParser(gram1)\n",
    "chart1 = parser1.chart_parse(sent)\n",
    "print((chart1.num_edges()))\n",
    "print((len(list(chart1.parses(gram1.start())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8781\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# Bottom-up, Left Corner 방식의 파싱\n",
    "# 질문 : 이게 무슨말이야 방구야??\n",
    "import nltk\n",
    "gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "sent = nltk.data.load('grammars/large_grammars/atis_sentences.txt')\n",
    "sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "testingsent = sent[25]\n",
    "sent = testingsent[0]\n",
    "parser2 = nltk.parse.BottomUpLeftCornerChartParser(gram1)\n",
    "chart2 = parser2.chart_parse(sent)\n",
    "print((chart2.num_edges()))\n",
    "print((len(list(chart2.parses(gram1.start())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# Bottom-up 필터를 사용한 Left Corner 방식의 파싱\n",
    "# 질문 : 바로 위 예제와 여기의 필터는 무슨 차이를 갖는거지??\n",
    "import nltk\n",
    "gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "sent = nltk.data.load('grammars/large_grammars/atis_sentences.txt')\n",
    "sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "testingsent=sent[25]\n",
    "sent=testingsent[0]\n",
    "parser3 = nltk.parse.LeftCornerChartParser(gram1)\n",
    "chart3 = parser3.chart_parse(sent)\n",
    "print((chart3.num_edges()))\n",
    "print((len(list(chart3.parses(gram1.start())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37763\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# Top-down 의 파싱\n",
    "# 얜 또 뭐래???\n",
    "import nltk\n",
    "gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "sent = nltk.data.load('grammars/large_grammars/atis_sentences.txt')\n",
    "sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "testingsent = sent[25]\n",
    "sent = testingsent[0]\n",
    "parser4 = nltk.parse.TopDownChartParser(gram1)\n",
    "chart4 = parser4.chart_parse(sent)\n",
    "print((chart4.num_edges()))\n",
    "print((len(list(chart4.parses(gram1.start())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13454\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# Incremental(증가하는) Bottom-up 파싱\n",
    "import nltk\n",
    "gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "sent = nltk.data.load('grammars/large_grammars/atis_sentences.txt')\n",
    "sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "testingsent = sent[25]\n",
    "sent = testingsent[0]\n",
    "parser5 = nltk.parse.IncrementalBottomUpChartParser(gram1)\n",
    "chart5 = parser5.chart_parse(sent)\n",
    "print((chart5.num_edges()))\n",
    "print((len(list(chart5.parses(gram1.start())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8781\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# Incremental(증가하는) Bottom-up, Left Corner 파싱\n",
    "import nltk\n",
    "gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "sent = nltk.data.load('grammars/large_grammars/atis_sentences.txt')\n",
    "sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "testingsent=sent[25]\n",
    "sent=testingsent[0]\n",
    "parser6 = nltk.parse.IncrementalBottomUpLeftCornerChartParser(gram1)\n",
    "chart6 = parser6.chart_parse(sent)\n",
    "print((chart6.num_edges()))\n",
    "print((len(list(chart6.parses(gram1.start())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# Bottom-up 필터를 사용한 Incremental(증가하는) Left Corner 파싱\n",
    "\n",
    "import nltk\n",
    "gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "sent = nltk.data.load('grammars/large_grammars/atis_sentences.txt')\n",
    "sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "testingsent=sent[25]\n",
    "sent=testingsent[0]\n",
    "parser7 = nltk.parse.IncrementalLeftCornerChartParser(gram1)\n",
    "chart7 = parser7.chart_parse(sent)\n",
    "print((chart7.num_edges()))\n",
    "print((len(list(chart7.parses(gram1.start())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37763\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# Incremental(증가하는) Top down 파싱\n",
    "\n",
    "import nltk\n",
    "gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "sent = nltk.data.load('grammars/large_grammars/atis_sentences.txt')\n",
    "sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "testingsent=sent[25]\n",
    "sent=testingsent[0]\n",
    "parser8 = nltk.parse.IncrementalTopDownChartParser(gram1)\n",
    "chart8 = parser8.chart_parse(sent)\n",
    "print((chart8.num_edges()))\n",
    "print((len(list(chart8.parses(gram1.start())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37763\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# Earley 파싱\n",
    "\n",
    "import nltk\n",
    "gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "sent = nltk.data.load('grammars/large_grammars/atis_sentences.txt')\n",
    "sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "testingsent=sent[25]\n",
    "sent=testingsent[0]\n",
    "parser9 = nltk.parse.EarleyChartParser(gram1)\n",
    "chart9 = parser9.chart_parse(sent)\n",
    "print((chart9.num_edges()))\n",
    "print((len(list(chart9.parses(gram1.start())))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 3 확률적 문맥 자유 문법 생성\n",
    "PCFG : Probabilistic Context-free Grammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "[A -> B B [0.3], A -> C B C [0.7], B -> B D [0.5], B -> C [0.5], C -> 'a' [0.1], C -> 'b' [0.9], D -> 'b' [1.0]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "from itertools import islice\n",
    "from nltk.grammar import PCFG, induce_pcfg, toy_pcfg1, toy_pcfg2\n",
    "gram2 = PCFG.fromstring(\"\"\"\n",
    "    A -> B B [.3] | C B C [.7]\n",
    "    B -> B D [.5] | C [.5]\n",
    "    C -> 'a' [.1] | 'b' [0.9]\n",
    "    D -> 'b' [1.0]\n",
    "    \"\"\")\n",
    "print(gram2.start())\n",
    "print(gram2.productions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A -> B B [0.3]\n"
     ]
    }
   ],
   "source": [
    "prod1 = gram2.productions()[0]\n",
    "print(prod1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A -> C B C [0.7]\n",
      "A\n",
      "(C, B, C)\n",
      "0.7\n"
     ]
    }
   ],
   "source": [
    "prod2 = gram2.productions()[1]\n",
    "print(prod2)\n",
    "print(prod2.lhs())\n",
    "print(prod2.rhs())\n",
    "print((prod2.prob()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 23 productions (start state = S)\n",
      "    S -> NP VP [1.0]\n",
      "    VP -> V NP [0.59]\n",
      "    VP -> V [0.4]\n",
      "    VP -> VP PP [0.01]\n",
      "    NP -> Det N [0.41]\n",
      "    NP -> Name [0.28]\n",
      "    NP -> NP PP [0.31]\n",
      "    PP -> P NP [1.0]\n",
      "    V -> 'saw' [0.21]\n",
      "    V -> 'ate' [0.51]\n",
      "    V -> 'ran' [0.28]\n",
      "    N -> 'boy' [0.11]\n",
      "    N -> 'cookie' [0.12]\n",
      "    N -> 'table' [0.13]\n",
      "    N -> 'telescope' [0.14]\n",
      "    N -> 'hill' [0.5]\n",
      "    Name -> 'Jack' [0.52]\n",
      "    Name -> 'Bob' [0.48]\n",
      "    P -> 'with' [0.61]\n",
      "    P -> 'under' [0.39]\n",
      "    Det -> 'the' [0.41]\n",
      "    Det -> 'a' [0.31]\n",
      "    Det -> 'my' [0.28]\n"
     ]
    }
   ],
   "source": [
    "# 확률적 차트 파싱\n",
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "from itertools import islice\n",
    "from nltk.grammar import PCFG, induce_pcfg, toy_pcfg1, toy_pcfg2\n",
    "tokens = \"Jack told Bob to bring my cookie\".split()\n",
    "grammar = toy_pcfg2\n",
    "print(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 4 CKY 차트 파싱 알고리즘\n",
    "Recursive Descent (귀납/회귀적 감소) 파싱의 단점은, 좌측 방향 재귀문제를 발생하여 복잡하다\n",
    "\n",
    "이를 극복하기 위해, CYK은 '동적 프로그래밍 접근'으로 $N^3$ 시간에 차트를 완성 가능하다\n",
    "\n",
    "CYK, Earley 모두 Bottom-up 차트 파싱 알고리즘을 활용한다\n",
    "\n",
    "https://stackoverflow.com/questions/26415163/python-nltk-cant-import-parse-cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Grammar with 12 productions>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CYK파싱 예제\n",
    "from nltk import CFG\n",
    "tok = [\"the\", \"kids\", \"opened\", \"the\", \"box\", \"on\", \"the\", \"floor\"]\n",
    "\n",
    "gram = CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    NP -> Det N | NP PP\n",
    "    VP -> V NP | VP PP\n",
    "    PP -> P NP\n",
    "    Det -> 'the'\n",
    "    N -> 'kids' | 'box' | 'floor'\n",
    "    V -> 'opened' \n",
    "    P -> 'on'\"\"\")\n",
    "gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PCFG\n",
    "toy_pcfg1 = PCFG.fromstring(\"\"\"\n",
    "    S -> NP VP [1.0]\n",
    "    NP -> Det N [0.5] | NP PP [0.25] | 'John' [0.1] | 'I' [0.15]\n",
    "    Det -> 'the' [0.8] | 'my' [0.2]\n",
    "    N -> 'man' [0.5] | 'telescope' [0.5]\n",
    "    VP -> VP PP [0.1] | V NP [0.7] | V [0.2]\n",
    "    V -> 'ate' [0.35] | 'saw' [0.65]\n",
    "    PP -> P NP [1.0]\n",
    "    P -> 'with' [0.61] | 'under' [0.39] \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 초기화 테이블을 구성하는 함수\n",
    "def init_nfst(tok, gram):\n",
    "    numtokens1 = len(tok)\n",
    "    # fill w/ dots\n",
    "    nfst = [[\".\" for i in range(numtokens1+1)]   for j in range(numtokens1+1)]\n",
    "    # 대각선으로 채우기\n",
    "    for i in range(numtokens1):\n",
    "        prod = gram.productions(rhs=tok[i])\n",
    "        nfst[i][i+1] = prod[0].lhs()\n",
    "    return nfst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테이블을 구성하는 코드\n",
    "def complete_nfst(nfst, tok, trace=False):\n",
    "    index1 = {} \n",
    "    for prod in gram.productions():\n",
    "        #make lookup reverse\n",
    "        index1[prod.rhs()] = prod.lhs()\n",
    "        numtokens1 = len(tok) \n",
    "    for span in range(2, numtokens1 +1):\n",
    "        for start in range(numtokens1 +1 -span):\n",
    "            #go down towards diagonal\n",
    "            end1 = start + span \n",
    "        for mid in range(start +1, end1):\n",
    "            nt1, nt2 = nfst[start][mid], nfst[mid][end1]\n",
    "    if (nt1,nt2) in index1:\n",
    "        if trace:\n",
    "            print(\"[%s] %3s [%s] %3s [%s] ==> [%s] %3s [%s]\" %\\\n",
    "                  (start, nt1, mid1, nt2, end1, start, index1[(nt1,nt2)], end)) \n",
    "    nfst[start1][end1] = index1[(nt1,nt2)]\n",
    "    return nfst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 디스플레이 테이블을 구성\n",
    "# def display(wfst, tok):\n",
    "#     print('\\nWFST ' + ' '.join([(\"%-4d\" % i) for i in range(1,len(wfst))]))\n",
    "#     for i in range(len(wfst)-1):\n",
    "#         print(\"%d \" % i)\n",
    "#         for j in range(1, len(wfst)):\n",
    "#             print(\"%-4s\" % wfst[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['.', Det, '.', '.', '.', '.', '.', '.', '.'],\n",
       " ['.', '.', N, '.', '.', '.', '.', '.', '.'],\n",
       " ['.', '.', '.', V, '.', '.', '.', '.', '.'],\n",
       " ['.', '.', '.', '.', Det, '.', '.', '.', '.'],\n",
       " ['.', '.', '.', '.', '.', N, '.', '.', '.'],\n",
       " ['.', '.', '.', '.', '.', '.', P, '.', '.'],\n",
       " ['.', '.', '.', '.', '.', '.', '.', Det, '.'],\n",
       " ['.', '.', '.', '.', '.', '.', '.', '.', N],\n",
       " ['.', '.', '.', '.', '.', '.', '.', '.', '.']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['the', 'kids', 'opened', 'the', 'box', 'on', 'the', 'floor']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 코드의 결과를 출력\n",
    "tok = [\"the\", \"kids\", \"opened\", \"the\", \"box\", \"on\", \"the\", \"floor\"]\n",
    "res1 = init_nfst(tok, gram)\n",
    "display(res1, tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res2 = complete_nfst(res1,tok)\n",
    "# display(res2, tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 5 Earley 차트 파싱 알고리즘\n",
    "1970년 발표한 알고리즘으로 Top-Down 파싱과 유사\n",
    "\n",
    "좌측방향 재귀처리를 하며, CNF를 필요로 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Sentence:\n",
      "John saw a dog\n",
      "['John', 'saw', 'a', 'dog']\n",
      "\n",
      "* Strategy: Bottom-up\n",
      "\n",
      "|.   John  .   saw   .    a    .   dog   .|\n",
      "|[---------]         .         .         .| [0:1] 'John'\n",
      "|.         [---------]         .         .| [1:2] 'saw'\n",
      "|.         .         [---------]         .| [2:3] 'a'\n",
      "|.         .         .         [---------]| [3:4] 'dog'\n",
      "|>         .         .         .         .| [0:0] NP -> * 'John'\n",
      "|[---------]         .         .         .| [0:1] NP -> 'John' *\n",
      "|>         .         .         .         .| [0:0] S  -> * NP VP\n",
      "|>         .         .         .         .| [0:0] NP -> * NP PP\n",
      "|[--------->         .         .         .| [0:1] S  -> NP * VP\n",
      "|[--------->         .         .         .| [0:1] NP -> NP * PP\n",
      "|.         >         .         .         .| [1:1] Verb -> * 'saw'\n",
      "|.         [---------]         .         .| [1:2] Verb -> 'saw' *\n",
      "|.         >         .         .         .| [1:1] VP -> * Verb NP\n",
      "|.         >         .         .         .| [1:1] VP -> * Verb\n",
      "|.         [--------->         .         .| [1:2] VP -> Verb * NP\n",
      "|.         [---------]         .         .| [1:2] VP -> Verb *\n",
      "|.         >         .         .         .| [1:1] VP -> * VP PP\n",
      "|[-------------------]         .         .| [0:2] S  -> NP VP *\n",
      "|.         [--------->         .         .| [1:2] VP -> VP * PP\n",
      "|.         .         >         .         .| [2:2] Det -> * 'a'\n",
      "|.         .         [---------]         .| [2:3] Det -> 'a' *\n",
      "|.         .         >         .         .| [2:2] NP -> * Det Noun\n",
      "|.         .         [--------->         .| [2:3] NP -> Det * Noun\n",
      "|.         .         .         >         .| [3:3] Noun -> * 'dog'\n",
      "|.         .         .         [---------]| [3:4] Noun -> 'dog' *\n",
      "|.         .         [-------------------]| [2:4] NP -> Det Noun *\n",
      "|.         .         >         .         .| [2:2] S  -> * NP VP\n",
      "|.         .         >         .         .| [2:2] NP -> * NP PP\n",
      "|.         [-----------------------------]| [1:4] VP -> Verb NP *\n",
      "|.         .         [------------------->| [2:4] S  -> NP * VP\n",
      "|.         .         [------------------->| [2:4] NP -> NP * PP\n",
      "|[=======================================]| [0:4] S  -> NP VP *\n",
      "|.         [----------------------------->| [1:4] VP -> VP * PP\n",
      "Nr edges in chart: 33\n",
      "(S (NP John) (VP (Verb saw) (NP (Det a) (Noun dog))))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NLTK 차트파서를 사용한 파싱\n",
    "\n",
    "import nltk\n",
    "# AssertionError: Not all parses found   # 오류가 발생\n",
    "# nltk.parse.chart.demo(print_times=False, trace=1,sent='I saw a dog', numparses=2)\n",
    "# nltk.parse.chart.demo(5, print_times=False, trace=1,sent='John saw a dog', numparses=2)\n",
    "nltk.parse.chart.demo(2, print_times=False, trace=1, sent='John saw a dog', numparses=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grammar with 18 productions (start state = S[])\n",
      "    S[] -> NP[] VP[]\n",
      "    PP[] -> Prep[] NP[]\n",
      "    NP[] -> NP[] PP[]\n",
      "    VP[] -> VP[] PP[]\n",
      "    VP[] -> Verb[] NP[]\n",
      "    VP[] -> Verb[]\n",
      "    NP[] -> Det[pl=?x] Noun[pl=?x]\n",
      "    NP[] -> 'John'\n",
      "    NP[] -> 'I'\n",
      "    Det[] -> 'the'\n",
      "    Det[] -> 'my'\n",
      "    Det[-pl] -> 'a'\n",
      "    Noun[-pl] -> 'dog'\n",
      "    Noun[-pl] -> 'cookie'\n",
      "    Verb[] -> 'ate'\n",
      "    Verb[] -> 'saw'\n",
      "    Prep[] -> 'with'\n",
      "    Prep[] -> 'under'\n",
      "\n",
      "* FeatureChartParser\n",
      "Sentence: I saw a dog\n",
      "|. I .saw. a .dog.|\n",
      "|[---]   .   .   .| [0:1] 'I'\n",
      "|.   [---]   .   .| [1:2] 'saw'\n",
      "|.   .   [---]   .| [2:3] 'a'\n",
      "|.   .   .   [---]| [3:4] 'dog'\n",
      "|[---]   .   .   .| [0:1] NP[] -> 'I' *\n",
      "|[--->   .   .   .| [0:1] S[] -> NP[] * VP[] {}\n",
      "|[--->   .   .   .| [0:1] NP[] -> NP[] * PP[] {}\n",
      "|.   [---]   .   .| [1:2] Verb[] -> 'saw' *\n",
      "|.   [--->   .   .| [1:2] VP[] -> Verb[] * NP[] {}\n",
      "|.   [---]   .   .| [1:2] VP[] -> Verb[] *\n",
      "|.   [--->   .   .| [1:2] VP[] -> VP[] * PP[] {}\n",
      "|[-------]   .   .| [0:2] S[] -> NP[] VP[] *\n",
      "|.   .   [---]   .| [2:3] Det[-pl] -> 'a' *\n",
      "|.   .   [--->   .| [2:3] NP[] -> Det[pl=?x] * Noun[pl=?x] {?x: False}\n",
      "|.   .   .   [---]| [3:4] Noun[-pl] -> 'dog' *\n",
      "|.   .   [-------]| [2:4] NP[] -> Det[-pl] Noun[-pl] *\n",
      "|.   .   [------->| [2:4] S[] -> NP[] * VP[] {}\n",
      "|.   .   [------->| [2:4] NP[] -> NP[] * PP[] {}\n",
      "|.   [-----------]| [1:4] VP[] -> Verb[] NP[] *\n",
      "|.   [----------->| [1:4] VP[] -> VP[] * PP[] {}\n",
      "|[===============]| [0:4] S[] -> NP[] VP[] *\n",
      "(S[]\n",
      "  (NP[] I)\n",
      "  (VP[] (Verb[] saw) (NP[] (Det[-pl] a) (Noun[-pl] dog))))\n"
     ]
    }
   ],
   "source": [
    "# NLTK Feature 차트파싱\n",
    "import nltk\n",
    "nltk.parse.featurechart.demo( print_times = False, \n",
    "        print_grammar = True, parser = nltk.parse.featurechart.FeatureChartParser, \n",
    "        sent = 'I saw a dog' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Earley 알고리즘을 구현한 NLTK코드\n",
    "# A demonstration of the Earley parsers.\n",
    "def demo(print_times = True, print_grammar = False,\n",
    "         print_trees = True, trace = 2,\n",
    "         sent = 'I saw John with a dog with my cookie', \n",
    "         numparses=5):\n",
    "\n",
    "    import sys, time\n",
    "    from nltk.parse.chart import demo_grammar\n",
    "    \n",
    "    # The grammar for ChartParser and SteppingChartParser:\n",
    "    grammar = demo_grammar()\n",
    "    if print_grammar:\n",
    "        print(\"* Grammar\", grammar)\n",
    "        # 샘플문장을 토큰화 한다\n",
    "        print(\"* Sentence:\", sent)\n",
    "        tokens = sent.split()\n",
    "        print(tokens)\n",
    "\n",
    "    # 파싱 작업을 시작한다.\n",
    "    earley = EarleyChartParser(grammar, trace=trace)\n",
    "    t = time.clock()\n",
    "    chart = earley.chart_parse(tokens)\n",
    "    parses = list(chart.parses(grammar.start()))\n",
    "    t = time.clock() - t\n",
    "    # 결과를 출력한다\n",
    "    if numparses: \n",
    "        assert len(parses) == numparses, 'Not all parses found'\n",
    "    if print_trees:\n",
    "        for tree in parses:\n",
    "            print(tree)\n",
    "        else: print(\"Nr trees:\", len(parses))\n",
    "    if print_times: \n",
    "        print(\"Time:\", t)\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
