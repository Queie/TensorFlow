{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "기본 Processer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Structure\n",
    "Input Data -> Inference(가설모델을 계산) -> Loss(손실계산) -> train(파라미터 갱신) -> Evaluation(평가)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def inference(x):pass\n",
    "def loss(x,y):pass\n",
    "def inputs() :pass \n",
    "def train(total_loss):pass\n",
    "def evaluation(sexx, x, y):pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess: pass\n",
    "#     tf.global_variables_initializer()\n",
    "#     #x,y = inputs()\n",
    "#     total_loss = loss(x,y)\n",
    "#     train_operation = train(total_loss)\n",
    "#     coord = tf.train.Coordinator()\n",
    "#     threads = tf.train.start_queue_runners(sess = sess, coord = coord)\n",
    "#     training_steps = 1000\n",
    "\n",
    "#     for step in range(training_steps):\n",
    "#         sess.run([train_operation])\n",
    "#         if step % 10 == 0:  \n",
    "#             print('loss :', sess.run(total_loss))\n",
    "\n",
    "#     evaluation(sess, x ,y)\n",
    "#     coord.request_stop()\n",
    "#     coord.join(threads)\n",
    "#     sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2 학습 Check Points\n",
    "tf.train.Saver() : 이진파일로 학습Graph를 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 학습 전 저장할 객체를 생성\n",
    "#saver = tf.train.Saver()\n",
    "\n",
    "# 학습 반복 중 모델을 저장 : -{step} 으로  my-mode-2000 체크포인트별 객체를 생성\n",
    "# if step % 1000 == 0:\n",
    "#     saver.save(sess, 'my-model-{step}', global_step = step )\n",
    "\n",
    "# 학습이 끝난 모델객체를 저장\n",
    "# saver.save(sess, 'my-model', global_step = training_steps)\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf.train.get_checkpoint_state() - 특정 지점에서 학습을 재개한다\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     initial_step = 0\n",
    "#     ckpt = tf.train.get_checkpoint_state(os.path.dirname(__file__))\n",
    "#     if ckpt and ckpt.model_checkpoint_path:\n",
    "#         saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "#         initial_step = int(ckpt.model_checkpoint_path.rsplit('-', 1)[1])\n",
    "\n",
    "# #actual training loop\n",
    "# for step in range(initial_step, training_steps):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3 Linear_Regression\n",
    "선형회귀 : Continuous 적인 real number 를 예측시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# 가설함수 모델 만들기\n",
    "W = tf.Variable(tf.zeros([2, 1]), name=\"weights\")\n",
    "b = tf.Variable(0., name=\"bias\")\n",
    "\n",
    "def inference(X):\n",
    "    return tf.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sequare Error 를 활용한 가설함수 검증하기\n",
    "def loss(X, Y):\n",
    "    Y_predicted = inference(X)\n",
    "    return tf.reduce_sum(tf.squared_difference(Y, Y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 입력데이터\n",
    "# Data from http://people.sc.fsu.edu/~jburkardt/datasets/regression/x09.txt\n",
    "def inputs():\n",
    "    weight_age = [[84, 46], [73, 20], [65, 52], [70, 30], [76, 57], [69, 25], [63, 28],\n",
    "                  [72, 36], [79, 57], [75, 44], [27, 24], [89, 31], [65, 52], [57, 23],\n",
    "                  [59, 60], [69, 48], [60, 34], [79, 51], [75, 50], [82, 34], [59, 46],\n",
    "                  [67, 23], [85, 37], [55, 40], [63, 30]]\n",
    "    blood_fat_content = [354, 190, 405, 263, 451, 302, 288, 385, 402, 365, 209, 290, 346, \n",
    "                         254, 395, 434, 220, 374, 308, 220, 311, 181, 274, 303, 244]\n",
    "    return tf.to_float(weight_age), tf.to_float(blood_fat_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 학습모델을 실행\n",
    "def train(total_loss):\n",
    "    learning_rate = 0.0000001\n",
    "    return tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 학습모델을 검증\n",
    "def evaluate(sess, X, Y):\n",
    "    print (\"[80., 25.]  to..\" , sess.run(inference([[80., 25.]]))) # ~ 303\n",
    "    print (\"[65., 25.]  to..\" , sess.run(inference([[65., 25.]]))) # ~ 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0  loss: [7608772.5] \n",
      "  100  loss: [5341925.5] \n",
      "  200  loss: [5339993.0] \n",
      "  300  loss: [5338747.0] \n",
      "  400  loss: [5337538.5] \n",
      "  500  loss: [5336334.0] \n",
      "  600  loss: [5335129.5] \n",
      "  700  loss: [5333925.5] \n",
      "  800  loss: [5332724.0] \n",
      "  900  loss: [5331523.0] \n",
      "[80., 25.]  to.. [[ 320.64968872]]\n",
      "[65., 25.]  to.. [[ 267.78182983]]\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session, setup boilerplate\n",
    "with tf.Session() as sess:\n",
    "    training_steps = 1000   # 반복학습할 횟수를 지정 \n",
    "    tf.global_variables_initializer().run()\n",
    "    X, Y = inputs()   # X ,Y 값 입력\n",
    "    total_loss = loss(X, Y)\n",
    "    train_op = train(total_loss)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    for step in range(training_steps):\n",
    "        sess.run([train_op])\n",
    "        if step % 100 == 0:\n",
    "            print (\" {0:4}  loss: {1} \".format(step, sess.run([total_loss])))\n",
    "    evaluate(sess, X, Y)\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4 Logistic Regression\n",
    "로지스틱 회귀분석 : 데이터를 통해서 Yes - No 선택적 결론의 회귀식 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Titanic 데이터 분석 샘플 (with Python)\n",
    "\n",
    "http://donnemartin.com/predicting-titanic-survivors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Kaggle의 Titanic 데이터를 분석\n",
    "# Download 1 https://www.kaggle.com/c/titanic/data\n",
    "# Download 2 https://github.com/pcsanwald/kaggle-titanic/blob/master/train.csv\n",
    "import tensorflow as tf; import os\n",
    "\n",
    "# 선형회귀식의 가설함수를 재활용\n",
    "W = tf.Variable(tf.zeros([5, 1]), name=\"weights\")\n",
    "b = tf.Variable(0., name=\"bias\")\n",
    "\n",
    "def combine_inputs(X):\n",
    "    return tf.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sigmoid 를 사용\n",
    "def inference(X):\n",
    "    return tf.sigmoid(combine_inputs(X))\n",
    "\n",
    "# Cross Entropy 를 사용\n",
    "def loss(X, Y):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=combine_inputs(X), labels=Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_csv(batch_size, file_name, record_defaults):\n",
    "    filename_queue = tf.train.string_input_producer([os.path.join(os.getcwd(), file_name)])\n",
    "    reader = tf.TextLineReader(skip_header_lines=1)\n",
    "    key, value = reader.read(filename_queue)\n",
    "    decoded = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "    return tf.train.shuffle_batch(decoded, batch_size=batch_size,\n",
    "                                  capacity=batch_size * 50, min_after_dequeue=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inputs():\n",
    "    survived, pclass, name, sex, age, sibsp, parch, ticket, fare, cabin, embarked = \\\n",
    "        read_csv(100, \"./data/train.csv\", [[0.0], [0], [\"\"], [\"\"], [0.0], [0.0], [0.0], [\"\"], [0.0], [\"\"], [\"\"]])\n",
    "    # convert categorical data\n",
    "    is_first_class = tf.to_float(tf.equal(pclass, [1]))\n",
    "    is_second_class = tf.to_float(tf.equal(pclass, [2]))\n",
    "    is_third_class = tf.to_float(tf.equal(pclass, [3]))\n",
    "    gender = tf.to_float(tf.equal(sex, [\"female\"]))\n",
    "    # Finally we pack all the features in a single matrix;\n",
    "    # We then transpose to have a matrix with one example per row and one feature per column.\n",
    "    features = tf.transpose(tf.stack([is_first_class, is_second_class, is_third_class, gender, age]))\n",
    "    survived = tf.reshape(survived, [100, 1])\n",
    "    return features, survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(total_loss):\n",
    "    learning_rate = 0.01\n",
    "    return tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(sess, X, Y):\n",
    "    predicted = tf.cast(inference(X) > 0.5, tf.float32)\n",
    "    print (sess.run(tf.reduce_mean(tf.cast(tf.equal(predicted, Y), tf.float32))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:  [0.94643354]\n",
      "100 loss:  [0.69507045]\n",
      "200 loss:  [0.61978805]\n",
      "300 loss:  [0.62616438]\n",
      "400 loss:  [0.78409117]\n",
      "500 loss:  [0.57193363]\n",
      "600 loss:  [0.57005286]\n",
      "700 loss:  [0.54663479]\n",
      "800 loss:  [0.53052485]\n",
      "900 loss:  [0.5549221]\n",
      "0.67\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session, setup boilerplate\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    X, Y = inputs()\n",
    "    total_loss = loss(X, Y)\n",
    "    train_op = train(total_loss)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    # actual training loop\n",
    "    training_steps = 1000\n",
    "    for step in range(training_steps):\n",
    "        sess.run([train_op])\n",
    "        # for debugging and learning purposes, see how the loss gets decremented thru training steps\n",
    "        if step % 100 == 0:\n",
    "            print (step,\"loss: \", sess.run([total_loss]))\n",
    "    evaluate(sess, X, Y)\n",
    "    import time\n",
    "    time.sleep(5)\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5 Softmax Classification\n",
    "다중 선택형 모델 # Softma example in TF using the classical Iris dataset\n",
    "\n",
    "출력 클래스에 n개의 뉴런을 사용한 신경망을 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download iris.data - https://archive.ics.uci.edu/ml/datasets/Iris\n",
    "import tensorflow as tf; import os\n",
    "\n",
    "# 4개의 특징값을 갖고, 3개의 출력 클래스를 갖는다 \n",
    "# 때문에 weight matrix 는 4 X 3\n",
    "W = tf.Variable(tf.zeros([4, 3]), name=\"weights\")\n",
    "b = tf.Variable(tf.zeros([3], name=\"bias\")) # so do the biases, one per class.\n",
    "\n",
    "def combine_inputs(X):\n",
    "    return tf.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf.nn.softmax() : 하나의 클래스 값을 갖는 학습샘플의 최적화 softmax()\n",
    "def inference(X):\n",
    "    return tf.nn.softmax(combine_inputs(X))\n",
    "\n",
    "# tf.nn.sparse_softmax_cross_entropy_with_logits() : A, B, C 중 하나의 클래스 값을 갖는 학습샘플의 최적화 softmax()\n",
    "def loss(X, Y):\n",
    "    return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=combine_inputs(X), labels=Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_csv(batch_size, file_name, record_defaults):\n",
    "    filename_queue = tf.train.string_input_producer([os.path.join(os.getcwd(), file_name)])\n",
    "    reader = tf.TextLineReader(skip_header_lines=1)\n",
    "    key, value = reader.read(filename_queue)\n",
    "    decoded = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "    return tf.train.shuffle_batch(decoded, batch_size=batch_size,\n",
    "                                  capacity=batch_size * 50, min_after_dequeue=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ./data/iris.data 의 Default 값을 호출\n",
    "# tf.stack() 텐서를 생성\n",
    "# tf.equal() 파일입력과 가능한 값을 비교\n",
    "def inputs():\n",
    "    sepal_length, sepal_width, petal_length, petal_width, label =\\\n",
    "        read_csv(100, \"./data/iris.data\", [[0.0], [0.0], [0.0], [0.0], [\"\"]])\n",
    "    # convert class names to a 0 based class index.\n",
    "    label_number = tf.to_int32(tf.argmax(tf.to_int32(tf.stack([\n",
    "                                tf.equal(label, [\"Iris-setosa\"]),\n",
    "                                tf.equal(label, [\"Iris-versicolor\"]),\n",
    "                                tf.equal(label, [\"Iris-virginica\"])])), 0))\n",
    "    features = tf.transpose(tf.stack([sepal_length, sepal_width, petal_length, petal_width]))\n",
    "    return features, label_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 학습 및 평가함수 : \n",
    "# tf.argmax() 참인 값을 갖는 텐서의 index 출력\n",
    "def train(total_loss):\n",
    "    learning_rate = 0.01\n",
    "    return tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "# 평가 결과는 tf.reduce_mean()를 적용\n",
    "def evaluate(sess, X, Y):\n",
    "    predicted = tf.cast(tf.arg_max(inference(X), 1), tf.int32)\n",
    "    print(sess.run(tf.reduce_mean(tf.cast(tf.equal(predicted, Y), tf.float32))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, Expect 5 fields but have 0 in record 0\n",
      "\t [[Node: DecodeCSV_1 = DecodeCSV[OUT_TYPE=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_STRING], field_delim=\",\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ReaderReadV2_1:1, DecodeCSV_1/record_defaults_0, DecodeCSV_1/record_defaults_1, DecodeCSV_1/record_defaults_2, DecodeCSV_1/record_defaults_3, DecodeCSV_1/record_defaults_4)]]\n"
     ]
    },
    {
     "ename": "OutOfRangeError",
     "evalue": "RandomShuffleQueue '_6_shuffle_batch_1/random_shuffle_queue' is closed and has insufficient elements (requested 100, current size 49)\n\t [[Node: shuffle_batch_1 = QueueDequeueManyV2[component_types=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch_1/random_shuffle_queue, shuffle_batch_1/n)]]\n\nCaused by op 'shuffle_batch_1', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-23-7b3fc77c7fcb>\", line 4, in <module>\n    X, Y = inputs()\n  File \"<ipython-input-21-2850925bf680>\", line 5, in inputs\n    sepal_length, sepal_width, petal_length, petal_width, label =        read_csv(100, \"./data/iris.data\", [[0.0], [0.0], [0.0], [0.0], [\"\"]])\n  File \"<ipython-input-20-dd54c9aa02b0>\", line 7, in read_csv\n    capacity=batch_size * 50, min_after_dequeue=batch_size)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/tensorflow/python/training/input.py\", line 1217, in shuffle_batch\n    name=name)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/tensorflow/python/training/input.py\", line 788, in _shuffle_batch\n    dequeued = queue.dequeue_many(batch_size, name=name)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 457, in dequeue_many\n    self._queue_ref, n=n, component_types=self._dtypes, name=name)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 946, in _queue_dequeue_many_v2\n    timeout_ms=timeout_ms, name=name)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nOutOfRangeError (see above for traceback): RandomShuffleQueue '_6_shuffle_batch_1/random_shuffle_queue' is closed and has insufficient elements (requested 100, current size 49)\n\t [[Node: shuffle_batch_1 = QueueDequeueManyV2[component_types=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch_1/random_shuffle_queue, shuffle_batch_1/n)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m~/Python/python-cpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python/python-cpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python/python-cpu/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: RandomShuffleQueue '_6_shuffle_batch_1/random_shuffle_queue' is closed and has insufficient elements (requested 100, current size 49)\n\t [[Node: shuffle_batch_1 = QueueDequeueManyV2[component_types=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch_1/random_shuffle_queue, shuffle_batch_1/n)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-7b3fc77c7fcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# for debugging and learning purposes, see how the loss gets decremented thru training steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"loss: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python/python-cpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python/python-cpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python/python-cpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/Python/python-cpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: RandomShuffleQueue '_6_shuffle_batch_1/random_shuffle_queue' is closed and has insufficient elements (requested 100, current size 49)\n\t [[Node: shuffle_batch_1 = QueueDequeueManyV2[component_types=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch_1/random_shuffle_queue, shuffle_batch_1/n)]]\n\nCaused by op 'shuffle_batch_1', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-23-7b3fc77c7fcb>\", line 4, in <module>\n    X, Y = inputs()\n  File \"<ipython-input-21-2850925bf680>\", line 5, in inputs\n    sepal_length, sepal_width, petal_length, petal_width, label =        read_csv(100, \"./data/iris.data\", [[0.0], [0.0], [0.0], [0.0], [\"\"]])\n  File \"<ipython-input-20-dd54c9aa02b0>\", line 7, in read_csv\n    capacity=batch_size * 50, min_after_dequeue=batch_size)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/tensorflow/python/training/input.py\", line 1217, in shuffle_batch\n    name=name)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/tensorflow/python/training/input.py\", line 788, in _shuffle_batch\n    dequeued = queue.dequeue_many(batch_size, name=name)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 457, in dequeue_many\n    self._queue_ref, n=n, component_types=self._dtypes, name=name)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 946, in _queue_dequeue_many_v2\n    timeout_ms=timeout_ms, name=name)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/markbaum/Python/python-cpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nOutOfRangeError (see above for traceback): RandomShuffleQueue '_6_shuffle_batch_1/random_shuffle_queue' is closed and has insufficient elements (requested 100, current size 49)\n\t [[Node: shuffle_batch_1 = QueueDequeueManyV2[component_types=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch_1/random_shuffle_queue, shuffle_batch_1/n)]]\n"
     ]
    }
   ],
   "source": [
    "# 작성한 Graph 를 실행\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    X, Y = inputs()\n",
    "    total_loss = loss(X, Y)\n",
    "    train_op = train(total_loss)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    # actual training loop\n",
    "    training_steps = 1000\n",
    "    for step in range(training_steps):\n",
    "        sess.run([train_op])\n",
    "        # for debugging and learning purposes, see how the loss gets decremented thru training steps\n",
    "        if step % 100 == 0:\n",
    "            print (step, \"loss: \", sess.run([total_loss]))\n",
    "\n",
    "    evaluate(sess, X, Y)\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 6 Multi-Layer Nural Networks\n",
    "여러 선형신경망의 가중치 합을 계산하며, 활성화(Activation) 및 전송(Transfer)함수를 적용하며\n",
    "\n",
    "활성화 함수는 1)선형회귀는 항등함수를 2)로지스틱 회귀는 Sigmoid를 사용한다\n",
    "\n",
    "cf)'선형분리'로 XOR는 부적합, 신경망 입출력에 '더 많은 뉴런'을 삽입함으로 써 극복 (Deep Learning 개념의 시작)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 7 Gradient Descent & Backpropagation\n",
    "tf.summary.scalar()   #손실을 텐서보드로 시각화 하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
