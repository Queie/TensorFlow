{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br>\n",
    "# Chapter 9 - 담화 분석 Discourse Analysis \n",
    "Knowing Is Believing (아는 것은 믿는 것이다)\n",
    "1. 담화 분석 소개\n",
    "2. 중심화 이론을 사용한 담화분석\n",
    "3. 대용어 복원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 1 담화 분석\n",
    "NER(Named-entity recognition), AR(Anaphora Resolution) 담화분석\n",
    "1. Discourse Representation Theory (DRT) - AR을 수행하는 방법을 제공\n",
    "2. Discourse Representation Structure (DRS) - 담화의 의미를 제공\n",
    "\n",
    "<img src=\"https://sourcecodebrowser.com/w3af/1.0~rc2svn3429/classextlib_1_1nltk_1_1sem_1_1logic_1_1_expression__coll__graph.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 First Order Predicate Logic (FOPL) \n",
    "명제 논리의 사실을 확장가능, text를 선형으로 변환한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([x],[John(x), Went(x)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExistsExpression exists x.(John(x) & Went(x))>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "expr_read = nltk.sem.DrtExpression.fromstring\n",
    "expr1 = expr_read('([x], [John(x), Went(x)])')\n",
    "print(expr1)\n",
    "\n",
    "# expr1.draw()   # 이미지로 출력\n",
    "expr1.fol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([x,y],[John(x), Went(x), Sam(y), Meet(x,y)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExistsExpression exists x y.(John(x) & Went(x) & Sam(y) & Meet(x,y))>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "expr_read = nltk.sem.DrtExpression.fromstring\n",
    "expr2 = expr_read('([x,y], [John(x), Went(x),Sam(y),Meet(x,y)])')\n",
    "\n",
    "print(expr2)\n",
    "#expr2.draw()\n",
    "expr2.fol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(([x],[John(x), eats(x)]) + ([y],[Sam(y), eats(y)]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<DRS ([x,y],[John(x), eats(x), Sam(y), eats(y)])>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "expr_read = nltk.sem.DrtExpression.fromstring\n",
    "expr3 = expr_read('([x], [John(x), eats(x)])+ ([y],[Sam(y),eats(y)])')\n",
    "\n",
    "print(expr3)\n",
    "#expr3.draw()\n",
    "expr3.simplify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AllExpression all x.(student(x) -> exists y.(book(y) & read(x,y)))>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "expr_read = nltk.sem.DrtExpression.fromstring\n",
    "expr4 = expr_read('([],[(([x],[student(x)])->([y],[book(y),read(x,y)]))])')\n",
    "\n",
    "expr4.fol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([u,x,y,z],[ram(x), food(y), eats(x,y), PRO(u), coffee(z), drinks(u,z)])\n",
      "([u,x,y,z],[ram(x), food(y), eats(x,y), (u = [x,y,z]), coffee(z), drinks(u,z)])\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "expr_read = nltk.sem.DrtExpression.fromstring\n",
    "expr5 = expr_read('([x,y],[ram(x),food(y),eats(x,y)])')\n",
    "expr6 = expr_read('([u,z],[PRO(u),coffee(z),drinks(u,z)])')\n",
    "\n",
    "expr7 = expr5 + expr6\n",
    "print(expr7.simplify())\n",
    "print(expr7.simplify().resolve_anaphora())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 중심화 이론을 사용한 담화분석\n",
    "중심화 (Centering Therory): 참가자의 관심, 사고방식, 담화의 일관성에 영향. Corpus 주석의 첫 단계에 해당(AR도 포함)\n",
    "1. 담화 참가자와 담화의 목적/ 의도간의 상호작용\n",
    "2. 참가자의 태도\n",
    "3. 담화 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 대용어 복원 Anaphora resolution\n",
    "대명사, 명사구가 지칭하는 구체적 개체(NER)를 발견\n",
    "1. 대명사\n",
    "2. 명확한 명사구\n",
    "3. 수량사/서수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br>\n",
    "# Chapter 10 - Evaluation of NLP Systems (NLP 시스템의 평가)\n",
    "Analyzing Performance(성능분석)\n",
    "1. NLP 시스템 평가\n",
    "2. NLP 도구의 평가(POS Tagger, Stemmer)\n",
    "3. 골드 데이터를 사용한 Parser\n",
    "4. IR 시스템의 평가\n",
    "5. 오류 식별 메트릭\n",
    "6. 어휘 매칭 기반 메트릭\n",
    "7. 구분 매칭 기반 메트릭\n",
    "8. 얕은 의미 매칭을 사용한 메트릭"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 1 NLP 시스템의 평가 필요성\n",
    "NLP 형태소 분석 알고리즘은 모듈의 오류를 초기에 식별하는 용도로 활용\n",
    "\n",
    "http://ra2kstar.tistory.com/170"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 01 Unigram POS 태거\n",
    "(NLP도구 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Others', 'NNS'),\n",
       " (',', ','),\n",
       " ('which', 'WDT'),\n",
       " ('are', 'BER'),\n",
       " ('reached', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('walking', 'VBG'),\n",
       " ('up', 'RP'),\n",
       " ('a', 'AT'),\n",
       " ('single', 'AP'),\n",
       " ('flight', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('stairs', 'NNS'),\n",
       " (',', ','),\n",
       " ('have', 'HV'),\n",
       " ('balconies', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# token에 tag 포함 [ (token, tag), (token, tag)... ]\n",
    "sentences = brown.tagged_sents(categories = 'news')  \n",
    "unigram_sent = nltk.UnigramTagger(sentences)\n",
    "\n",
    "# token만 포함     [ token, token, token....]\n",
    "sent = brown.sents(categories = 'news')              \n",
    "unigram_sent.tag(sent[2008])  # 4632개 token 중 '2008'번을 tag를 덧붙여 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9349006503968017"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 데이터와 테스트 데이터 검증\n",
    "unigram_sent.evaluate(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3698\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# from nltk.corpus import brown\n",
    "sentences = brown.tagged_sents(categories = 'news')\n",
    "sz = int(len(sentences)*0.8)\n",
    "sz  # 데이터 Train/ Test 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8026879907509996"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sents = sentences[:sz]\n",
    "testing_sents  = sentences[sz:]\n",
    "unigram_tagger = nltk.UnigramTagger(training_sents)\n",
    "unigram_tagger.evaluate(testing_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 02 N-gram POS 태거\n",
    "(NLP도구 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('Others', 'NNS'), None), ((',', ','), None), (('which', 'WDT'), None), (('are', 'BER'), None), (('reached', 'VBN'), None), (('by', 'IN'), None), (('walking', 'VBG'), None), (('up', 'IN'), None), (('a', 'AT'), None), (('single', 'AP'), None), (('flight', 'NN'), None), (('of', 'IN'), None), (('stairs', 'NNS'), None), ((',', ','), None), (('have', 'HV'), None), (('balconies', 'NNS'), None), (('.', '.'), None)]\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# from nltk.corpus import brown\n",
    "sentences = brown.tagged_sents(categories = 'news')\n",
    "sz = int(len(sentences) * 0.8)\n",
    "training_sents = sentences[:sz]\n",
    "testing_sents  = sentences[sz:]\n",
    "bigram_tagger  = nltk.UnigramTagger(training_sents)\n",
    "bigram_tagger  = nltk.BigramTagger(training_sents)\n",
    "bigram_tagger.tag(sentences[2008])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('The', 'AT'), None),\n",
       " (('population', 'NN'), None),\n",
       " (('of', 'IN'), None),\n",
       " (('the', 'AT'), None),\n",
       " (('Congo', 'NP'), None),\n",
       " (('is', 'BEZ'), None),\n",
       " (('13.5', 'CD'), None),\n",
       " (('million', 'CD'), None),\n",
       " ((',', ','), None),\n",
       " (('divided', 'VBN'), None),\n",
       " (('into', 'IN'), None),\n",
       " (('at', 'IN'), None),\n",
       " (('least', 'AP'), None),\n",
       " (('seven', 'CD'), None),\n",
       " (('major', 'JJ'), None),\n",
       " (('``', '``'), None),\n",
       " (('culture', 'NN'), None),\n",
       " (('clusters', 'NNS'), None),\n",
       " ((\"''\", \"''\"), None),\n",
       " (('and', 'CC'), None),\n",
       " (('innumerable', 'JJ'), None),\n",
       " (('tribes', 'NNS'), None),\n",
       " (('speaking', 'VBG'), None),\n",
       " (('400', 'CD'), None),\n",
       " (('separate', 'JJ'), None),\n",
       " (('dialects', 'NNS'), None),\n",
       " (('.', '.'), None)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "un_sent = sentences[4203]\n",
    "bigram_tagger.tag(un_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09186376993111421\n"
     ]
    }
   ],
   "source": [
    "print(bigram_tagger.evaluate(testing_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 03 Method를 Bootstrap으로 보완\n",
    "(NLP도구 3) : Back-off 알고리즘으로 보완(교차삽입)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8117443036755142"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# from nltk.corpus import brown\n",
    "sentences = brown.tagged_sents(categories = 'news')\n",
    "sz = int(len(sentences) * 0.8)\n",
    "\n",
    "training_sents = sentences[:sz]\n",
    "testing_sents  = sentences[sz:]\n",
    "\n",
    "s0 = nltk.DefaultTagger('NNP')\n",
    "s1 = nltk.UnigramTagger(training_sents, backoff = s0)\n",
    "s2 = nltk.BigramTagger(training_sents, backoff = s1)\n",
    "s2.evaluate(testing_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 2 행태학을 활용한 범주의 분석\n",
    "1. 행태학 단서\n",
    "2. 구문 단서\n",
    "3. 의미 단서\n",
    "\n",
    "언어학자들은 위의 3가지 단서를 활용하여 문장의 범주를 결정하고자 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 01 행태학 단서\n",
    "\n",
    "Morphological clues 는 단어의 범주를 결정하기 위해, \n",
    "\n",
    "접두사(prefix), 접미사(suffix), 삽입사(infix), 접사정보(affix information)를 사용한다\n",
    "\n",
    "ex ) -ity, -ness, -ment 등 품사를 바꾸는 접사"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 구문 단서\n",
    "Syntactic clues 는 명사뒤 형용사, 동사뒤 부사 등\n",
    "\n",
    "문법적인 내용을 활용하여 결정을 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 의미 단서\n",
    "Semantic information "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 04 청크 파서\n",
    "Chunk Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44084599507856814\n"
     ]
    }
   ],
   "source": [
    "# Chunk 파서의 기본형태\n",
    "#import nltk\n",
    "chunkparser = nltk.RegexpParser(\"\")\n",
    "print(nltk.chunk.accuracy(chunkparser, \n",
    "                          nltk.corpus.conll2000.chunked_sents('train.txt', \n",
    "                                                              chunk_types = ('NP',))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CD,JJ 등과 같은 '태그'를 찾는 Naive Chunk Parser의 평가에 기초한 분석\n",
    "# import nltk\n",
    "grammar = r\"NP: {<[CDJNP].*>+}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "print(nltk.chunk.accuracy(cp, \n",
    "                          nltk.corpus.conll2000.chunked_sents('train.txt', \n",
    "                                                              chunk_types=('NP',))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Confidence',\n",
       " 'Forte',\n",
       " 'Waterbury',\n",
       " 'skepticism',\n",
       " 'Meridian',\n",
       " '792',\n",
       " 'decliners',\n",
       " 'aftershock-damping',\n",
       " 'Exabyte',\n",
       " 'Hans-Dietrich',\n",
       " 'busload',\n",
       " 'closure',\n",
       " 'laborer',\n",
       " '4.9',\n",
       " 'Sweezey']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chunk 된 데이터에 대한 \n",
    "# '조건부 도수 분포'conditional frequency distribution에 근거한 계산\n",
    "# https://stackoverflow.com/questions/31270374/inc-object-with-nltk-for-py-2-7\n",
    "def chunk_tags(train):\n",
    "    cfd = nltk.ConditionalFreqDist() # chunk freqdist\n",
    "    for t in train:\n",
    "        for prev_word, tag, chunktag in nltk.chunk.tree2conlltags(t):\n",
    "            if chunktag == \"O\":\n",
    "                #cfreqdist[tag].inc(False)\n",
    "                cfd[prev_word][False] += 1\n",
    "            else:\n",
    "                #cfreqdist[tag].inc(True)\n",
    "                cfd[prev_word][True] += 1\n",
    "    return [tag   for tag in cfd.conditions()    if cfd[tag].max() == True]\n",
    "\n",
    "training_sents = nltk.corpus.conll2000.chunked_sents('train.txt', chunk_types=('NP',))\n",
    "chunk_tags(training_sents)[::1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 05 청크 파서의 평가\n",
    "Chunk Parser\n",
    "\n",
    "guessed 와 correct가 사용된다 (train/ test 대신)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbkAAABECAIAAACTTdwMAAAACXBIWXMAAA3XAAAN1wFCKJt4AAAAHXRFWHRTb2Z0d2FyZQBHUEwgR2hvc3RzY3JpcHQgOS4xOeMCIOUAAAz0SURBVHic7Z09bBtHFseHii2H8ke8CigffEEoDZMi1BUHL20DdyoCmCoSIJ1E4Jp8HGAaSIp0pLrYHZkuRQKQRT5cklcGSLFzgAsVB4vj4g7kFYddkoARnUXcrmObCk8f4RUvnlsvl9zlckku7fcrhNVwd/bNcOa/b96blULdbpcgCIIgA5mbtgEIgiAzAGolgiCIM6iVCIIgzqBWIgiCOINaiSAI4gxqJYIgiDOolQiCIM6gViIIgjhzYtoGIMivlMvlSqWSSqUkSaKUTtscBHkG9CuRQJDP5yml+Xyec14oFKZtDoJYQb8SCQS6rsuyTAhJp9OMsWmbgyBWQvg+OBIEOOeMMV3XU6kUiCaCBArUSiRYMMY459lsdtqGIMgzYLwSCQT5fB4OksmkruvTNQZBesF4JRIIFEUhhMiyrChKKpWatjkIYgXX4EhQMAyDc55MJqdtCILYgFqJIAjiDMYrEQRBnEGtRBAEcQa1EkEQxBnUSgRBEGdQKxEEQZzB/ZVIIGDV6r1m82+q+s/d3T+++ealaPQqpfLy8rTtQpBfwT1DyCTQ9va0VosQolSrhJB///TTP+7f19vt3YcPD46PB1974dw56fTppbNnf//66y+fPJlYWZEWFqTTp1FJkUmCWon4A6tWCSHG/n6lXieEaK2Wsb9PCPlrrTbgqlMnTvz36IgQci4cvkrpn65e/fPXX197662j4+NKo9E+OCCEhAgZMEYvRaPS6dOEEDkaJYTElpZoJEIISa6u+tY2BEGtRFxitNu80SCE8GZTf/IEDgghWqtVb7UsJ78SDs/Nzf18cNA5PBSFb164cPH8+eirr+49fqzu7f3rwQNCyKVoNBmPp65cEU7i5pdfaq0Wv3mTEKLt7ZUrlUq9/pdKhRCyMD//xoULJ+bmjo6P/37/vqh5YX7+1MmT8y+99ODRI4slK5EISCcqKTIiqJXIr/BGw2i3ydNlsrG/D6tmW8fwD2+88fPBweHx8cL8/INHj+bm5syKCQolR6OLZ87I0SiNRLRWS6lWy5UKnLaRSKyvribjcbq0ZKm5eOfOje++U/N5y0flnZ1Kvc5qtXvNJiHkUjSaWF6OLS11CdGfPOHNpkW1f/faa4dHR6+Ew0e//NLtdkOhEFxopp+SysvL4KsiiAC18gUClslaq6Xu7ZGnjqHRbveKyPmFBXD05Gi0c3jYPjg4e+rUjw8fPu50Hjx6ZD5fyCIhZH111RxGNNptVqsp1Wp5Z+fh/v75hYXNy5dBIgcokba3F8tmCx98kH777X4nQLWsVoNqk/F4YmVlM5GgS0u80dBarUq9DlrPG42H+/vmRv32/PnO4aG8vFz98cffnDs34JFg7gRCCOg+QSV9UUGtfH6w5E/IUzU064UAwnzSwgJ4UuBSPe50zr78MogpBBzNCgLaQSMRaWHBIosWM8wL55VIZBO8SNfLXvnmTRqJlD/5xPFMVq2CaIJ8w70SKyubly+LcyB6AI3izabl2QD9IEej0AOvnjnznydPiAvnGpX0RQO1cpZwnz8RMxmkjRCyvrpKCDELHKtWoSpQhH6yCHlnR6XjjUbp7l3zGjl15UoyHveQrc6WSp//8EP3m2/cX2K02+WdnUqjAT4sIWQjkUisrPQzAJ4rIIi82bQ8Tq7F46LtNBIRNVieRkJJe59G/fqfRiK9YQdkJkCtDBBD5U8ssbbBfo1FFnulARxMl7JoBsKIlkDk5uXLo/hWvNFI3LpV+vhjs3s41OWwSAf1X4lEkvG449qfmHqp16fujcD2Sl6/r8/Wr78WjxNU0pkCtXKiDJU/gekklsku9xXCLYTHZNFZIYuw5PS2YLQ4cSIQ6U3abKGZTDIeL3744SiViIApq9WgE67F43I0ak67DwYcSdA+x84c/IwZXUlxS+l0Qa30GW/5E2JyDN07F4NlUUTioObR42gQiDT7a5uJhHvdGYr0t9+yWk37/HO/KrREUUHfE8vLHlxgx/SRbaZrMEJJLSPHdklhiTXj5vzJgFo5HKPnT4inzX0WB6dfgmLACtEzlkDktXh8fXUVks5+3aKX8s5O6quvKp99No75b4kbwB7PobJPFoZKH3l7aNk+g1FJJwlqpRV/8ycegInXTxYte3TGF+Eq7+yI1Svsyxk9EDkUoY8+yrzzTn6c/3vHdvtRv42fHir3kD7yhvvVDL7m5JkXTivHlz/xbEy/PToDti6OCQhECu0QWREfA5HuMb/AMwHcbD/y5S6jpI+8AbEay+PfVknxNacBPIdaOYH8iTdYtepm66KHZPSIQCyvdPeu2O5jee9wKvR7gWfc9Nt+NKawg4/pI2+MrqQvyJbSmdTKSeZPPFvYb4/OdGXRDAQizdt9xqcIHnB8gWcCQBfxZnPY7Uej39f39JFnS1x6Hs/95vwgauW08ifeEMnowVsXR4nr+wusss3bfbylgyeA+xd4xo3t9iMQzcl43xNIH3mg3+b851JJp6OVU8+feMNxjw6NRAIliwKRxDC/d+h7MM53PLzAMwFgr7sv249GZJLpI8/mkeflNaexaGWg8ifeGBxFGuseHR+BWR20QKR7RnyBZwL4vv1odKaSPvLAzL3m5JtWZkslEdaxMMn8yYgU79wp3Llju0cnCMPLPUa7Ld+65fgH0IKPLy/wTACx111sP8pvbk4x0mrGMX2U39wM2mQcVklpJDLuQeKnVsLBxPIn4wDcBDKpPTpjJVsqwR+PCE4owAO80Zi5bwG2HwXcfzenj7befXe2pmrva06EkLFuxSXBzO0gCIIEDfyftwiCIM6gViIIgjiDWokgCOIMaiWCIIgzqJUIgiDOnDD/YhiGJEnmEsYY5zybzXqoWtM0TdPgWJZlqNkwDM65+bRkMumhcs55qVRKJBKbm5vm8mw2m8/nxa+9LepX2K9Cl5gbSymllPYrHBO2jbLAOTcMAzqcMUYIkWVZ07TeQseqfMeN/WAe9CS0RZIkSZJEJ0uSJMsy6TP2pgsYDMeUUsMw/O32GZq8vjCFAd81oShKtwfbQpdkMplut6uqaiaTqVQqUBtUCD/hBM/02qaq6uAT+hU6fuTSnt4abAt9x+UtNjY2xJmi820LJ4wb+1VVLRQK4ldhKlyr63qhUCiVSuZPzWMvCJib6W+3z9zkHZHJD/j/+5WMsVKppCgKISQWi6XTafFRsVgkhKiqurW1BQIMXlgsFjMMI51OD1ZlSmk+n89ms7Isw2NfcOPGDWc5dwdjTFGURCIh3DfbFg1opoWh2jgmDMPI5XKxWExV1cXFRfARoKVQCE60+0ZRShVFsbgDtoW+ILz1SqWSSqXg2/dsP7hjcGwYRiwWM38qSVI6nTavKsizY8+XtiwuLuq6DhNB0zT4diillUpFzA6X+Njtsz55gXw+r+v64uKiJEmqqkIJCc6Ad5TqXC6n63rX9FTXdT2Xy8Gn5uNezJotHvgD7jUstk6c413c+JXu22i53F+/0tyBvQ/AUqkkPCaXt8hkMqqqQnNEhbaFvpDJZMTg6XXuPNhfKBRg6VAoFKBmOM5kMplMZmNjQywsBow9b4gKdV23dF23x+fth7mZ/nb7zE3eXhRFgRtdv369G7wBf8JRTMVinlIKYs85V1V12DiI8AhmgqHaWCwWe59stoWjIDwmcaDrugdvV8RSHQtHZ2trq1gsgrMgemMU+9PpNISkzeEqSilUDm64xbUkfow9xtj6+jocW2yGrhOzYyjG1O2CmZu8YK2lhwMy4J210vbesVhsqO6GSLaHe02LodooJolhGGKdYls4CqAOnHNKKSSgyuWyt6qy2Wxv02wLR0TTNKhTqJgv9nPObbtUkqTFxUVLoS9jT5blXC43jjDFOLp9ADM3eYMz4F+6+ez/M8nlcqFQqFwudzodSinEBXZ3d2VZLhaLjLG1tTVK6e7u7vfff9/pdMrlcq1Wsx24jLFyuVytVhlj29vbn376aTgcJoRAlIdzvr29LZ7Vw5LNZiHNt729Xa/XZVnuLbFtUb9Cy+XJZNJNG4F79+7VajXOebVaTT19gd+2cFjq9TpjDGxQFGV9fT0cDpdKpVAoxBirVquc8/fee29AS83AN9LpdGRZhhpSqZRtoTdre/niiy/q9bphGLdv315fX6eUjmI/AHFbMW5hiG5vbzPGGGPhcHhtba3f2PNMOBwW30WhUEilUhcvXuSc3759G7oOZsf777/fr4ZisVgqlWCAhUIhTdN87/ZZmbz9KBQKMB7K5fLq6irnvF6vr62tBWXAW9bkuq67DAS4P3O62Nrp0nj3bVRV1ZKC71foAUVRRGyuX0k3qN+IrVUzZL+ZgFv4/E1eQRAGDP6dIQRBEGfwvR0EQRBnUCsRBEGcQa1EEARxBrUSQRDEGdRKBEEQZ1ArEQRBnEGtRBAEcQa1EkEQxBnUSgRBEGf+B2hKTznWpEdDAAAAAElFTkSuQmCC",
      "text/plain": [
       "Tree('S', [('the', 'DT'), ('little', 'JJ'), ('cat', 'NN'), ('sat', 'VBD'), ('on', 'IN'), ('the', 'DT'), ('mat', 'NN')])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chunk 평가를 수행하기 위한 계산\n",
    "import nltk\n",
    "correct = nltk.chunk.tagstr2tree(\"[ the/DT little/JJ cat/NN ] sat/VBD on/IN [ the/DT mat/NN ]\")\n",
    "correct.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy: 100.0%%\n",
      "    Precision:    100.0%%\n",
      "    Recall:       100.0%%\n",
      "    F-Measure:    100.0%%\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"NP: {<[CDJNP].*>+}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "\n",
    "grammar = r\"NP: {<PRP|DT|POS|JJ|CD|N.*>+}\"\n",
    "chunk_parser = nltk.RegexpParser(grammar)\n",
    "\n",
    "tagged_tok = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"cat\", \"NN\"),(\"sat\", \"VBD\"), (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n",
    "chunkscore = nltk.chunk.ChunkScore()\n",
    "guessed = cp.parse(correct.flatten())\n",
    "chunkscore.score(correct, guessed)\n",
    "\n",
    "# print 함수를 사용해야 결과를 구체적으로 볼 수 있다.\n",
    "print(chunkscore)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uni-gram, Bi-gram을 활용한 Chunk 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<<<< 295 page >>>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>>chunker_data = [[(t,c) for w,t,c in nltk.chunk.\n",
    "tree2conlltags(chtree)]\n",
    ">>>\n",
    "for chtree in nltk.corpus.conll2000.chunked_\n",
    "sents('train.txt')]\n",
    ">>> unigram_chunk = nltk.UnigramTagger(chunker_data)\n",
    ">>> print nltk.tag.accuracy(unigram_chunk, chunker_data)\n",
    "0.781378851068\n",
    ">>> bigram_chunk = nltk.BigramTagger(chunker_data, backoff=unigram_\n",
    "chunker)\n",
    ">>> print nltk.tag.accuracy(bigram_chunk, chunker_data)\n",
    "0.893220987404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> from nltk.corpus import brown\n",
    ">>> suffix_freqdist = nltk.FreqDist()\n",
    ">>> for wrd in brown.words():\n",
    "...\n",
    "wrd = wrd.lower()\n",
    "...\n",
    "suffix_freqdist[wrd[-1:]] += 1\n",
    "...\n",
    "suffix_fdist[wrd[-2:]] += 1\n",
    "...\n",
    "suffix_fdist[wrd[-3:]] += 1\n",
    ">>> common_suffixes = [suffix for (suffix, count) in suffix_freqdist.\n",
    "most_common(100)]\n",
    ">>> print(common_suffixes)\n",
    "['e', ',', '.', 's', 'd', '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> def pos_feature(wrd):\n",
    "...\n",
    "feature = {}\n",
    "...\n",
    "for suffix in common_suffixes:\n",
    "...\n",
    "feature['endswith({})'.format(suffix)] = wrd.lower().\n",
    "endswith(suffix)\n",
    "...\n",
    "return feature\n",
    ">>> tagged_wrds = brown.tagged_wrds(categories='news')\n",
    ">>> featureset = [(pos_feature(n), g) for (n,g) in tagged_wrds]\n",
    ">>> size = int(len(featureset) * 0.1)\n",
    ">>> train_set, test_set = featureset[size:], featureset[:size]\n",
    ">>> classifier1 = nltk.DecisionTreeClassifier.train(train_set)\n",
    ">>> nltk.classify.accuracy(classifier1, test_set)\n",
    "0.62705121829935351\n",
    ">>> classifier.classify(pos_features('cats'))\n",
    "'NNS'\n",
    ">>> print(classifier.pseudocode(depth=4))\n",
    "if endswith(,) == True: return ','\n",
    "if endswith(,) == False:\n",
    "if endswith(the) == True: return 'AT'\n",
    "if endswith(the) == False:\n",
    "if endswith(s) == True:\n",
    "if endswith(is) == True: return 'BEZ'\n",
    "if endswith(is) == False: return 'VBZ'\n",
    "if endswith(s) == False:\n",
    "if endswith(.) == True: return '.'\n",
    "if endswith(.) == False: return 'NN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> import nltk\n",
    ">>> from nltk.corpus import brown\n",
    ">>> sentences = brown.tagged_sents(categories='news')\n",
    ">>> sent = brown.sents(categories='news')\n",
    ">>> pattern = [\n",
    "(r'.*ing$', 'VBG'),\n",
    "# for gerunds\n",
    "(r'.*ed$', 'VBD'),\n",
    "# for simple past\n",
    "    (r'.*es$', 'VBZ'),\n",
    "# for 3rd singular present\n",
    "(r'.*ould$', 'MD'),\n",
    "# for modals\n",
    "(r'.*\\'s$', 'NN$'),\n",
    "# for possessive nouns\n",
    "(r'.*s$', 'NNS'),\n",
    "# for plural nouns\n",
    "(r'^-?[0-9]+(.[0-9]+)?$', 'CD'), # for cardinal numbers\n",
    "(r'.*', 'NN')\n",
    "# for nouns (default)\n",
    "]\n",
    ">>> regexpr_tagger = nltk.RegexpTagger(pattern)\n",
    ">>> regexpr_tagger.tag(sent[3])\n",
    "[('``', 'NN'), ('Only', 'NN'), ('a', 'NN'), ('relative', 'NN'),\n",
    "('handful', 'NN'), ('of', 'NN'), ('such', 'NN'), ('reports', 'NNS'),\n",
    "('was', 'NNS'), ('received', 'VBD'), (\"''\", 'NN'), (',', 'NN'),\n",
    "('the', 'NN'), ('jury', 'NN'), ('said', 'NN'), (',', 'NN'), ('``',\n",
    "'NN'), ('considering', 'VBG'), ('the', 'NN'), ('widespread', 'NN'),\n",
    "('interest', 'NN'), ('in', 'NN'), ('the', 'NN'), ('election', 'NN'),\n",
    "(',', 'NN'), ('the', 'NN'), ('number', 'NN'), ('of', 'NN'), ('voters',\n",
    "'NNS'), ('and', 'NN'), ('the', 'NN'), ('size', 'NN'), ('of', 'NN'),\n",
    "('this', 'NNS'), ('city', 'NN'), (\"''\", 'NN'), ('.', 'NN')]\n",
    ">>> regexp_tagger.evaluate(sentences)\n",
    "0.20326391789486245"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> import nltk\n",
    ">>> from nltk.corpus import brown\n",
    ">>> freqd = nltk.FreqDist(brown.words(categories='news'))\n",
    ">>> cfreqd = nltk.ConditionalFreqDist(brown.tagged_\n",
    "words(categories='news'))\n",
    ">>> mostfreq_words = freqd.most_common(100)\n",
    ">>> likelytags = dict((word, cfreqd[word].max()) for (word, _) in\n",
    "mostfreq_words)\n",
    ">>> baselinetagger = nltk.UnigramTagger(model=likelytags)\n",
    ">>> baselinetagger.evaluate(brown_tagged_sents)\n",
    "0.45578495136941344\n",
    ">>> sent = brown.sents(categories='news')[3]\n",
    ">>> baselinetagger.tag(sent)\n",
    "[('``', '``'), ('Only', None), ('a', 'AT'), ('relative', None),\n",
    "('handful', None), ('of', 'IN'), ('such', None), ('reports', None),\n",
    " >>> baselinetagger = nltk.UnigramTagger(model=likely_tags,\n",
    "...\n",
    "backoff=nltk.\n",
    "DefaultTagger('NN'))\n",
    "def performance(cfreqd, wordlist):\n",
    "lt = dict((word, cfreqd[word].max()) for word in wordlist)\n",
    "baseline_tagger = nltk.UnigramTagger(model=lt, backoff=nltk.\n",
    "DefaultTagger('NN'))\n",
    "return baseline_tagger.evaluate(brown.tagged_\n",
    "sents(categories='news'))\n",
    "def display():\n",
    "import pylab\n",
    "word_freqs = nltk.FreqDist(brown.words(categories='news')).most_\n",
    "common()\n",
    "words_by_freq = [w for (w, _) in word_freqs]\n",
    "cfd = nltk.ConditionalFreqDist(brown.tagged_\n",
    "words(categories='news'))\n",
    "sizes = 2 ** pylab.arange(15)\n",
    "perfs = [performance(cfd, words_by_freq[:size]) for size in sizes]\n",
    "pylab.plot(sizes, perfs, '-bo')\n",
    "pylab.title('Lookup Tagger Performance with Varying Model Size')\n",
    "pylab.xlabel('Model Size')\n",
    "pylab.ylabel('Performance')\n",
    "pylab.show()\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> import nltk\n",
    ">>> from nltk.stem.lancaster import LancasterStemmer\n",
    ">>> stri=LancasterStemmer()\n",
    ">>> stri.stem('achievement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConseNPChunkTagger(nltk.TaggerI):\n",
    "def __init__(self, train_sents):\n",
    "train_set = []\n",
    "for tagsent in train_sents:\n",
    "untagsent = nltk.tag.untag(tagsent)\n",
    "history = []\n",
    "for i, (word, tag) in enumerate(tagsent):\n",
    "featureset = npchunk_features(untagsent, i, history)\n",
    "train_set.append( (featureset, tag) )\n",
    "history.append(tag)\n",
    "self.classifier = nltk.MaxentClassifier.train(\n",
    "train_set, algorithm='megam', trace=0)\n",
    "def tag(self, sentence):\n",
    "history = []\n",
    "for i, word in enumerate(sentence):\n",
    "featureset = npchunk_features(sentence, i, history)\n",
    "tag = self.classifier.classify(featureset)\n",
    "history.append(tag)\n",
    "return zip(sentence, history)\n",
    "class ConseNPChunker(nltk.ChunkParserI): [4]\n",
    "def __init__(self, train_sents):\n",
    "tagsent = [[((w,t),c) for (w,t,c) in\n",
    "nltk.chunk.tree2conlltags(sent)]\n",
    "for sent in train_sents]\n",
    "self.tagger = ConseNPChunkTagger(tagsent)\n",
    "def parse(self, sentence):\n",
    "tagsent = self.tagger.tag(sentence)\n",
    "conlltags = [(w,t,c) for ((w,t),c) in tagsent]\n",
    "return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chapter 10 - 8\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "sentences = brown.tagged_sents(categories='news')\n",
    "sent = brown.sents(categories='news')\n",
    "pattern = [(r'(January)$','Jan')]\n",
    "regexpr_tagger = nltk.RegexpTagger(pattern)\n",
    "print(regexpr_tagger.tag(sent[3]))\n",
    "print(regexpr_tagger.evaluate(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chapter 10 - 9\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "freqd = nltk.FreqDist(brown.words(categories='news'))\n",
    "cfreqd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news'))\n",
    "mostfreq_words = freqd.most_common(100)\n",
    "likelytags = dict((word, cfreqd[word].max()) for (word, _) in mostfreq_words)\n",
    "baselinetagger = nltk.UnigramTagger(model=likelytags)\n",
    "\n",
    "sent = brown.sents(categories='news')[3]\n",
    "print(baselinetagger.tag(sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 10.py\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stri=LancasterStemmer()\n",
    "print(stri.stem('achievement'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
